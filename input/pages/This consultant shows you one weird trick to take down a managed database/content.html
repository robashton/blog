<h3 id="sysadmins-hate-him-">Sysadmins hate him...</h3>
<p>I was visiting a client last week who have been having trouble with their RavenDB instance for a few months and understandably getting a bit frustrated as time went on.</p>
<h2 id="the-scene">The scene</h2>
<p>I arrived, drank some coffee and we hit a room with the projector in it, and brought up the graphs of resource usage on the server running RavenDB - they&#39;ve been pretty handy with the <a href="http://www.splunk.com">splunk</a> and they have quite a few graphs! (Their usage of Splunk was <em>awesome</em> actually, can highly recommend looking at it)</p>
<p>Memory usage looks something like this through the day</p>
  <script type="text/javascript" src="/d3.v2.js"></script>
  <script type="text/javascript" src="/dimple.js"></script>

  <div id="initial-memory-usage"></div>

  <script type="text/javascript">
    var svg = dimple.newSvg("#initial-memory-usage", 590, 400);
    var myChart = new dimple.chart(svg, [
    { Hour: 0, Memory: 15},
    { Hour: 1, Memory: 15},
    { Hour: 2, Memory: 15},
    { Hour: 3, Memory: 15},
    { Hour: 4, Memory: 0},
    { Hour: 5, Memory: 2},
    { Hour: 6, Memory: 4},
    { Hour: 7, Memory: 4},
    { Hour: 8, Memory: 4},
    { Hour: 9, Memory: 6},
    { Hour: 10, Memory: 6},
    { Hour: 11, Memory: 7},
    { Hour: 12, Memory: 8},
    { Hour: 13, Memory: 10},
    { Hour: 14, Memory: 10},
    { Hour: 15, Memory: 10},
    { Hour: 16, Memory: 14},
    { Hour: 17, Memory: 14},
    { Hour: 18, Memory: 10},
    { Hour: 19, Memory: 10},
    { Hour: 20, Memory: 15},
    { Hour: 21, Memory: 15},
    { Hour: 22, Memory: 15},
    { Hour: 23, Memory: 15}
    ]);
    myChart.setBounds(60, 30, 510, 305)
    var x = myChart.addCategoryAxis("x", "Hour");
    x.addOrderRule("Hour");
    myChart.addMeasureAxis("y", "Memory");
    myChart.addSeries(null, dimple.plot.bar);
    myChart.draw();
  </script>


<p>That 4am block is a result of an automated process to kill their RavenDB instance every day because if they left it running it would being down the server when people were actually using the system - not so good! (It starts spiking around 9am because it starts being under quite a reasonable load).</p>
<h2 id="my-line-of-questioning-on-seeing-this">My line of questioning on seeing this</h2>
<ul>
<li>How many databases on that one instance: <em>12</em></li>
<li>How much memory on the server: <em>16gb</em></li>
<li>How many cores on that server: <em>2</em></li>
<li>How much data in the databases?: <em>Between 500mb and 13gb</em></li>
<li>How many documents in that largest database?: <em>Er, not that many</em></li>
<li>How big are those documents?: <em>Some are quite big, they have PDFs attached to them</em></li>
</ul>
<h1 id="ah-">Ah.</h1>
<p>The thing is, RavenDB can deal with large documents. Internally it does quite a few things to avoid objects ending up on the <a href="http://msdn.microsoft.com/en-us/magazine/cc534993.aspx">Large Object Heap</a> or being promoted to the 2nd generation.</p>
<ul>
<li>Using streams in and out of core storage</li>
<li>Using streams in an out of HTTP</li>
<li>De-serializing only into RavenObject structures (lots of small objects)</li>
<li>Not holding onto objects any longer than it has to</li>
</ul>
<p>If you were to create objects with lots of fields that reached the above size in all likelihood RavenDB&#39;s practises around this kind of thing would result in happy developers, happy ops and happy sales teams?</p>
<p>But byte arrays that are automatically put on the Large Object Heap? There is little Raven can do about these, as when the objects internally are de-serialized into tokens, the smallest token it can make with them is however large the byte array is! </p>
<p>Under what circumstances does RavenDB load these fields?</p>
<ul>
<li>Indexing</li>
<li>Querying</li>
<li>Loading</li>
</ul>
<p>Imagine now that you create a new index on the server and it has to </p>
<ul>
<li>run through all of the documents to put content into Lucene</li>
<li>To do this it has to de-serialize them</li>
<li>When being de-serialized .NET is going to say &quot;That field is large, it is going on the Large Object Heap&quot;</li>
<li>It is going to have to look for space on the LOH</li>
<li>It is going to expand the LOH</li>
<li>They&#39;re all different sizes, it is unlikely to find space in the middle very often</li>
<li>The LOH is going to keep expanding during the indexing process</li>
<li>The machine is going to run out of memory</li>
</ul>
<p>This is just typical .NET behaviour, and to make things worse, when the issues first started being noticed the first port of call was to open Raven Studio and start inspecting the server (performing queries), thus adding to the problem and causing even more hilarious memory spikes.</p>
<p>To give an indication, when opening up the performance counters for the server the kind of thing we were seeing looked like this:</p>
<p>  <img src="/img/lho.png"></p>
<p>Yes indeed, that&#39;s nearly all the memory on the server being allocated to the LHO as a result of excessive large objects of varying sizes being aggressively loaded through the indexing and querying processes.</p>
<h1 id="the-solution-">The solution?</h1>
<p>Much like with every other database out there, storing binary blobs in a store which is built for querying/transactions isn&#39;t ideal - but there are two options available here</p>
<ul>
<li>External storage (s3, fileservers, anything else)</li>
<li>RavenDB Attachments</li>
</ul>
<p>The latter isn&#39;t encouraged as it&#39;s just a convenience - but to prove a point I generated 1.5 million documents of varying sizes with byte arrays on the fields to reproduce the problem successfully on my laptop (that&#39;s actually the screenshot above), then migrated them into attachments to show what a difference this would make as attachments are <em>never</em> loaded fully into memory.</p>
<p>  <img src="/img/beforeafter.jpg"></p>
<p>What a difference choosing an appropriate store makes! In the second number the &quot;PDFs&quot; are <em>still being stored in RavenDB</em>, just not in the primary document store.</p>
<p>When I left the client their server was sitting flat at 4gb consumption (with the database still full of PDFs, but instructions in how to avoid causing issues until they had been purged)</p>
<h1 id="the-summary">The summary</h1>
<ul>
<li>Well, I think Raven could benefit from having some sort of warning when it sees this sort of usage, although it&#39;s not <em>that</em> common so not really a priority</li>
<li>There is little a database can do to get around this sort of thing, save sticking things in off-heap storage - but that&#39;s not going to work when your indexing is written in .NET</li>
</ul>
<p>I&#39;m currently <a href="https://github.com/robashton/cravendb">writing my own database</a> in a different managed platform and I&#39;m strongly considering sticking indexing into its own process to avoid this sort of long-term build up of issues. That said - the JVM doesn&#39;t do per-process GC so that might not help that much.</p>
<p>Either way it&#39;s interesting and points to one of the limitations of writing a database or any high throughput system in a managed environment if you&#39;re going to be expecting big chunks of data that can&#39;t be broken up somehow. (Okay, this is quite specific, and will rarely catch anybody out).</p>
