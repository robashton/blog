<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Rob Ashton's blog]]></title><description><![CDATA[Software development dumping ground]]></description><link>http://codeofrob.com</link><image><url>http://codeofrob.com/img/cover.jpg</url><title>Rob Ashton&apos;s blog</title><link>http://codeofrob.com</link></image><generator>RSS for Node</generator><lastBuildDate>Tue, 30 Mar 2021 14:42:17 GMT</lastBuildDate><atom:link href="http://feeds.feedburner.com/robashton" rel="self" type="application/rss+xml"/><author><![CDATA[Rob Ashton]]></author><item><title><![CDATA[Encoding h264 with Nvidia]]></title><description><![CDATA[<p>This blog entry is part of my &quot;<a href="/entries/blogging-the-mundane.html">blog about mundane stuff</a>&quot; series.</p>
<p>I covered my <a href="/entries/decoding-h264-with-nvidia.html">decoding</a> test before having a look at <a href="/entries/cuda-context-management-with-nvenc.html">context management</a>, so now it&#39;s time to look at re-encoding the h264 and demonstrating a round trip through the nvidia hardware.</p>
<h1 id="what-we-have-already">What we have already</h1>
<p>We had this workflow for testing</p>
<pre><code>
  #workflow {
    generator = #read_from_ts { name = source, filename  = &lt;&lt;<span class="hljs-string">&quot;foo.ts&quot;</span>&gt;&gt; },
    processors = [
      #nvidia_decoder { name = decode, <span class="hljs-keyword">from</span> = { source, ?video_frames_with_stream_id(<span class="hljs-number">256</span>) } },
      #x264_encoder { name = encode, <span class="hljs-keyword">from</span> = decode },
      #ts_writer { name = write, <span class="hljs-keyword">from</span> = encode, filename  = &lt;&lt;<span class="hljs-string">&quot;out.ts&quot;</span>&gt;&gt; }
    ]
  }

</code></pre><p>and what I want is</p>
<pre><code>  #workflow {
    generator = #read_from_ts { name = source, filename  = &lt;&lt;<span class="hljs-string">&quot;foo.ts&quot;</span>&gt;&gt; },
    processors = [
      #nvidia_decoder { name = decode, <span class="hljs-keyword">from</span> = { source, ?video_frames_with_stream_id(<span class="hljs-number">256</span>) } },
      #nvidia_encoder { name = encode, <span class="hljs-keyword">from</span> = decode },
      #ts_writer { name = write, <span class="hljs-keyword">from</span> = encode, filename  = &lt;&lt;<span class="hljs-string">&quot;out.ts&quot;</span>&gt;&gt; }
    ]
  }
</code></pre><p>In the decode entry we had a series of pointers to surfaces that we&#39;d pulled out of the decoder along with the corresponding timestamps, these are in NV12 planar format and byte-aligned (and I&#39;m storing all that information alongside the pointer to that surface in a struct so now we just need to see what the API looks like to turn this surface into h264 once again...)</p>
<p>This looks something like this (It actually looks nothing like this as most of this info is stored in a linked list tied to a central pool which contains the shared information), but this is at least representative of what we have.</p>
<pre><code>
  <span class="hljs-keyword">typedef</span> <span class="hljs-class"><span class="hljs-keyword">struct</span> _<span class="hljs-title">nvidia_allocated_surface</span>  {</span>
    <span class="hljs-keyword">uint8_t</span>* data;
    <span class="hljs-keyword">int</span> bpp;
    <span class="hljs-keyword">int</span> byte_width;
    <span class="hljs-keyword">int</span> byte_height;
    <span class="hljs-keyword">size_t</span> pitch;
  } nvidia_allocated_surface;
</code></pre><p>This surface is allocated <em>on the GPU</em> via CUDA, and the GPU is where we want to do our encode so hopefully we can pretty much use this data directly.</p>
<h1 id="the-api">The API</h1>
<p>The encode API looks nothing like the decode API in that it&#39;s sat in its own header file (which is again expected to be included as part of the repo, but the library we&#39;re going to be loading is <em>definitely</em>  expected to exist on the runtime as part of the driver install.</p>
<ul>
<li><em>Include/nvEncodeAPI.h</em> contains all definitions/enums/functions/etc</li>
</ul>
<p>and somewhere on the host OS installed as part of the driver package (for me, /run/opengl-driver/lib/)</p>
<ul>
<li><em>libnvidia-encode.so</em></li>
</ul>
<h1 id="creating-the-api">Creating the API</h1>
<p>The functions that we end up using are loaded as a struct of function pointers and we actually only link a single function directly from that library to get hold of that struct. As a consequence of this, every struct being passed into a function in this API tends to be versioned indepedendently, such is life.</p>
<pre><code class="language-c">
  <span class="hljs-keyword">uint32_t</span> version = <span class="hljs-number">0</span>;
  <span class="hljs-keyword">uint32_t</span> currentVersion = (NVENCAPI_MAJOR_VERSION &lt;&lt; <span class="hljs-number">4</span>) | NVENCAPI_MINOR_VERSION;
  NV_ENCODE_API_FUNCTION_LIST nvenc = {NV_ENCODE_API_FUNCTION_LIST_VER};

  NvEncodeAPIGetMaxSupportedVersion(&amp;version);
  <span class="hljs-keyword">if</span> (currentVersion &gt; version)
  {
    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>; <span class="hljs-comment">// or whatever</span>
  }

  NvEncodeAPICreateInstance(&amp;nvenc);

  <span class="hljs-comment">// use the functions on nvenc</span>
</code></pre>
<p>This doesn&#39;t do anything other than a <em>dlopen</em> of the library exporting the functions and copy the pointers to those functions onto the nvenc struct so they can be called. Invoke it once, cache the results somewhere and then we can get to the business of talking to the GPU.</p>
<h1 id="creating-an-encoder">Creating an encoder</h1>
<p>Unlike the decode stack, the encoder doesn&#39;t seem to be built <em>directly</em> on top of CUDA, it having the capability of instantiating on top of OpenGL/DirectX/Cuda as options (and then utilise surfaces from those systems). Because we&#39;re using the decoder (and planning on using CUDA to run upscaling algorithms and such), it makes sense to use CUDA for the encoder too.</p>
<p>Because of this, we&#39;ll want to use our already-created CUDA context and pass this into the open session call.</p>
<pre><code>
  NV_ENC_OPEN_ENCODE_SESSION_EX_PARAMS <span class="hljs-attr">encodeSessionExParams</span> = { NV_ENC_OPEN_ENCODE_SESSION_EX_PARAMS_VER };
  encodeSessionExParams.<span class="hljs-attr">device</span> = ctx;
  encodeSessionExParams.<span class="hljs-attr">deviceType</span> = NV_ENC_DEVICE_TYPE_CUDA;
  encodeSessionExParams.<span class="hljs-attr">apiVersion</span> = NVENCAPI_VERSION;
  void* <span class="hljs-attr">encoder</span> = NULL;

  nvenc.nvEncOpenEncodeSessionEx(&amp;encodeSessionExParams, &amp;encoder);
</code></pre><p>Now, this doesn&#39;t do very much other than give us an encode session that isn&#39;t initialise - and to initialise it we need to tell it what form our desired output needs to take. It is entirely possible to spend a day or two wondering why this call gives you back &#39;NV_ENC_ERR_INVALID_PARAM&#39; if it&#39;s not set up correctly (and reading the sample code doesn&#39;t help because the configuration of the encoder happens across multiple files because of the way that the samples are written), the below seems to be the the minimum required..</p>
<p>In the Real World (tm) we have an input surface with a known width/height from the decoder, here we&#39;ll hard code it along with the frame rate (I know my source is 25fps). In the Real World a load of these args will be passed in from Erlang, not C.</p>
<pre><code class="language-c">
  NV_ENC_INITIALIZE_PARAMS initializeParams = { NV_ENC_INITIALIZE_PARAMS_VER };
  NV_ENC_CONFIG encodeConfig = { NV_ENC_CONFIG_VER };
  NV_ENC_PRESET_CONFIG presetConfig = { NV_ENC_PRESET_CONFIG_VER, { NV_ENC_CONFIG_VER } };

  initializeParams.encodeConfig = &amp;encodeConfig;

  <span class="hljs-comment">// The essentials</span>
  initializeParams-&gt;encodeGUID = NV_ENC_CODEC_H264_GUID;
  initializeParams-&gt;presetGUID = NV_ENC_PRESET_P4_GUID;

  <span class="hljs-comment">// These need to line up with the input surface dimensions</span>
  initializeParams-&gt;encodeWidth = <span class="hljs-number">576</span>;
  initializeParams-&gt;encodeHeight = <span class="hljs-number">720</span>;

  <span class="hljs-comment">// Ditto these</span>
  initializeParams-&gt;frameRateNum = <span class="hljs-number">25</span>;
  initializeParams-&gt;frameRateDen = <span class="hljs-number">1</span>;

  <span class="hljs-comment">// I seemed to need these as well</span>
  initializeParams.enablePTD = <span class="hljs-number">1</span>;
  initializeParams.encodeConfig-&gt;frameIntervalP = <span class="hljs-number">3</span>;
  initializeParams.encodeConfig-&gt;gopLength = <span class="hljs-number">50</span>;
  initializeParams.tuningInfo = NV_ENC_TUNING_INFO_HIGH_QUALITY;

</code></pre>
<p>Once this initialize params basics is set up, we can ask the encoder to populate the actual details for us based on the preset and codec specified in the &#39;essentials&#39;</p>
<pre><code class="language-c">
  nvenc.nvEncGetEncodePresetConfigEx(encode_session-&gt;encoder, initializeParams.encodeGUID, initializeParams.presetGUID, tuningInfo, &amp;presetConfig);
  <span class="hljs-built_in">memcpy</span>(initializeParams.encodeConfig, &amp;presetConfig.presetCfg, <span class="hljs-keyword">sizeof</span>(NV_ENC_CONFIG));
</code></pre>
<p>I&#39;m not entirely sure why we don&#39;t just invoke nvEncGetEncoderPresetConfigEx on the struct already contained on the initializeParams, but the samples all end up doing this and the above code is precarious enough already (honestly, the number of ways this can go wrong with a single error is infuriating!). So we stick with the samples way of doing stuff!</p>
<p>I also needed to then go and set up the IDR frequency to match our requested gop length (the samples do this too) and not doing this resulted in NV_ENC_ERR_INVALID_PARAM so..</p>
<pre><code class="language-c">
    initializeParams.encodeConfig-&gt;encodeCodecConfig.h264Config.idrPeriod = initializeParams.encodeConfig-&gt;gopLength;
</code></pre>
<p>With all of the above done correctly, a call to the init fn will return a success and if not I&#39;m really sorry you&#39;re on your own because I&#39;ve served my time already.</p>
]]></description><link>http://codeofrob.com/entries/encoding-h264-with-nvidia.html</link><guid isPermaLink="true">http://codeofrob.com/entries/encoding-h264-with-nvidia.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Thu, 19 Nov 2020 09:30:00 GMT</pubDate></item><item><title><![CDATA[CUDA Context Management with NvEnc]]></title><description><![CDATA[<p>This blog entry is part of my &quot;<a href="/entries/blogging-the-mundane.html">blog about mundane stuff</a>&quot; series.</p>
<p>In the <a href="/entries/decoding-h264-with-nvidia.html">previous blog entry</a> I touched on context management in CUDA being a set of choices with conflicting information from documentation to popular code samples and such.</p>
<h1 id="what-is-the-problem">What is the &quot;problem&quot;?</h1>
<p>Most operations with CUDA expect there to be a CUDA context bound to the current thread. All operations against that CUDA context are serialized (unless those operations are bound to a specific stream, in which case they are serialized against that stream). That&#39;s it, that&#39;s the whole goal of &quot;context&quot; in CUDA, and we have more than one way of realising this in the API].</p>
<h1 id="the-api">The API</h1>
<p>I suspect this API has been subject to a bit of churn since CUDA was first realised and this would go some way to expaining the various ways of dealing with the context, starting off with the method that the samples tend to use..</p>
<pre><code>  <span class="hljs-regexp">//</span>
  <span class="hljs-regexp">//</span> On startup
  <span class="hljs-regexp">//</span>

  cuCtxCreate(&amp;ctx, flags, dev);

  <span class="hljs-regexp">//</span>
  <span class="hljs-regexp">//</span> Repeat below until finished
  <span class="hljs-regexp">//</span>

  cuCtxPushCurrent(ctx);

  <span class="hljs-regexp">//</span> TODO: some operation 

  cuCtxPopCurrent(NULL);

  <span class="hljs-regexp">//</span>
  <span class="hljs-regexp">//</span>  When finished
  <span class="hljs-regexp">//</span>

  cuCtxDestroy(ctx);

</code></pre><p>If we look at the API documentation for <a href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__CTX.html#group__CUDA__CTX_1g65dc0012348bc84810e2103a40d8e2cf">cuCtxCreate</a>, the very firist sentence we see is &quot;In most cases it is recommended to use cuDevicePrimaryCtxRetain.&quot;</p>
<p>So immediately on looking up the documentation for the API that the code samples use, we are told to use something else. Now - the way I understand this, is that you can create contexts within a section of your code and use <em>Push</em> and <em>Pop</em> when using that context, and then code being invoked whilst this context is valid can do the same and you can end up with a stack of contexts that works happily together. Some searching around this reveals that there are performance penalties or even limitations over the number of active contexts in an application at the same time (operations are serialized anyway) and what we <em>can</em> do is simply get hold of the primary context with</p>
<pre><code>  <span class="hljs-regexp">//</span>
  <span class="hljs-regexp">//</span> On startup
  <span class="hljs-regexp">//</span>


  cuDevicePrimaryCtxRetain(&amp;ctx, dev);

  <span class="hljs-regexp">//</span>
  <span class="hljs-regexp">//</span> Repeat below until finished
  <span class="hljs-regexp">//</span>

  cuCtxSetCurrent(&amp;ctx);

  <span class="hljs-regexp">//</span> TODO: some operation

  cuCtxSetCurrent(NULL);

  <span class="hljs-regexp">//</span>
  <span class="hljs-regexp">//</span>  When finished
  <span class="hljs-regexp">//</span>

  cuDevicePrimaryCtxRelease(ctx);

</code></pre><p>Now some more disparate info found in the recesses of Google/Stackoverflow/Nvidia forums</p>
<ul>
<li>Originally contexts weren&#39;t bindable to multiple threads at the same time</li>
<li>Decode sessions cannot share contexts across threads by default</li>
<li>Contexts take up a chunk of ram, buffers are not sharable across contexts</li>
<li>A primary context is analogous to the device itself</li>
<li>A failure on the primary context is going to cascade  into all users of that context</li>
<li>contexts can be defined as &#39;floating&#39; if they&#39;re not bound to a thread by default</li>
<li>most operations against a context are async, and not complete until you call &#39;synchronise&#39;</li>
</ul>
<p>It turns out that the nvidia decode/encode API provides another mechanism on top of contexts - the <em>lock</em>, which actually means we can share the same context across multiple decode sessions without too much issue. These are the choices I made with the above information given the needs of our encode/decode work:</p>
<ul>
<li>We will be running multiple encode/decode <em>processes</em> that should be completely isolated, this implies that context <em>needs</em> creating per process.</li>
<li>We can share the context throughout that process so long as we use locks</li>
<li>Locks perform the same job as Push/Set/Pop, but with a mutex involved to make sure all work is serialized across threads</li>
</ul>
<pre><code>  <span class="hljs-comment">//</span>
  <span class="hljs-comment">// On startup</span>
  <span class="hljs-comment">//</span>


  cu<span class="hljs-constructor">CtxCreate(&amp;<span class="hljs-params">ctx</span>, <span class="hljs-params">flags</span>, <span class="hljs-params">dev</span>)</span>;
  cuvid<span class="hljs-constructor">CtxLockCreate(&amp;<span class="hljs-params">lock</span>, <span class="hljs-params">ctx</span>)</span>;


  <span class="hljs-comment">//</span>
  <span class="hljs-comment">// Repeat below until finished</span>
  <span class="hljs-comment">//</span>

  cuvid<span class="hljs-constructor">CtxLock(<span class="hljs-params">lock</span>, 0)</span>;

  <span class="hljs-comment">// <span class="hljs-doctag">TODO:</span> some operation</span>

  cuvid<span class="hljs-constructor">CtxUnlock(<span class="hljs-params">lock</span>, 0)</span>;

  <span class="hljs-comment">//</span>
  <span class="hljs-comment">//  When finished</span>
  <span class="hljs-comment">//</span>

  cuvid<span class="hljs-constructor">CtxLockDestroy(<span class="hljs-params">lock</span>)</span>;
  cu<span class="hljs-constructor">CtxDestroy(<span class="hljs-params">ctx</span>)</span>;

</code></pre><p>In my tests (spinning up multiple processes/tests), this seemed to be the route to getting a lowish resource usage, a good throughput and most importantly a lack of errors. If more throughput is required, then the concept of &#39;streams&#39; can be utilised against this same context for further parallelisation (that seems to be a case of creating streams per ... well... stream of work and just passing that reference around as a synchronisation point into the various API calls).</p>
<p>Because the cuvidCtxLock is a cuvid concept and not a CUDA concept, we we can pass a pointer to this lock into the decoder instantiated in the last blog entry so it will automatically use that lock when performing operations against the bound context and play nicely with our code.</p>
<p>We can replace every instance of Push and Pop in that blog entry with Lock and Unlock, and add the lock to the decoder creation params to take advantage of this</p>
<pre><code>  decode_create_info.vidLock = session-&gt;lock;
  cuvid<span class="hljs-constructor">CtxLock(<span class="hljs-params">session</span>-&gt;<span class="hljs-params">lock</span>, 0)</span>;
  cr = cuvid<span class="hljs-constructor">CreateDecoder(&amp;<span class="hljs-params">session</span>-&gt;<span class="hljs-params">decoder</span>, &amp;<span class="hljs-params">decode_create_info</span>)</span>;
  cuvid<span class="hljs-constructor">CtxUnlock(<span class="hljs-params">session</span>-&gt;<span class="hljs-params">lock</span>, 0)</span>;
</code></pre><p>I couldn&#39;t work out if we actually wanted to use Lock/Unlock around decoder creation, but it didn&#39;t hurt so in it went.</p>
<p>I <em>think</em> that this is how these APIs should be used, I give no claims to actual correctness - the documentation is vague and contradictory in places with samples/such but I have stress tested this and I&#39;ve also demonstrated the failure cases to myself (multiple threads, no locks) to hilarity so it&#39;s probably close enough to be right.</p>
<p>Note: The transform/encode side of the API doesn&#39;t provide config to use this locking mechanism and that feels a little bit like the left hand not knowing what the right hand is doing, but hey ho - use what we can, when we can.</p>
]]></description><link>http://codeofrob.com/entries/cuda-context-management-with-nvenc.html</link><guid isPermaLink="true">http://codeofrob.com/entries/cuda-context-management-with-nvenc.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Tue, 17 Nov 2020 09:30:00 GMT</pubDate></item><item><title><![CDATA[Decoding h264 with Nvidia]]></title><description><![CDATA[<p>This blog entry is part of my &quot;<a href="/entries/blogging-the-mundane.html">blog about mundane stuff</a>&quot; series.</p>
<p>The scene is set and I&#39;ve <a href="/entries/exploring-the-nvidia-code-samples-and-docs.html">set my laptop up to do Nvidia work</a>,so now to look at the decode process in isolation.</p>
<p>The first task for me will be to attempt to decode one of my &#39;known good&#39; h264 sources. Now - my sources are nearly all transport streams with audio (often multiple) and such, so I&#39;ll need to parse those files, pull the streams out of them, filter the video stream from a single pid and send the h264 frame data into the API for decode. This is quite a lot of work and there is no way of testing whether the results are good short of either dumping them to disk and telling ffmpeg what it&#39;s looking at (raw frames), or piping that data into an encoder in code and writing out a fresh transport stream with the round-tripped h264.</p>
<p>I&#39;ll take that second option because I already have all the code required to do this in Erlang in our proprietary workflow engine - a simplified example of the code I&#39;ll write to test my decoder appears below. (Working inside of a mature codebase has its advantages)</p>
<pre><code class="language-erlang">
  #workflow {
    generator = #read_from_ts { name = source, filename  = &lt;&lt;<span class="hljs-string">&quot;foo.ts&quot;</span>&gt;&gt; },
    processors = [
      #nvidia_decoder { name = decode, from = { source, ?video_frames_with_stream_id(<span class="hljs-number">256</span>) } },
      #x264_encoder { name = encode, from = decode },
      #ts_writer { name = write, from = encode, filename  = &lt;&lt;<span class="hljs-string">&quot;out.ts&quot;</span>&gt;&gt; }
    ]
  }
</code></pre>
<p>Now to get to this point, the &#39;best&#39; way to achieve this is to try to mirror the underlying API as best as possible in Erlang and write as direct a NIF as possible under this. We can assume that&#39;s what I&#39;m then using in the Erlang, that allows me to write a test Erlang file and simply calls one or two of the methods with minimum config and build that up as I go. So we&#39;ll say that that&#39;s what I&#39;m doing with an <em>nvidia_test.erl</em> calling into an <em>nvidia_api.erl</em> which my <em>nvidia_decoder.erl</em> will eventually leverage.</p>
<p>We can therefore jump straight into the C and write some functions that we can assume are being called from Erlang with the appropriate arguments.</p>
<h1 id="the-api">The API</h1>
<p>The Decode API is contained in a couple of header files in the SDK tarfile</p>
<ul>
<li><em>Interface/cuviddec.h</em> all the enums and functions</li>
<li><em>Interface/nvcuvid.h</em>  some  high level helpers (includes the above)</li>
</ul>
<p>A pre-built library for this exists in</p>
<ul>
<li><em>Lib/[os]/[arch]/libnvcuvid.so</em></li>
</ul>
<p>but on my OS this library is shipped with nvidia-x11 and that&#39;s the package I am loading it from at runtime, but it is also shipped with the opengl-drivers package. Googling around this I can see confusion over whether this is supposed to be vendored or not (why would we need to have X libs installed in order to run a decode process for example).</p>
<p>I suspect that we&#39;d just vendor this in production rather than install packages we don&#39;t need.</p>
<h1 id="parsing-the-h264">Parsing the h264</h1>
<p>Each frame has a blob of &#39;data&#39;, which is essentially a sequence of NAL units, some of which are metadata describing the video content and some of which are the data itself. We can parse this ourselves in Erlang and pass just the data into the decoder, or we can just pass the whole lot into Nvidia&#39;s parser and use the callbacks provided by that parser to then feed a decoder.</p>
<p>We do actually have code lying around for parsing h264, which is why we even have the concept of a &#39;frame&#39; at all in the above code, but by far the easiest way to use the Nvidia decoder to use that parser as it then ends up providing the exact structures required <em>for</em> that decoder.</p>
<p>Creating a parser is quite easy, we populate a CUVIDPARSERPARAMS with appropriate config and init the darned thing - this is all host code for execution on the CPU and is not remotely specific to nvidia and doesn&#39;t require any hardware setup. In my code, the parser parameters are passed in from Erlang, but hard coded below. <em>decode_session</em> is a pointer to a struct containing both the parser and a pile of information also passed in by the Erlang.</p>
<pre><code class="language-c">
  CUVIDPARSERPARAMS videoParserParameters = {}
  videoParserParameters.CodecType = cudaVideoCodec_H264;
  videoParserParameters.ulMaxNumDecodeSurfaces = <span class="hljs-number">1</span>;
  videoParserParameters.ulClockRate = <span class="hljs-number">90000</span>;
  videoParserParameters.ulMaxDisplayDelay = <span class="hljs-number">0</span>;
  videoParserParameters.pUserData = decode_session;

  videoParserParameters.pfnSequenceCallback = decode_session_handle_video_sequence;
  videoParserParameters.pfnDecodePicture = decode_session_handle_picture_decode;
  videoParserParameters.pfnDisplayPicture = decode_session_handle_picture_display;


 cuvidCreateVideoParser(&amp;decode_session-&gt;parser, &amp;videoParserParameters);
</code></pre>
<p>There are three callbacks on this struct, which are very much designed to work around the decode itself. </p>
<ul>
<li><em>pfnSequenceCallback</em>: Stream information, create the decoder if you want / Stream has changed and decoder needs reconfiguring</li>
<li><em>pfnDecodePicture</em>: Here is data and information about that data, stick it into the decoder</li>
<li><em>pfnDisplayPicture</em>: You should pulling a frame out of the decoder here cos it&#39;s ready</li>
</ul>
<p>If the reader is unfamiliar with video streams, the reason we have two callbacks for decode/display is that frames in the incoming h264 stream are not necessarily in display order because you can have references to previous/future frames in encoded h264 data. It&#39;s not simply &#39;one in, one out&#39;.</p>
<p>The form of the data going in therefore is &#39;packets&#39; with timestamps which we can use for correlation coming back out again, calling the code from Erlang (but removing all the NIF mess) looks like this, simply throwing data at the parser and letting it do its job.</p>
<pre><code class="language-c">
<span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">decode_frame</span><span class="hljs-params">(decode_session* session, <span class="hljs-keyword">void</span>* data, <span class="hljs-keyword">int</span> size, <span class="hljs-keyword">int64_t</span> timestamp)</span>
</span>{
  CUVIDSOURCEDATAPACKET packet = { <span class="hljs-number">0</span> };
  packet.payload = data;
  packet.payload_size = size;
  packet.flags = CUVID_PKT_TIMESTAMP;
  packet.timestamp = timestamp;

  <span class="hljs-keyword">if</span> (size == <span class="hljs-number">0</span>) {
    packet.flags |= CUVID_PKT_ENDOFSTREAM;
  }

  cuCtxPushCurrent(session-&gt;ctx);
  cr = cuvidParseVideoData(session-&gt;parser, &amp;packet);
  cuCtxPopCurrent(session-&gt;ctx);

}

</code></pre>
<p>My first test effectively boiled down to calling this parser with a couple of frames with empty callbacks and printfs just to make sure that things were initialising as expected and the callbacks were being invoked.</p>
<pre><code class="language-c">
  <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">int</span> CUDAAPI <span class="hljs-title">decode_session_handle_video_sequence</span><span class="hljs-params">(<span class="hljs-keyword">void</span> *obj, CUVIDEOFORMAT* pVideoFormat)</span>
  </span>{
    TRACE(<span class="hljs-string">&quot;handle_video_sequence  \r\n&quot;</span>);
  }
</code></pre>
<p>Each of these callbacks present us with a <em>void\</em> obj<em>, which is the *pUserData</em> passed into the parser parameters on creation, we&#39;re using a struct here containing the parser and parameters, and it makes sense to stash the decoder in this struct too.</p>
<h1 id="initialising-the-hardware">Initialising the hardware</h1>
<p>We can&#39;t create a decoder until we&#39;ve parsed some h264, but the decoder API is built directly on top of some CUDA constructs and those constructs will need creating up front if we are to create that decoder. </p>
<p>All CUDA operations revolve around having a CUDA context created around the device we want to use for the CUDA operations, ignoring the return results (not something I&#39;m doing in the real code), a basic setup looks thus. (I&#39;ve also stripped the Erlang comms from these implementations, as messaging code and binary reference counting is outside the scope of this blog entry).</p>
<pre><code class="language-c">
 CUdevice cuDevice = <span class="hljs-number">0</span>;
 CUcontext ctx = <span class="hljs-number">0</span>;


 cuInit(<span class="hljs-number">0</span>);
 cuDeviceGet(&amp;cuDevice, <span class="hljs-number">0</span>);
 cuCtxCreate(&amp;ctx, <span class="hljs-number">0</span>, cuDevice);
</code></pre>
<p>Now, for all operations involving CUDA, this context will need binding to the current thread (except where in cases where various Nvidia APIs helpfully do this for us) and there are multiple ways of managing that context, the documentation tells us to do it one way whilst saying that another way is the default and the samples go on to do it in a whole other manner. I&#39;ll actually try and cover this in the next entry because it might save somebody some time in the future (or somebody might e-mail me to tell me I&#39;ve completely missed the point, that&#39;d be quite nice).</p>
<p>We stash this context on the struct being used in the parser above so I can then use it in calls later, but that&#39;s the extent of the setup we can do until we&#39;ve parsed some of the stream.</p>
<h1 id="creating-the-decoder">Creating the decoder</h1>
<p>In the callback for <em>pfnSequenceCallback</em>, we get told about the video format in the struct <em>CUVIDEOFORMAT</em>, and get passed our struct as a <em>void\</em> <em>, so the first thing to do here is grab that struct because it has some config on it, and then create a *CUVIDDECODECREATEINFO</em> and populate it from both the config and the information about the video given to us by the parser.</p>
<pre><code class="language-c">  <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">int</span> CUDAAPI <span class="hljs-title">decode_session_handle_video_sequence</span><span class="hljs-params">(<span class="hljs-keyword">void</span> *obj, CUVIDEOFORMAT* pVideoFormat)</span> </span>{
    CUresult cr;
    decode_session *session = (decode_session *) obj;
    CUVIDDECODECREATEINFO decode_create_info = { <span class="hljs-number">0</span> };

    decode_create_info.ulWidth = pVideoFormat-&gt;coded_width;
    decode_create_info.ulHeight = pVideoFormat-&gt;coded_height;
    decode_create_info.CodecType = pVideoFormat-&gt;codec;
    decode_create_info.ChromaFormat = pVideoFormat-&gt;chroma_format;
    decode_create_info.bitDepthMinus8 = pVideoFormat-&gt;bit_depth_luma_minus8;
    decode_create_info.ulTargetWidth = pVideoFormat-&gt;coded_width;
    decode_create_info.ulTargetHeight = pVideoFormat-&gt;coded_height;
    decode_create_info.ulNumDecodeSurfaces = pVideoFormat-&gt;min_num_decode_surfaces;
    decode_create_info.ulNumOutputSurfaces = <span class="hljs-number">2</span>;
    decode_create_info.ulIntraDecodeOnly = <span class="hljs-number">0</span>;
    decode_create_info.Reserved1 = <span class="hljs-number">0</span>;
    decode_create_info.ulCreationFlags = cudaVideoCreate_PreferCUVID;

    decode_create_info.ulMaxWidth = session-&gt;max_width;
    decode_create_info.ulMaxHeight = session-&gt;max_height;
    decode_create_info.OutputFormat = session-&gt;output_format;
    decode_create_info.DeinterlaceMode = session-&gt;deinterlace_mode;
  }
</code></pre>
<p>Creating the decoder itself is just a case of binding the CUDA context to the current thread and invoking the relevant API.</p>
<pre><code class="language-c">
  cuCtxPushCurrent(session-&gt;ctx);
  cr = cuvidCreateDecoder(&amp;session-&gt;decoder, &amp;decode_create_info);
  cuCtxPopCurrent(<span class="hljs-literal">NULL</span>);
</code></pre>
<p>The callback expects a return value of *&lt; 0* if there is a failure, or  <em>min_num_decode_surfaces</em> if there is a success, so...</p>
<pre><code class="language-c">
  <span class="hljs-keyword">if</span> ( CUDA_SUCCESS == cr ) {
    <span class="hljs-keyword">return</span> pVideoFormat-&gt;min_num_decode_surfaces;
  } <span class="hljs-keyword">else</span>  {
    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;
  }
</code></pre>
<p>Bit of a faff, but fairly linear at least, with these steps completed we&#39;ll start receiving parsed frame data into <em>pfnDecodePicture</em> which we can feed directly into the decoder.</p>
<pre><code class="language-c">
  <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">int</span> CUDAAPI <span class="hljs-title">decode_session_handle_picture_decode</span><span class="hljs-params">(<span class="hljs-keyword">void</span>* obj, CUVIDPICPARAMS* pPicParams)</span>
  </span>{
    decode_session *session = (decode_session *) obj;
    CUresult cr;

    cuCtxPushCurrent(session-&gt;ctx);
    cr = cuvidDecodePicture(session-&gt;decoder, pPicParams);
    cuCtxPopCurrent(<span class="hljs-literal">NULL</span>);

    <span class="hljs-keyword">if</span> ( CUDA_SUCCESS == cr ) {
      <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>;
    } <span class="hljs-keyword">else</span> {
      <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;
    }
}
</code></pre>
<p><em>pPicParams</em> contains the data as well as information about this frame (such as whether it&#39;s an iframe or not) and conveniently the parser gives the exact data that the decoder expects in order to do its job. With this, we&#39;ll start getting invocations of <em>pfnDisplayPicture</em> with raw frame data which we can send back into our application for further processing. This is actually a bit more complicated, as up until now the memory management has been taken care of us by the APIs we are using.</p>
<p>The parser is accepting our host memory buffers, and then creating buffers in host memory for its own data which are being copied into device memory by the decoder, and in order to get the data out we&#39;ll need to copy it out of these decoder managed buffers into either host or device memory.</p>
<p>Copying data out of device memory into host memory is expensive because of limited bandwidth and proximity but is necessary at <em>some</em> point if we are to output that data to anywhere useful. It is however not always desirable to immediately do this if we are then to do an encode or transform on the GPU, we probably want to create buffers on the device itself and perform a copy from the decoder-managed buffers onto our own managed buffers.</p>
<p>Our code will assume that we want the memory copied into more device memory and we&#39;ll incur another copy if we want to subsequently copy it out onto the host; this is for convenience as in the real world we&#39;ll almost certainly be doing further operations on the GPU and only moving data to the host for testing (as in the above workflow) or for very specific operations (such as some sort of fingerprinting or overlay scenario).</p>
<p>So, the first thing we need to do here is call cuvidMapVideoFrame, which will block until the frame specified by <em>picture_index</em> has been decoded and is ready for copying out. MapVideoFrame <em>effectively</em> locks that decoder-owned buffer and readies it for use in further calls (copying it into something we own). Once  we&#39;ve called Map, we check the decode status to make sure that the data is worth copying out in the first place. I&#39;ve left us a <em>...</em> as a placeholder for the next steps.</p>
<pre><code class="language-c">
<span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">int</span> CUDAAPI <span class="hljs-title">decode_session_handle_picture_display</span><span class="hljs-params">(<span class="hljs-keyword">void</span>* obj, CUVIDPARSERDISPINFO* pDispInfo)</span>
</span>{
  decode_session *session = (decode_session *) obj;
  CUresult cr = <span class="hljs-number">0</span>;
  CUdeviceptr srcFrame = <span class="hljs-number">0</span>;
  <span class="hljs-keyword">unsigned</span> <span class="hljs-keyword">int</span> srcPitch = <span class="hljs-number">0</span>;
  CUVIDGETDECODESTATUS status = {<span class="hljs-number">0</span>};

  CUVIDPROCPARAMS vpp = {<span class="hljs-number">0</span>};
  vpp.progressive_frame = pDispInfo-&gt;progressive_frame;
  vpp.second_field = pDispInfo-&gt;repeat_first_field + <span class="hljs-number">1</span>;
  vpp.top_field_first = pDispInfo-&gt;top_field_first;
  vpp.unpaired_field = pDispInfo-&gt;repeat_first_field &lt; <span class="hljs-number">0</span>;

  cuCtxPushCurrent(session-&gt;ctx);

  <span class="hljs-keyword">if</span>((cr = cuvidMapVideoFrame(session-&gt;decoder, pDispInfo-&gt;picture_index, &amp;srcFrame, &amp;srcPitch, &amp;vpp)) != CUDA_SUCCESS) {
    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;
  }

  cr = cuvidGetDecodeStatus(session-&gt;decoder, pDispInfo-&gt;picture_index, &amp;status);

  <span class="hljs-keyword">if</span> (cr == CUDA_SUCCESS &amp;&amp; (status.decodeStatus == cuvidDecodeStatus_Error || status.decodeStatus == cuvidDecodeStatus_Error_Concealed))
  {
    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;
  }

  <span class="hljs-comment">// ... </span>

  cuCtxPopCurrent(<span class="hljs-literal">NULL</span>);

</code></pre>
<p>And now we&#39;re at the meat of it, we&#39;ve locked the decoded frame and need somewhere to put it. In the real world we operate a pool of surfaces which can be passed into Erlang and reference counted before being returned to the pool when Erlang has finished using that surface. In this example we&#39;ll just create a surface on demand and assume that somebody will destroy it (or re-use it) at some point - obviously creation is expensive so it is best not to be doing this on demand in reality.</p>
<p>We have to calculate the sizes for this surface based on the <em>CUVIDEOFORMAT</em> that we received in the pfnSequenceCallback, <em>bpp</em> will change depending on the bit depth of the source, but the only output format the decoder actually supports is NV12 so I&#39;ve hard coded byte height to (Chroma = 1 * height) + (Luma  = 0.5 * height), as those are the planes  that we&#39;ll expect in this format. If you don&#39;t know about planar formats then go and <a href="https://wiki.videolan.org/YUV">read about them</a> if you&#39;re writing this code haha, you&#39;ll need to understand it.</p>
<pre><code class="language-c">
  <span class="hljs-keyword">void</span>* dstFrame;
  <span class="hljs-keyword">int</span> dstPitch;
  <span class="hljs-keyword">int</span> bpp = pVideoFormat-&gt;bit_depth_luma_minus8 &gt; <span class="hljs-number">0</span> ? <span class="hljs-number">2</span> : <span class="hljs-number">1</span>;
  <span class="hljs-keyword">int</span> frame_width = pVideoFormat-&gt;display_area.right - pVideoFormat-&gt;display_area.left;
  <span class="hljs-keyword">int</span> byte_width = frame_width * bpp;
  <span class="hljs-keyword">int</span> byte_height = (pVideoFormat-&gt;display_area.right - pVideoFormat-&gt;display_area.left) * <span class="hljs-number">1.5</span>;

  cuMemAllocPitch((CUdeviceptr *)&amp;dstFrame, &amp;dstPitch, frameWidth  * bpp, byte_height, <span class="hljs-number">16</span>);
</code></pre>
<p>We allocate pitched memory (also known as byte-aligned where the pitch is the stride..), essentially we want our buffer to be a be a nice round number, usually a power of 2 because  it makes for efficient read/writes. The pitch is output into &#39;dstPitch&#39; which we&#39;ll need to use later on when using this buffer because maths.</p>
<p>Assuming we have a buffer created as above, we can copy from our mapped video frame into our new device memory with</p>
<pre><code class="language-c">
  CUDA_MEMCPY2D m = { <span class="hljs-number">0</span> };
  CUresult cr;

  m.srcMemoryType = CU_MEMORYTYPE_DEVICE;
  m.srcDevice = (CUdeviceptr)srcFrame;
  m.srcPitch = srcPitch;
  m.dstMemoryType = CU_MEMORYTYPE_DEVICE;
  m.dstDevice = (CUdeviceptr)dstFrame;
  m.dstPitch = dstPitch;
  m.WidthInBytes = byte_width;
  m.Height = byte_height;

  cuMemcpy2D(&amp;m);
</code></pre>
<p>If we wanted this in host memory, we&#39;d do a straight up malloc of (byte_width * byte_height) with a pitch of byte_width, and copy it out this way</p>
<pre><code>
  CUDA_MEMCPY2D <span class="hljs-attr">m</span> = { <span class="hljs-number">0</span> };
  CUresult cr;

  m.<span class="hljs-attr">srcMemoryType</span> = CU_MEMORYTYPE_DEVICE;
  m.<span class="hljs-attr">srcDevice</span> = (CUdeviceptr)srcFrame;
  m.<span class="hljs-attr">srcPitch</span> = srcPitch;
  m.<span class="hljs-attr">dstMemoryType</span> = CU_MEMORYTYPE_HOST;
  m.<span class="hljs-attr">dstHost</span> = (CUdeviceptr)dstFrame;
  m.<span class="hljs-attr">dstPitch</span> = byte_width;
  m.<span class="hljs-attr">WidthInBytes</span> = byte_width;
  m.<span class="hljs-attr">Height</span> = byte_height;

  cuMemcpy2D(&amp;m);
</code></pre><p>That data can then be fired into libx264 along with <em>pDispInfo-&gt;timestamp</em>, encoded and viewed with pleasure. I guess the next post I should  probably talk about CUDA context management before I get to the job of encoding or transforming this data and  maybe it&#39;s worth talking a little about how I&#39;m managing reference counted surfaces between Erlang and C as well as that&#39;s a whole world of fun after this.</p>
<p>Mundane, but hopefully useful to me or somebody else in the future.</p>
]]></description><link>http://codeofrob.com/entries/decoding-h264-with-nvidia.html</link><guid isPermaLink="true">http://codeofrob.com/entries/decoding-h264-with-nvidia.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Thu, 12 Nov 2020 09:30:00 GMT</pubDate></item><item><title><![CDATA[Exploring the NVIDIA code samples and docs]]></title><description><![CDATA[<p>This blog entry is part of my &quot;<a href="/entries/blogging-the-mundane.html">blog about mundane stuff</a>&quot; series.</p>
<h1 id="the-sdk">The SDK</h1>
<p>There seems to be a single tar file available for the NVEnc/Dec stuff containing the headers, sample code and some PDFs that are semi-useful for learning how all this stuff ties together, so I unpacked that locally to the project I was working in and set up paths so I could include the header files from it. </p>
<p>Step one in learning a new SDK or whatever is usually to</p>
<ul>
<li>A) RTFM</li>
<li>B) Look at the samples</li>
<li>C) Look at how others do it</li>
</ul>
<p>For <em>A</em> we have some PDFs lying about with 101 instructions on how to tie stuff together (not sufficient by itself), in theory there are some full SDK references available (found via Google for older versions but not for latest, must only be for the the upper class developers signed into some portal or another that I don&#39;t have access to - or I&#39;m terrible at Google), and we have the various header files with reasonably well documented functions lying about.</p>
<p>They do provide some good &#39;you will need to do these things in order to get a context with which to do...&#39;, but they&#39;re best read alongside the sample code to get some understanding of why it does what it does.</p>
<p>For <em>B</em> there is a full suite of samples, doing anything we pretty much might want to do with the SDK - but sadly (as is unfortunately common with SDKs such as this) rather than write samples directly against the APIs as would be convenient for the learner, they&#39;ve built a whole abstraction in C++ and then used <em>that</em> for all the samples because it&#39;s  convenient for the author - that&#39;s <em>obviously</em> easier to learn from than just some sample code that does the bare minimum in a linear fashion - <em>rolleyes</em>.</p>
<p>For <em>C</em> we look towards <a href="https://github.com/FFmpeg/FFmpeg/">ffmpeg</a>, (<a href="https://github.com/FFmpeg/FFmpeg/blob/master/libavcodec/nvenc.c">nvenc</a>/<a href="https://github.com/FFmpeg/FFmpeg/blob/master/libavcodec/nvdec.c">nvdec</a>) because that <em>works</em> but acknowledge that usually when they implement various pipelines/codecs that they&#39;re often not done in the most efficient manner because it&#39;s a big pluggable abstract system and that&#39;s not the stated goal or necessarily even compatible with their architecture.</p>
<p>Irregardless of the quality of the samples (That&#39;s a real word as of 2020, sorry I don&#39;t make the rules), they are where I found the most value for learning how the NV stuff works, even if it meant tracing through step by step and pulling the relevant function calls and the order they&#39;re invoked in into my own test code.</p>
<p>From any perspective, it&#39;s pretty obvious from looking at the header files/pdfs that encode and decode are very much their own things done in their own way with their own enums and own flags and own API design, it makes sense from a hardware perspective that these are dedicated processes but that it bubbles up to the API itself so dramatically is amusing - I imagine this is the result of separate teams working largely in isolation and <a href="https://en.wikipedia.org/wiki/Conway%27s_law">Conway&#39;s Law</a> holding steady...</p>
<h1 id="building-the-samples">Building the samples</h1>
<p>This was a bit of a dance for me, having the wrong versions of GCC about for the purposes of building the rest of our stack initially, but essentially I found out where the right versions of GCC had ended up from my shell.nix and manually told cmake about them..</p>
<pre><code>
mkdir build &amp;&amp; cd build

cmake -DCMAKE_LIBRARY_PATH=/run/opengl-driver/lib/ -DCMAKE_C_COMPILER=/nix/store/l2abq8hpgdjc4x7dwdps7zqcnxmjmjp4-gcc-<span class="hljs-keyword">wrapper</span><span class="hljs-number">-8.3</span><span class="hljs-number">.0</span>/bin/gcc -DCMAKE_CXX_COMPILER=/nix/store/l2abq8hpgdjc4x7dwdps7zqcnxmjmjp4-gcc-<span class="hljs-keyword">wrapper</span><span class="hljs-number">-8.3</span><span class="hljs-number">.0</span>/bin/g++ ..
make
make install
</code></pre><p>Not ideal but hey ho, this gave me a pile of binaries in my &#39;build&#39; directory which I can invoke to find out what my hardware is capable of (in the case of decode)</p>
<pre><code>~/nvidia/Samples/build]$ ./AppDec -h

Options:
-i             Input file path
-o             Output file path
-outplanar     Convert output to planar format
-gpu           Ordinal of GPU to use
-crop l,t,r,b  Crop rectangle in left,top,right,bottom (ignored for case 0)
-resize WxH    Resize to dimension W times H (ignored for case 0)

Decoder Capability

GPU in use: GeForce GTX 1050
Codec  JPEG   BitDepth <span class="hljs-number"> 8 </span>  ChromaFormat  4:2:0  Supported <span class="hljs-number"> 1 </span> MaxWidth <span class="hljs-number"> 32768 </span> MaxHeight <span class="hljs-number"> 16384 </span> MaxMBCount <span class="hljs-number"> 67108864 </span> MinWidth <span class="hljs-number"> 64 </span>  MinHeight <span class="hljs-number"> 64 </span>  SurfaceFormat  NV12
Codec  MPEG1  BitDepth <span class="hljs-number"> 8 </span>  ChromaFormat  4:2:0  Supported <span class="hljs-number"> 1 </span> MaxWidth <span class="hljs-number"> 4080 </span>  MaxHeight <span class="hljs-number"> 4080 </span>  MaxMBCount <span class="hljs-number"> 65280 </span>    MinWidth <span class="hljs-number"> 48 </span>  MinHeight <span class="hljs-number"> 16 </span>  SurfaceFormat  NV12
Codec  MPEG2  BitDepth <span class="hljs-number"> 8 </span>  ChromaFormat  4:2:0  Supported <span class="hljs-number"> 1 </span> MaxWidth <span class="hljs-number"> 4080 </span>  MaxHeight <span class="hljs-number"> 4080 </span>  MaxMBCount <span class="hljs-number"> 65280 </span>    MinWidth <span class="hljs-number"> 48 </span>  MinHeight <span class="hljs-number"> 16 </span>  SurfaceFormat  NV12
Codec  MPEG4  BitDepth <span class="hljs-number"> 8 </span>  ChromaFormat  4:2:0  Supported <span class="hljs-number"> 1 </span> MaxWidth <span class="hljs-number"> 2032 </span>  MaxHeight <span class="hljs-number"> 2032 </span>  MaxMBCount <span class="hljs-number"> 8192 </span>     MinWidth <span class="hljs-number"> 48 </span>  MinHeight <span class="hljs-number"> 16 </span>  SurfaceFormat  NV12
Codec  H264   BitDepth <span class="hljs-number"> 8 </span>  ChromaFormat  4:2:0  Supported <span class="hljs-number"> 1 </span> MaxWidth <span class="hljs-number"> 4096 </span>  MaxHeight <span class="hljs-number"> 4096 </span>  MaxMBCount <span class="hljs-number"> 65536 </span>    MinWidth <span class="hljs-number"> 48 </span>  MinHeight <span class="hljs-number"> 16 </span>  SurfaceFormat  NV12
Codec  HEVC   BitDepth <span class="hljs-number"> 8 </span>  ChromaFormat  4:2:0  Supported <span class="hljs-number"> 1 </span> MaxWidth <span class="hljs-number"> 8192 </span>  MaxHeight <span class="hljs-number"> 8192 </span>  MaxMBCount <span class="hljs-number"> 262144 </span>   MinWidth <span class="hljs-number"> 144 </span> MinHeight <span class="hljs-number"> 144 </span> SurfaceFormat  NV12
Codec  HEVC   BitDepth <span class="hljs-number"> 10 </span> ChromaFormat  4:2:0  Supported <span class="hljs-number"> 1 </span> MaxWidth <span class="hljs-number"> 8192 </span>  MaxHeight <span class="hljs-number"> 8192 </span>  MaxMBCount <span class="hljs-number"> 262144 </span>   MinWidth <span class="hljs-number"> 144 </span> MinHeight <span class="hljs-number"> 144 </span> SurfaceFormat  NV12 P016
Codec  HEVC   BitDepth <span class="hljs-number"> 12 </span> ChromaFormat  4:2:0  Supported <span class="hljs-number"> 1 </span> MaxWidth <span class="hljs-number"> 8192 </span>  MaxHeight <span class="hljs-number"> 8192 </span>  MaxMBCount <span class="hljs-number"> 262144 </span>   MinWidth <span class="hljs-number"> 144 </span> MinHeight <span class="hljs-number"> 144 </span> SurfaceFormat  NV12 P016
Codec  HEVC   BitDepth <span class="hljs-number"> 8 </span>  ChromaFormat  4:4:4  Supported <span class="hljs-number"> 0 </span> MaxWidth <span class="hljs-number"> 0 </span>     MaxHeight <span class="hljs-number"> 0 </span>     MaxMBCount <span class="hljs-number"> 0 </span>        MinWidth <span class="hljs-number"> 0 </span>   MinHeight <span class="hljs-number"> 0 </span>   SurfaceFormat  N/A
Codec  HEVC   BitDepth <span class="hljs-number"> 10 </span> ChromaFormat  4:4:4  Supported <span class="hljs-number"> 0 </span> MaxWidth <span class="hljs-number"> 0 </span>     MaxHeight <span class="hljs-number"> 0 </span>     MaxMBCount <span class="hljs-number"> 0 </span>        MinWidth <span class="hljs-number"> 0 </span>   MinHeight <span class="hljs-number"> 0 </span>   SurfaceFormat  N/A
Codec  HEVC   BitDepth <span class="hljs-number"> 12 </span> ChromaFormat  4:4:4  Supported <span class="hljs-number"> 0 </span> MaxWidth <span class="hljs-number"> 0 </span>     MaxHeight <span class="hljs-number"> 0 </span>     MaxMBCount <span class="hljs-number"> 0 </span>        MinWidth <span class="hljs-number"> 0 </span>   MinHeight <span class="hljs-number"> 0 </span>   SurfaceFormat  N/A
Codec  VC1    BitDepth <span class="hljs-number"> 8 </span>  ChromaFormat  4:2:0  Supported <span class="hljs-number"> 1 </span> MaxWidth <span class="hljs-number"> 2032 </span>  MaxHeight <span class="hljs-number"> 2032 </span>  MaxMBCount <span class="hljs-number"> 8192 </span>     MinWidth <span class="hljs-number"> 48 </span>  MinHeight <span class="hljs-number"> 16 </span>  SurfaceFormat  NV12
Codec  VP8    BitDepth <span class="hljs-number"> 8 </span>  ChromaFormat  4:2:0  Supported <span class="hljs-number"> 0 </span> MaxWidth <span class="hljs-number"> 0 </span>     MaxHeight <span class="hljs-number"> 0 </span>     MaxMBCount <span class="hljs-number"> 0 </span>        MinWidth <span class="hljs-number"> 0 </span>   MinHeight <span class="hljs-number"> 0 </span>   SurfaceFormat  N/A
Codec  VP9    BitDepth <span class="hljs-number"> 8 </span>  ChromaFormat  4:2:0  Supported <span class="hljs-number"> 1 </span> MaxWidth <span class="hljs-number"> 8192 </span>  MaxHeight <span class="hljs-number"> 8192 </span>  MaxMBCount <span class="hljs-number"> 262144 </span>   MinWidth <span class="hljs-number"> 128 </span> MinHeight <span class="hljs-number"> 128 </span> SurfaceFormat  NV12
Codec  VP9    BitDepth <span class="hljs-number"> 10 </span> ChromaFormat  4:2:0  Supported <span class="hljs-number"> 1 </span> MaxWidth <span class="hljs-number"> 8192 </span>  MaxHeight <span class="hljs-number"> 8192 </span>  MaxMBCount <span class="hljs-number"> 262144 </span>   MinWidth <span class="hljs-number"> 128 </span> MinHeight <span class="hljs-number"> 128 </span> SurfaceFormat  NV12 P016
Codec  VP9    BitDepth <span class="hljs-number"> 12 </span> ChromaFormat  4:2:0  Supported <span class="hljs-number"> 1 </span> MaxWidth <span class="hljs-number"> 8192 </span>  MaxHeight <span class="hljs-number"> 8192 </span>  MaxMBCount <span class="hljs-number"> 262144 </span>   MinWidth <span class="hljs-number"> 128 </span> MinHeight <span class="hljs-number"> 128 </span> SurfaceFormat  NV12 P016
</code></pre><p>Honestly it&#39;s kinda impressive how much my nearly three year old laptop is capable of, dedicated hardware decode of 4k HEVC? Sure thing... </p>
<h1 id="the-sample-code-then">The sample code then</h1>
<p>The sample code is essentially boils down to a pile of projects under &#39;AppEncode&#39;, &#39;AppDecode&#39;, and &#39;AppTranscode&#39; directories - in theory demonstrating in isolation how to do those tasks, I say in theory because all of the examples are effectively a main(int argc, char** arg) that parse the command line and then spin up the shared code in the NvDecoder/NvEncoder directories.</p>
<p>That&#39;s super annoying, as it means the useful code is sat inside two C++ classes plus a pile of inheritance to change the behaviour at times and if you want to work out what one of these &#39;isolated&#39; examples are actually doing you have to traipse up and down a pile of abstracted mess to work out how the API is actually being invoked. This is a masterclass in how <em>not</em> to write useful example code for an SDK in case anybody is still unclear as to how I feel about this.</p>
<p>The useful code for decoding exists in a single class, <em>NvDecoder</em>, with most of the interesting code taking place in <em>Decode</em> and <em>GetFrame</em>. There is a lot of work happening under the covers which will need looking at in a following blog post, but what is interesting here is that Nvidia provide the means not only to decode the video bitstream, but also to parse the codec around that bitstream. That&#39;s kinda cool because this isn&#39;t a small feat - it&#39;s quite common for video decoder APIs to only deal with the bitstream which makes the cost of entry quite high unless you&#39;ve already got all of that code lying about. Of course if you&#39;re already parsing the nals and such in your pipeline you can skip that aspect of the Nvidia API (this being what is recommended) but because their parser is built to work with their decoder that&#39;s clearly what I&#39;m going to start with.</p>
<p>The useful code for encoding exists in a single class, <em>NvEncoder</em>, but with various overrides in things like <em>NvEncoderCuda</em>, <em>NvEncoderD3D11</em>, etc. So that&#39;s a pain in the arse, not to mention the heap of boolean decisions around #IFDEFs, if(m_bSomeFlag) making it difficult to work out what one of the isolated examples is doing.  It is safe to say that understanding the encoding process is going to be frustrating.</p>
<h1 id="next-steps">Next steps</h1>
<p>Honestly getting this far was a faff in itself, some of it was harder because of my Nixos setup and some of it was easier (Side by side GCC installations and such are never that much fun), and my driver set up whilst easy on paper took me a few attempts to get correct because of my previous efforts in entirely disabling the GPU...</p>
<p>Having the samples building and working though, we can be confident that if I write the right code in the next steps that the rest of it will work...</p>
]]></description><link>http://codeofrob.com/entries/exploring-the-nvidia-code-samples-and-docs.html</link><guid isPermaLink="true">http://codeofrob.com/entries/exploring-the-nvidia-code-samples-and-docs.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Tue, 10 Nov 2020 09:30:00 GMT</pubDate></item><item><title><![CDATA[Getting started with NVEnc]]></title><description><![CDATA[<p>This blog entry is part of my &quot;<a href="/entries/blogging-the-mundane.html">blog about mundane stuff</a>&quot; series.</p>
<p>Mundane blog posts here we go! I recently got asked about adding NVIDIA capability to our stack and that is a process I went through and completed with only moderate frustration. I don&#39;t tend to write very much C in my day to day job (This is by design, nobody wants me writing C and I don&#39;t want to be writing C) but here we are writing C because that&#39;s the easiest way to integrate native stuff with Erlang and I&#39;m not about to go and learn Rust just to play with an SDK that we may not end up using in production. I will go and learn Rust in 2021 though, Rust is where our native stuff is heading but I digress - thisis the life of a developer working in this sort of environment - putting off learning new things until it becomes strictly necessary or we&#39;d  never get any bloody work done.</p>
<p>We&#39;ll ignore the Erlang bit for the most part, because I could probably write several blog posts on my experience of writing NIFs (badly) alone, and we&#39;ll just take a quick overview of how the NVIDIA stuff fits together because coming to this task entirely from scratch with no knowledge of how the various bits fit together made set up that little bit more interesting.</p>
<h1 id="my-os">My OS</h1>
<p>I run Nixos, my entire system including drivers, software packages, configuration and such exist on Github in a <a href="https://github.com/robashton/nixos-install">repository</a> and it makes set up on new hardware or re-paving from scratch on existing hardware a very simply task of cloning a repo and running it against that hardware.</p>
<p>The hardest thing about the setup of this laptop with the Nvidia chip on it was disabling the GPU in the first place entirely so to squeeze as much battery life out of it as possible whilst sitting in bars at conferences and such and writing code until somebody wants to chat to me. (I actually paved this laptop whilst sat at a conference in Lithuania, that seems a different world now..). There is a perfectly good integrated Intel GPU (that also does hardware accelerated encodes/decodes too!) that I&#39;ve been using for the lifetime of his hardware (and we use this stuff in production so it makes sense to use it locally).</p>
<p>The first thing I needed to do was update the kernel to latest and survey what the state of Nvidia drivers/etc is in this world having not looked at it for a couple of years. The answer is &#39;in a state of flux&#39; (<em>when isn&#39;t it?</em>), but it looks like we&#39;ve finally got sensible offloading of the GPU built in and in theory we can run an X session off-screen for doing rendering and such on that GPU without draining the battery just because I was silly enough to open vim inside a terminal emulator. </p>
<p>The support for the encode/decode functionality is built into these drivers in the form of dynamically loaded libraries and they need to be set up properly before anything will work. On Windows this probably just means running an EXE, on MacOS it probably means dragging an icon onto another icon and on Nixos I need configure the drivers with the information of where to find the hardware and how I want it to operate.</p>
<p>I had been lying to ACPI to get my touchpad working with Nvidia disabled, but now comes the time to tell the truth once more..</p>
<pre><code>    <span class="hljs-attr">boot.kernelParams</span> = [ <span class="hljs-string">&quot;acpi_osi=Linux&quot;</span> ]<span class="hljs-comment">;</span></code></pre><p>Hey NVIDIA, this is where my hardware is (located using lspci)</p>
<pre><code>  <span class="hljs-attr">hardware.nvidia.prime.nvidiaBusId</span> = <span class="hljs-string">&quot;PCI:1:0:0&quot;</span><span class="hljs-comment">;</span>
  <span class="hljs-attr">hardware.nvidia.prime.intelBusId</span> = <span class="hljs-string">&quot;PCI:0:2:0&quot;</span><span class="hljs-comment">;</span></code></pre><p>And I want the official drivers pls, blacklist nouveau - don&#39;t even think about it pal.</p>
<pre><code>  <span class="hljs-attr">hardware.nvidia.modesetting.enable</span> = <span class="hljs-literal">true</span><span class="hljs-comment">;</span>
  <span class="hljs-attr">hardware.nvidia.prime.offload.enable</span> = <span class="hljs-literal">true</span><span class="hljs-comment">;</span>
  <span class="hljs-attr">hardware.nvidia.nvidiaPersistenced</span> = <span class="hljs-literal">true</span><span class="hljs-comment">;</span>

  <span class="hljs-attr">services.xserver.videoDrivers</span> = [ <span class="hljs-string">&quot;nvidia&quot;</span> ]<span class="hljs-comment">;</span>
  <span class="hljs-attr">boot.kernelModules</span> = [ <span class="hljs-string">&quot;nvidia-uvm&quot;</span> <span class="hljs-string">&quot;nvidia-drm&quot;</span> ]<span class="hljs-comment">;</span>
  <span class="hljs-attr">boot.blacklistedKernelModules</span> = [ <span class="hljs-string">&quot;nouveau&quot;</span> ]<span class="hljs-comment">;</span></code></pre><p>And also the X11 packages for this stuff</p>
<pre><code>  environment.systemPackages = with pkgs<span class="hljs-comment">; [</span>
    linuxPackages.nvidia_x11
  ]<span class="hljs-comment">;</span></code></pre><p>And may as well get the opengl stuff set up while we&#39;re here, although it&#39;s not strictly useful for the encodes it&#39;s good for testing the hardware itself.</p>
<pre><code>  hardware.<span class="hljs-attr">opengl</span> = {
    <span class="hljs-attr">enable</span> = <span class="hljs-literal">true</span>;
    <span class="hljs-attr">driSupport</span> = <span class="hljs-literal">true</span>;
  }</code></pre><p>I also follow the official guidance in setting up a quick bash script to run things with the Nvidia GPU instead of the default Intel one.</p>
<p>Having a Nixos setup is quite nice, a quick rebuild and I have a new boot option to start up with all of this enabled (and the old option is still there in case I got anything wrong, which I definitely did in my first few passes here).</p>
<p>Anyway, this gives me a few things..</p>
<ul>
<li><em>glxgears</em>:  Woo, spinny gears on my laptop powered by Intel</li>
<li><em>nvidia-offload glxgears</em>: Woo, spinny gears powered by Nvidia</li>
<li><em>/run/opengl-driver/lib/libnvidia-encode.so</em>: Library for doing encode stuff</li>
<li><em>/run/opengl-driver/lib/dri/nvidia_drv_video.so</em>: The actual video driver</li>
</ul>
<p>Happy this is all setup, I need to look next at the tools required to write code that uses these things...</p>
<h1 id="the-project-stuff">The project stuff</h1>
<p>I don&#39;t tend to install SDKs or even development tools and such in my global environment, it nearly always ends up being the case that I need a different version of something for one project or another and because I&#39;m on Nixos I just use Nix shells for the individual projects and their development requirements <a href="https://purerl-cookbook.readthedocs.io/en/latest/devenv/nix.html">(Something I have written a bit about)</a></p>
<p>It turns out I just need to add these packages to my environment and I&#39;m good to go, I already have nvidia_x11, but I &#39;add&#39; it again here so I can use it to generate some environment variables later on in the shell.nix so everything lines up.</p>
<pre><code>
  <span class="hljs-attribute">cudatoolkit</span>
  <span class="hljs-attribute">nvidia</span>-video-sdk
  <span class="hljs-attribute">linuxPackages</span>.nvidia_x<span class="hljs-number">11</span>
</code></pre><p>As a bonus, I also go and add these because it&#39;ll make the code samples build and run properly (and as code samples typically seem to be the main entrance to SDKs like this that&#39;s a helpful thing.</p>
<pre><code><span class="hljs-code">    cmake
    pkgconfig
    gcc8
    ffmpeg-full
</span></code></pre><p>As a further bonus, I go and add <em>gdb</em> to this list because I literally don&#39;t have any development tools on my host OS and I&#39;m bound to cause a few SIGSEGVs over the next couple of weeks that need debugging.</p>
<p>The cuda toolkit doesn&#39;t (at time of writing) work with the modern version of GCC on my OS so I needed to explicitly pull GCC8 and set that in the environment so that when I&#39;m building code with our standard makefiles we&#39;ll do the right thing.</p>
<pre><code>  shellHook = <span class="hljs-string">&#x27;&#x27;</span>
    <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">CUDA_PATH</span>=<span class="hljs-variable">${pkgs.cudatoolkit}</span>
    <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">LD_LIBRARY_PATH</span>=<span class="hljs-variable">${pkgs.linuxPackages.nvidia_x11}</span>/lib
    <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">EXTRA_LDFLAGS</span>=<span class="hljs-string">&quot;-L/lib -L<span class="hljs-variable">${pkgs.linuxPackages.nvidia_x11}</span>/lib&quot;</span>
    <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">EXTRA_CCFLAGS</span>=<span class="hljs-string">&quot;-I/usr/include&quot;</span>
    <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">CC</span>=<span class="hljs-variable">${pkgs.gcc8}</span>/bin/gcc
    <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">CXX</span>=<span class="hljs-variable">${pkgs.gcc8}</span>/bin/g++
  <span class="hljs-string">&#x27;&#x27;</span>;</code></pre><p>Anyway, that&#39;s my setup - seeing as I&#39;ve made it to 1000 words already I&#39;ll leave the next post of &quot;exploring the samples, SDK surface area, documentation, etc&quot; to the next one. mundane <em>and</em> wordy, everybody&#39;s favourite.</p>
]]></description><link>http://codeofrob.com/entries/getting-started-with-nvenc.html</link><guid isPermaLink="true">http://codeofrob.com/entries/getting-started-with-nvenc.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Thu, 05 Nov 2020 09:30:00 GMT</pubDate></item><item><title><![CDATA[Blogging the mundane]]></title><description><![CDATA[<p>I was having a chat with a few friends over a glass or two of whisky the other evening (over Google Hangouts or whatever it is called these days) and the subject turned to whether any of us had become lazy about our online output as time had gone on and as we were getting old. (All of us really).</p>
<p><strong>&quot;Why don&#39;t you blog any more?&quot;</strong></p>
<p>My own personal answer is quite simply that I don&#39;t think anybody actually cares about anything I do these days, a vanishingly small number of people want to hear about anything to do with Erlang, an even smaller number of people have any interest in Purescript and the intersection of those two subjects doesn&#39;t do very much to increase the number of people willing to click a link with any of this content behind it. It&#39;s somewhat disheartening to write pages upon pages of documentation that nobody is going to care about until it&#39;s gotten so stale it&#39;s now useless. (See also: <a href="https://purerl-cookbook.readthedocs.io/en/latest/">The Purerl Cookbook</a>).</p>
<p>Conversations with folk at conferences on these subjects doesn&#39;t do much to boost my interest in sharing, you can literally see people glaze over the moment you mention anything that isn&#39;t</p>
<ul>
<li>A) YAML</li>
<li>B) Kubernetes</li>
<li>C) YAML and Kubernetes</li>
</ul>
<p>Outside of that, anything I might do tends to be pretty niche, have a high ramp-up cost (for me) and be quite specific to my day to day job of moving bytes from one place to another with some sort of transform in the way making the whole process a lot slower than it could be (Yes, that&#39;s literally all of our jobs). </p>
<p>Once I have invested days learning about something new (to me) I seldom feel the inclination of trying to write about it - even if I found it a challenge to learn myself in the first place because </p>
<ul>
<li>A) Nobody cares </li>
<li>B) I doubt anything I can write on the subject will help anybody further than existing materials have helped me </li>
</ul>
<p><em>A challenge then</em></p>
<p>Ignoring all of the above, I&#39;ll try to (between now and the end of the year) blog about some of the day to day mundane things &quot;wot I have done&quot;, even if the only person it helps is me in a year&#39;s time when I open code long since untouched and can&#39;t remember how any of it works. In the absence of conferences (these online replacements don&#39;t count because they don&#39;t have beanbags to chat to people on) this will perhaps be half a replacement for that experience (In that I get to talk, but not to listen, unless somebody wants to tweet me something interesting in response).</p>
<p>Have at you - the next blog post(s) will be about NVIDIA GPU based transcoding and such cos I recently wrote a driver for our own proprietary workflows to integrate with the GPU on my laptop and the documentation wasn&#39;t great but the results were pretty nifty indeed.</p>
]]></description><link>http://codeofrob.com/entries/blogging-the-mundane.html</link><guid isPermaLink="true">http://codeofrob.com/entries/blogging-the-mundane.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Tue, 03 Nov 2020 09:30:00 GMT</pubDate></item><item><title><![CDATA[Purerl Updates - Message Routing From Legacy Code]]></title><description><![CDATA[<p>We&#39;ve covered the essential upgrades to Pinto and Stetson, so now we&#39;ll cover a quick bonus topic which is one of the concepts thrown into Pinto to help with all of this work.</p>
<h1 id="previous-purerl-posts">Previous Purerl posts</h1>
<ul>
<li><a href="/entries/introducing-pinto-and-stetson---opinionated-purescript-bindings-to-otp-and-cowboy.html">Introduction to Pinto/Stetson - Opinionated Bindings to OTP/Cowboy</a></li>
<li><a href="/entries/the-structure-of-an-end-to-end-purescript-otp-project.html">The structure of an end-to-end purescript OTP project</a></li>
<li><a href="/entries/building-on-top-of-otp-with-purescript-with-pinto.html">Building on top of OTP with Purescript with Pinto</a></li>
<li><a href="/entries/building-a-purescript-web-server-with-stetson-and-pinto.html">Building a Purescript web server with Stetson and Pinto</a></li>
<li><a href="/entries/shared-code-twixt-purescript-server-and-client.html">Shared code twixt Purescript server and client</a></li>
<li><a href="/entries/purescript-interop-with-native-erlang---interacting-with-redis.html">Purescript interop with native Erlang, interaction with Redis</a></li>
</ul>
<h1 id="updates">Updates</h1>
<ul>
<li><a href="/entries/updates-to-pinto+stetson---purerl-in-progress.html">Nix overlays for Purerl/etc</a></li>
<li><a href="/entries/purerl-updates---typed-routes-in-stetson.html">Typed routing for Stetson</a></li>
<li><a href="/entries/purerl-updates---arbitrary-messages-and-handle_info-in-gen-servers.html">Arbitrary messages and handle_info in gen_servers</a></li>
<li><a href="/entries/purerl-updates---arbitrary-messages-and-stetson-handlers.html">Arbitrary messages and Stetson handlers</a></li>
<li><a href="/entries/purerl-updates---monitors-in-stetson-and-pinto.html">Monitors for arbitrary pids from Gen servers + Stetson handlers</a></li>
<li>MessageRouting in Pinto to easily bind to legacy code that sends us messages</li>
</ul>
<h1 id="the-problem">The problem</h1>
<p>A <em>lot</em> of legacy Erlang code (ours included) will have something along the lines of </p>
<pre><code class="language-erlang">
cool_api:do_something().
</code></pre>
<p>That behind the scenes will almost immediately do a call to <em>self()</em> to get the caller pid and then probably spin up some more processes and start sending messages back to us.</p>
<pre><code class="language-erlang">
  do_something() -&gt;
    Self = self(),
    spawn_link(<span class="hljs-keyword">fun</span> Fun() -&gt;
      <span class="hljs-keyword">receive</span>
        _ -&gt;  ok
      <span class="hljs-keyword">after</span> <span class="hljs-number">1000</span> -&gt;
         Self ! hi
         Fun()
      <span class="hljs-keyword">end</span>
    <span class="hljs-keyword">end</span>).
</code></pre>
<p>In this case, we&#39;ve got a native function called do_something() that captures the current pid, spins up a process which will stop if it receives anything and otherwise every second send a message back to the parent (hi).</p>
<p>If we were to write FFI for this, it&#39;d look a lot like this:</p>
<pre><code class="language-haskell">
<span class="hljs-keyword">foreign</span> <span class="hljs-keyword">import</span> doSomething :: <span class="hljs-type">Effect</span> <span class="hljs-type">Pid</span>
<span class="hljs-keyword">foreign</span> <span class="hljs-keyword">import</span> stop :: <span class="hljs-type">Effect</span> <span class="hljs-type">Unit</span>

</code></pre>
<pre><code class="language-erlang">
<span class="hljs-function"><span class="hljs-title">doSomething</span><span class="hljs-params">()</span> -&gt;</span>
  <span class="hljs-keyword">fun</span>() -&gt;
    cool_api:do_something()
  <span class="hljs-keyword">end</span>.

<span class="hljs-function"><span class="hljs-title">stop</span><span class="hljs-params">(Pid)</span> -&gt;</span>
  <span class="hljs-keyword">fun</span>() -&gt;
    Pid ! this_will_stop_you_cos_you_received_something
  <span class="hljs-keyword">end</span>.
</code></pre>
<p>We would immediately start receiving atoms of &#39;hi&#39; to the calling process, which unless we happen to be very specific and careful, won&#39;t know how to receive them, for example in a gen server.</p>
<pre><code class="language-haskell">
<span class="hljs-class"><span class="hljs-keyword">type</span> <span class="hljs-type">State</span> = {}</span>

<span class="hljs-title">serverName</span> :: <span class="hljs-type">ServerName</span> <span class="hljs-type">State</span> <span class="hljs-type">Atom</span>
<span class="hljs-title">serverName</span> = <span class="hljs-type">Local</span> $ atom <span class="hljs-string">&quot;listener&quot;</span>

<span class="hljs-title">startLink</span> :: <span class="hljs-type">Effect</span> <span class="hljs-type">StartLinkResult</span>
<span class="hljs-title">startLink</span> =
  <span class="hljs-type">Gen</span>.buildStartLink serverName init $ <span class="hljs-type">Gen</span>.defaultStartLink { handleInfo = handleInfo }

<span class="hljs-title">init</span> :: <span class="hljs-type">Gen</span>.<span class="hljs-type">Init</span> <span class="hljs-type">State</span> <span class="hljs-type">Atom</span>
<span class="hljs-title">init</span> args = <span class="hljs-keyword">do</span>
  <span class="hljs-type">Gen</span>.lift <span class="hljs-type">CoolApi</span>.doSomething
  pure $ {}

<span class="hljs-title">handleInfo</span> :: <span class="hljs-type">Atom</span> -&gt; <span class="hljs-type">State</span> -&gt; <span class="hljs-type">Gen</span>.<span class="hljs-type">HandleInfo</span> <span class="hljs-type">State</span> <span class="hljs-type">Atom</span>
<span class="hljs-title">handleInfo</span> msg state = <span class="hljs-keyword">do</span>
  <span class="hljs-comment">-- got an atom, woo</span>
  <span class="hljs-comment">-- not much to do with it</span>
  pure <span class="hljs-type">CastNoReply</span> state
</code></pre>
<p>This will work, it&#39;s a gen server that knows how to receive atoms - but it&#39;s unlikely we&#39;ll want to write a gen server that only receives atoms and nothing else - as soon as we add a timer, monitor or subscribe to anything else we&#39;ll want to change our message type into an ADT so that we can dispatch over the various message types.</p>
<pre><code class="language-haskell">
<span class="hljs-class"><span class="hljs-keyword">type</span> <span class="hljs-type">Msg</span> = <span class="hljs-type">CoolApiMsg</span> <span class="hljs-type">Atom</span></span>
         | <span class="hljs-type">Tick</span>
</code></pre>
<p>So we&#39;re going to need some way to map this. </p>
<p><em>The old way</em>: We&#39;d register a mapping function with the gen server that would recognise the cool_api messages and convert them into the right type for us, this was janky AF and has been deleted in the latest Pinto</p>
<p><em>the new way</em>: Proxy process that receives the message, translates it and then sends it on to the main process</p>
<p>That proxy process is a burden to create because if we start spawning processes in Erlang, we need to make sure we monitor the parent so we terminate when it does, yada yada yada what a mess. Thankfully this is what <em>MessageRouter</em> in Pinto is for, it neatly wraps up this common pattern safely so we don&#39;t have to.</p>
<h1 id="pintomessagerouter">Pinto.MessageRouter</h1>
<p>The message router exports three functions of interest, one of which invokes a router on top of a process that will always start, and one on top of a process that might fail, the third takes a RouterRef (returned on success) and terminates the router.</p>
<pre><code class="language-haskell">
    startRouter :: <span class="hljs-keyword">forall</span> handle msg. <span class="hljs-type">Effect</span> handle -&gt; (handle -&gt; <span class="hljs-type">Effect</span> <span class="hljs-type">Unit</span>) -&gt; (msg -&gt; <span class="hljs-type">Effect</span> <span class="hljs-type">Unit</span>) -&gt;  <span class="hljs-type">Effect</span> (<span class="hljs-type">RouterRef</span> handle)

    maybeStartRouter = maybeStartRouterImpl <span class="hljs-type">RouterRef</span>

    stopRouter  :: <span class="hljs-keyword">forall</span> handle. <span class="hljs-type">RouterRef</span> handle -&gt; <span class="hljs-type">Effect</span> <span class="hljs-type">Unit</span>
</code></pre>
<p>We&#39;ll focus on the simple case. </p>
<ul>
<li>Given an <em>Effect handle</em> - ie something that returns some reference to whatever is created (in our case a pid)</li>
<li>Given a function that given that handle, terminates the process</li>
<li>Given a callback that takes &#39;whatever is received&#39; and &#39;does something to it&#39; (<em>Effect Unit</em>)</li>
<li>We&#39;ll get an <em>Effect</em> of <em>(RouterRef handle)</em> back (which we can hold onto in order to terminate the whole show by calling stopRouter)</li>
</ul>
<p>Wrapping our legacy API is &quot;simple&quot; now that we&#39;ve already written the FFI for it</p>
<pre><code class="language-haskell">
<span class="hljs-keyword">import</span> Pinto.MessageRouting <span class="hljs-keyword">as</span> MR

<span class="hljs-title">wrappedDoSomething</span> :: <span class="hljs-keyword">forall</span>. (<span class="hljs-type">Atom</span> -&gt; <span class="hljs-type">Effect</span> <span class="hljs-type">Unit</span>) -&gt; <span class="hljs-type">Effect</span> (<span class="hljs-type">MR</span>.<span class="hljs-type">RouterRef</span> <span class="hljs-type">Pid</span>)
<span class="hljs-title">wrappedDoSomething</span> recv = <span class="hljs-type">MR</span>.startRouter <span class="hljs-type">CoolApi</span>.doSomething <span class="hljs-type">CoolApi</span>.stop recv

</code></pre>
<p>With this, we can re-write our gen server with the message lifted into the appropriate type</p>
<pre><code class="language-haskell">
<span class="hljs-class"><span class="hljs-keyword">type</span> <span class="hljs-type">State</span> = {}</span>
<span class="hljs-class"><span class="hljs-keyword">data</span> <span class="hljs-type">Msg</span> = <span class="hljs-type">Tick</span> | <span class="hljs-type">DoSomething</span> <span class="hljs-type">Atom</span></span>

<span class="hljs-title">serverName</span> :: <span class="hljs-type">ServerName</span> <span class="hljs-type">State</span> <span class="hljs-type">Msg</span>
<span class="hljs-title">serverName</span> = <span class="hljs-type">Local</span> $ atom <span class="hljs-string">&quot;listener&quot;</span>

<span class="hljs-title">startLink</span> :: <span class="hljs-type">Effect</span> <span class="hljs-type">StartLinkResult</span>
<span class="hljs-title">startLink</span> =
  <span class="hljs-type">Gen</span>.buildStartLink serverName init $ <span class="hljs-type">Gen</span>.defaultStartLink { handleInfo = handleInfo }

<span class="hljs-title">init</span> :: <span class="hljs-type">Gen</span>.<span class="hljs-type">Init</span> <span class="hljs-type">State</span> <span class="hljs-type">Msg</span>
<span class="hljs-title">init</span> args = <span class="hljs-keyword">do</span>
  self &lt;- <span class="hljs-type">Gen</span>.self
  <span class="hljs-type">Gen</span>.lift <span class="hljs-type">Wrapper</span>.wrappedDoSomething $ send self &lt;&lt;&lt; <span class="hljs-type">DoSomething</span>
  pure $ {}

<span class="hljs-title">handleInfo</span> :: <span class="hljs-type">Msg</span> -&gt; <span class="hljs-type">State</span> -&gt; <span class="hljs-type">Gen</span>.<span class="hljs-type">HandleInfo</span> <span class="hljs-type">State</span> <span class="hljs-type">Atom</span>
<span class="hljs-title">handleInfo</span> msg state = <span class="hljs-keyword">do</span>
  <span class="hljs-keyword">case</span> msg <span class="hljs-keyword">of</span>
    <span class="hljs-type">Tick</span> -&gt; ...
    <span class="hljs-type">DoSomething</span> msg -&gt; ...
</code></pre>
<p>It means an extra process per router, so isn&#39;t something we want to be using if we&#39;re going to be spinning up 1000s of short lived versions of it, but for that sort of thing we&#39;re in specialist territory where we&#39;d be using a look up table or dropping to plain ol&#39; Erlang. (See also <em>Pinto.Timer</em> which just uses the underlying mechanisms to send messages of the right type directly without an intermediary process.</p>
<p>The point is that wrapping up legacy code that sends us arbitrary messages has been turned into a relatively small amount of work as a result of these changes, so long as we supply a start function and a stop function and a callback that knows what to do with the messages we can transform and then send accordingly. This has been used across our codebases with great success (as well as in Pinto itself) and has enabled our gen servers and web handlers to remain clean and receive the right typed messages.</p>
]]></description><link>http://codeofrob.com/entries/purerl-updates---message-routing-from-legacy-code.html</link><guid isPermaLink="true">http://codeofrob.com/entries/purerl-updates---message-routing-from-legacy-code.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Tue, 14 Jul 2020 09:30:00 GMT</pubDate></item><item><title><![CDATA[Purerl Updates - Monitors in Stetson and Pinto]]></title><description><![CDATA[<p>We&#39;ve managed to get nicely typed arbitrary messages into our web handlers and gen servers, now it&#39;s time to look at Monitors.</p>
<h1 id="previous-purerl-posts">Previous Purerl posts</h1>
<ul>
<li><a href="/entries/introducing-pinto-and-stetson---opinionated-purescript-bindings-to-otp-and-cowboy.html">Introduction to Pinto/Stetson - Opinionated Bindings to OTP/Cowboy</a></li>
<li><a href="/entries/the-structure-of-an-end-to-end-purescript-otp-project.html">The structure of an end-to-end purescript OTP project</a></li>
<li><a href="/entries/building-on-top-of-otp-with-purescript-with-pinto.html">Building on top of OTP with Purescript with Pinto</a></li>
<li><a href="/entries/building-a-purescript-web-server-with-stetson-and-pinto.html">Building a Purescript web server with Stetson and Pinto</a></li>
<li><a href="/entries/shared-code-twixt-purescript-server-and-client.html">Shared code twixt Purescript server and client</a></li>
<li><a href="/entries/purescript-interop-with-native-erlang---interacting-with-redis.html">Purescript interop with native Erlang, interaction with Redis</a></li>
</ul>
<h1 id="updates">Updates</h1>
<ul>
<li><a href="/entries/updates-to-pinto+stetson---purerl-in-progress.html">Nix overlays for Purerl/etc</a></li>
<li><a href="/entries/purerl-updates---typed-routes-in-stetson.html">Typed routing for Stetson</a></li>
<li><a href="/entries/purerl-updates---arbitrary-messages-and-handle_info-in-gen-servers.html">Arbitrary messages and handle_info in gen_servers</a></li>
<li><a href="/entries/purerl-updates---arbitrary-messages-and-stetson-handlers.html">Arbitrary messages and Stetson handlers</a></li>
<li>Monitors for arbitrary pids from Gen servers + Stetson handlers</li>
<li>MessageRouting in Pinto to easily bind to legacy code that sends us messages</li>
</ul>
<h1 id="monitors">Monitors</h1>
<p>A reasonably common pattern for monitors in some of our code is</p>
<ul>
<li>Internal server manages a collection of pids that are recipients of data streams</li>
<li>web handlers open and register with this server to receive those data streams</li>
</ul>
<p>In this case <em>sometimes</em></p>
<ul>
<li>It&#39;s useful for the internal server to monitor the subscribers and remove the pids when they become invalid</li>
<li>It&#39;s useful for the web handler to monitor the server, so it can close the connection if that goes away</li>
</ul>
<p>This isn&#39;t always the case, sometimes a static message bus is a better option, sometimes pids can be checked ad-hoc, but for the purpose of this example we&#39;ll assume that this is exactly what we want as it&#39;ll be a nice end-to-end example of message passing and monitoring in Purerl.</p>
<h1 id="the-internal-server">The internal server</h1>
<p>So we&#39;ll define a basic gen server that keeps a state that&#39;s a map of pids to functions that receive data <em>(Binary -&gt; Effect Unit)</em>, and set up a timer to send us a <em>Tick</em> message after 500ms - our message type will therefore just be either that <em>Tick</em> message, or a message telling us that a client has disconnected. We&#39;ll configure the gen server to use a <em>handleInfo</em> function when these come in (explored further below).</p>
<pre><code class="language-haskell">
<span class="hljs-class"><span class="hljs-keyword">type</span> <span class="hljs-type">State</span> = {
  <span class="hljs-title">handlers</span> :: <span class="hljs-type">Map</span>.<span class="hljs-type">Map</span> <span class="hljs-type">Pid</span> <span class="hljs-type">MessageHandler</span>
}</span>

<span class="hljs-class"><span class="hljs-keyword">type</span> <span class="hljs-type">MessageHandler</span> = (<span class="hljs-type">Binary</span> -&gt; <span class="hljs-type">Effect</span> <span class="hljs-type">Unit</span>)</span>

<span class="hljs-class"><span class="hljs-keyword">data</span> <span class="hljs-type">Msg</span> = <span class="hljs-type">ClientDisconnected</span> <span class="hljs-type">Pid</span></span>
         | <span class="hljs-type">Tick</span>

<span class="hljs-title">startLink</span> :: <span class="hljs-type">BookWatchingStartArgs</span> -&gt; <span class="hljs-type">Effect</span> <span class="hljs-type">StartLinkResult</span>
<span class="hljs-title">startLink</span> args =
  <span class="hljs-type">Gen</span>.buildStartLink serverName (init args) $ <span class="hljs-type">Gen</span>.defaultStartLink { handleInfo = handleInfo }

<span class="hljs-title">init</span> :: <span class="hljs-type">BookWatchingStartArgs</span> -&gt; <span class="hljs-type">Gen</span>.<span class="hljs-type">Init</span> <span class="hljs-type">State</span> <span class="hljs-type">Msg</span>
<span class="hljs-title">init</span> args = <span class="hljs-keyword">do</span>
  self &lt;- <span class="hljs-type">Gen</span>.self
  void $ <span class="hljs-type">Gen</span>.lift $ <span class="hljs-type">Timer</span>.sendAfter <span class="hljs-number">500</span> <span class="hljs-type">Tick</span> self
  pure $ {
    handlers: <span class="hljs-type">Map</span>.empty
  }
</code></pre>
<p>We can export a function <em>registerClient</em> for clients to invoke in order to start receiving data, while we&#39;re still in the process that called us we can get its pid by calling out to &#39;<em>Pinto.self</em>&#39;, and then in the context of the gen server, we&#39;ll get our own pid so we can add the monitor in the next function <em>addHandler</em>.</p>
<p>As we have the pid of our calling process, we can invoke <em>Monitor.pid</em>, and pass in a callback that disregards the message given to us when the monitor pops and just sends a message with the handler pid back to our <em>handleInfo</em>. Once we&#39;re monitoring the handler, we can add it to our map using the pid as a key so we can easily remove it later when we get the message telling us it went down.</p>
<pre><code class="language-haskell">
<span class="hljs-title">registerClient</span> :: <span class="hljs-type">MessageHandler</span> -&gt; <span class="hljs-type">Effect</span> <span class="hljs-type">Unit</span>
<span class="hljs-title">registerClient</span> handler = <span class="hljs-keyword">do</span>
  handlerPid &lt;- <span class="hljs-type">Pinto</span>.self
  <span class="hljs-type">Gen</span>.doCall serverName \state -&gt; <span class="hljs-keyword">do</span>
     self &lt;- <span class="hljs-type">Gen</span>.self
     newState &lt;- <span class="hljs-type">Gen</span>.lift $ addHandler handler self handlerPid state
     pure $ <span class="hljs-type">CallReply</span> unit newState

<span class="hljs-title">addHandler</span> :: <span class="hljs-type">MessageHandler</span> -&gt; <span class="hljs-type">Process</span> <span class="hljs-type">Msg</span> -&gt; <span class="hljs-type">Pid</span> -&gt; <span class="hljs-type">State</span> -&gt; <span class="hljs-type">Effect</span> <span class="hljs-type">State</span>
<span class="hljs-title">addHandler</span> handler self handlerPid state@{ handlers } = <span class="hljs-keyword">do</span>
  void $ <span class="hljs-type">Logger</span>.info1 <span class="hljs-string">&quot;Adding handler ~p as it has connected&quot;</span> handlerPid
  void $ <span class="hljs-type">Monitor</span>.pid handlerPid (\_ -&gt; self ! <span class="hljs-type">ClientDisconnected</span> handlerPid)
  pure $ state { handlers = <span class="hljs-type">Map</span>.insert handlerPid handler handlers }
</code></pre>
<p>All that is left therefore, is to handle the messages we might receive into the handleInfo we configured earlier as part of startLink. </p>
<ul>
<li>If we get a ClientDisconnected, we simply remove the handler from our map so we no longer send any data to it</li>
<li>If we get a Tick, we invoke sendData on the map of handlers, before scheduling another tick for 500ms time</li>
</ul>
<pre><code class="language-haskell">
<span class="hljs-title">handleInfo</span> :: <span class="hljs-type">Msg</span> -&gt; <span class="hljs-type">State</span> -&gt; <span class="hljs-type">Gen</span>.<span class="hljs-type">HandleInfo</span> <span class="hljs-type">State</span> <span class="hljs-type">Msg</span>
<span class="hljs-title">handleInfo</span> msg state@{ handlers  } = <span class="hljs-keyword">do</span>
  <span class="hljs-keyword">case</span> msg <span class="hljs-keyword">of</span>
     <span class="hljs-type">ClientDisconnected</span> handlerPid -&gt; <span class="hljs-keyword">do</span>
        void $ <span class="hljs-type">Gen</span>.lift $ <span class="hljs-type">Logger</span>.info1 <span class="hljs-string">&quot;Removing ~p as it disconnected&quot;</span> handlerPid
        pure $ <span class="hljs-type">CastNoReply</span> $ state { handlers = <span class="hljs-type">Map</span>.delete handlerPid handlers }
     <span class="hljs-type">Tick</span> -&gt; <span class="hljs-keyword">do</span>
        <span class="hljs-type">Gen</span>.lift $ sendData handlers
        self &lt;- <span class="hljs-type">Gen</span>.self
        void $ <span class="hljs-type">Gen</span>.lift $ <span class="hljs-type">Timer</span>.sendAfter <span class="hljs-number">500</span> <span class="hljs-type">Tick</span> self
        pure $ <span class="hljs-type">CastNoReply</span> $ state 
</code></pre>
<p>Sending data is easy, seeing as the handlers are just a list of effectful callbacks of <em>(Binary -&gt; Effect Unit)</em></p>
<pre><code class="language-haskell">
<span class="hljs-title">sendData</span> :: <span class="hljs-type">Map</span>.<span class="hljs-type">Map</span> <span class="hljs-type">Pid</span> <span class="hljs-type">MessageHandler</span> -&gt; <span class="hljs-type">Effect</span> <span class="hljs-type">Unit</span>
<span class="hljs-title">sendData</span> handlers = <span class="hljs-keyword">do</span>
  freshData &lt;- getDataFromSomeNativeCode
  void $ traverse (\handler -&gt; <span class="hljs-keyword">do</span> handler freshData) $ <span class="hljs-type">Map</span>.values handlers 
  pure unit
</code></pre>
<p>So that&#39;s an entire gen server, which </p>
<ul>
<li>Allows the registration of callbacks that&#39;ll accept data</li>
<li>Monitors the pids of the calling process, and removes the callbacks when the monitor pops</li>
<li>Ticks every 500ms and traverses over the callbacks to send the data</li>
</ul>
<p>Note: Because of the callback/pids there is nothing stopping us using this code from any other Purerl (or indeed Erlang). None of this is Pinto specific and this is very much by design.</p>
<h1 id="subscribing-and-monitoring-from-stetson">Subscribing (and monitoring) from Stetson</h1>
<p>Speaking of other Purerl, a lot of Purerl gets written using Stetson to support Rest/Websockets/Streams/etc; so that&#39;s where we&#39;ll subscribe to this data. We&#39;ll also then add a monitor to that streaming process that closes the connection when it goes away. (We could also just block while we wait for it to restart for example).</p>
<p>So, we&#39;ll define a data type for our messages, we&#39;re either receiving data that needs to be streamed, our data source died, or our data source was already down when we tried to connect to it.</p>
<pre><code class="language-haskell">
<span class="hljs-class"><span class="hljs-keyword">data</span> <span class="hljs-type">DataStreamMessage</span> = <span class="hljs-type">Data</span> <span class="hljs-type">Binary</span></span>
                       | <span class="hljs-type">DataSourceDied</span>
                       | <span class="hljs-type">DataSourceAlreadyDown</span>

</code></pre>
<p>We&#39;ll just kick off our handler with <em>Loop.handler</em>, start a streamed reply with a status code 200 and make sure that Stetson knows we&#39;re doing a Loop, we&#39;re typed as a <em>StetsonHandler DataStreamMessage Unit</em> because we receive <em>DataSteamMessage</em> and don&#39;t store any state of our own.</p>
<pre><code class="language-haskell">
<span class="hljs-title">dataStream</span> :: <span class="hljs-type">StetsonHandler</span> <span class="hljs-type">DataStreamMessage</span> <span class="hljs-type">Unit</span>
<span class="hljs-title">dataStream</span> =
  <span class="hljs-type">Loop</span>.handler (\req -&gt; <span class="hljs-keyword">do</span>
               req2 &lt;- streamReply (<span class="hljs-type">StatusCode</span> <span class="hljs-number">200</span>) <span class="hljs-type">Map</span>.empty req
               <span class="hljs-type">Loop</span>.initResult req2 unit)
</code></pre>
<p>In our <em>Loop.init</em>, we&#39;ll get our own typed process <em>(Process DataStreamMessage)</em>, invoking &#39;<em>Process.send</em>&#39; on this gives us a function of type (Msg -&gt; Effect Unit) so we&#39;ll compose that with a constructor for our own data type (Data) giving us the correct function type of <em>(Binary -&gt; Effect Unit)</em></p>
<p>Using <em>Gen.monitor</em> with the server name of <em>MonitorExample</em>, we can detect when that process dies - there are two effectful callbacks for this, one for when the process dies and one for if the process is already down (there is no pid to monitor). </p>
<pre><code class="language-haskell">
    # <span class="hljs-type">Loop</span>.init (\req state -&gt; <span class="hljs-keyword">do</span> 
                      self &lt;- <span class="hljs-type">Loop</span>.self
                      void $ <span class="hljs-type">Loop</span>.lift $ <span class="hljs-type">MonitorExample</span>.registerClient $ send self &lt;&lt;&lt; <span class="hljs-type">Data</span>
                      void $ <span class="hljs-type">Loop</span>.lift $ <span class="hljs-type">Gen</span>.monitor <span class="hljs-type">MonitorExample</span>.serverName (\_ -&gt; send self <span class="hljs-type">DataSourceDied</span>) (send self <span class="hljs-type">DataSourceAlreadyDown</span>)
                      pure unit)
</code></pre>
<p>We receive those messages in the <em>Loop.info</em> callback</p>
<ul>
<li>if we get data then we can stream that directly to the client and carry on looping</li>
<li>if the data source died then we unceremoniously terminate the stream</li>
<li>if the data source is already down then we unceremoniously terminate the stream</li>
</ul>
<pre><code class="language-haskell">    # <span class="hljs-type">Loop</span>.info (\msg req state -&gt;  <span class="hljs-keyword">do</span>
                <span class="hljs-keyword">case</span> msg <span class="hljs-keyword">of</span>
                     <span class="hljs-type">Data</span> binary -&gt; <span class="hljs-keyword">do</span>
                        _ &lt;- <span class="hljs-type">Loop</span>.lift $ streamBody binary req
                        pure $ <span class="hljs-type">LoopOk</span> req state

                     <span class="hljs-type">DataSourceDied</span> -&gt;  <span class="hljs-keyword">do</span>
                       pure $ <span class="hljs-type">LoopStop</span> req state

                     <span class="hljs-type">DataSourceAlreadyDown</span> -&gt;  <span class="hljs-keyword">do</span>
                       pure $ <span class="hljs-type">LoopStop</span> req state
</code></pre>
<p>And that&#39;s that, we have a gen server running which will send data to any subscribers and clean up when those subscribers terminate, and we have a loop handler that&#39;ll subcribe to that data source and clean up if that data source dies. There is a lot going on here but it&#39;s worth unpicking as there are a lot of useful concepts here neatly packaged into a single example.</p>
<p>By sticking to plain ol&#39; pids and callbacks, all of this code remains portable and not tied to either of these libraries, which is handy because at some point somebody smarter than I will write something more Purerl idiomatic for web serving and OTP wrapping and we&#39;ll probably want to switch to those things..</p>
]]></description><link>http://codeofrob.com/entries/purerl-updates---monitors-in-stetson-and-pinto.html</link><guid isPermaLink="true">http://codeofrob.com/entries/purerl-updates---monitors-in-stetson-and-pinto.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Thu, 09 Jul 2020 09:30:00 GMT</pubDate></item><item><title><![CDATA[Purerl Updates - Arbitrary messages and Stetson handlers]]></title><description><![CDATA[<p>An extra post was required on this topic..</p>
<h1 id="previous-purerl-posts">Previous Purerl posts</h1>
<ul>
<li><a href="/entries/introducing-pinto-and-stetson---opinionated-purescript-bindings-to-otp-and-cowboy.html">Introduction to Pinto/Stetson - Opinionated Bindings to OTP/Cowboy</a></li>
<li><a href="/entries/the-structure-of-an-end-to-end-purescript-otp-project.html">The structure of an end-to-end purescript OTP project</a></li>
<li><a href="/entries/building-on-top-of-otp-with-purescript-with-pinto.html">Building on top of OTP with Purescript with Pinto</a></li>
<li><a href="/entries/building-a-purescript-web-server-with-stetson-and-pinto.html">Building a Purescript web server with Stetson and Pinto</a></li>
<li><a href="/entries/shared-code-twixt-purescript-server-and-client.html">Shared code twixt Purescript server and client</a></li>
<li><a href="/entries/purescript-interop-with-native-erlang---interacting-with-redis.html">Purescript interop with native Erlang, interaction with Redis</a></li>
</ul>
<h1 id="updates">Updates</h1>
<ul>
<li><a href="/entries/updates-to-pinto+stetson---purerl-in-progress.html">Nix overlays for Purerl/etc</a></li>
<li><a href="/entries/purerl-updates---typed-routes-in-stetson.html">Typed routing for Stetson</a></li>
<li><a href="/entries/purerl-updates---arbitrary-messages-and-handle_info-in-gen-servers.html">Arbitrary messages and handle_info in gen_servers</a></li>
<li>Arbitrary messages and Stetson handlers</li>
<li>Monitors for arbitrary pids from Gen servers + Stetson handlers</li>
<li>MessageRouting in Pinto to easily bind to legacy code that sends us messages</li>
</ul>
<h1 id="the-story-so-far">The story so far</h1>
<p>Stetson was thrown together at the same time as Pinto to enable me to start building real products in Purerl and I didn&#39;t have a lot of use for websockets at that time. When the need occurred in a client project, I added a new handler (WebSocketHandler) to Stetson to deal with this with an &#39;emitter&#39; function as part of the &#39;init&#39; callback and got on with my life.</p>
<pre><code class="language-haskell">
<span class="hljs-title">busEvents</span> :: <span class="hljs-type">ReceivingStetsonHandler</span> <span class="hljs-type">ExternalMessage</span> <span class="hljs-type">Unit</span>
<span class="hljs-title">busEvents</span> =

  <span class="hljs-type">WebSocket</span>.handler (\req -&gt; <span class="hljs-type">WebSocket</span>.initResult req unit)
  # <span class="hljs-type">WebSocket</span>.init (\emitter s -&gt;  <span class="hljs-keyword">do</span>
                             <span class="hljs-type">Bus</span>.callback msg <span class="hljs-type">ExternalMessages</span>.bus $ emitter &lt;&lt;&lt; <span class="hljs-type">ExternalMsg</span>
                             pure $ <span class="hljs-type">NoReply</span> s
                             )
  # <span class="hljs-type">WebSocket</span>.handle (\msg state -&gt; pure $ <span class="hljs-type">NoReply</span> state)
  # <span class="hljs-type">WebSocket</span>.info (\msg state -&gt; pure $ <span class="hljs-type">Reply</span> ((<span class="hljs-type">TextFrame</span> $ writeJSON msg) : nil) state)
  # <span class="hljs-type">WebSocket</span>.yeeha
</code></pre>
<p>While I was &quot;getting on with my life&quot;, people were writing code on top of this, and a pull request came into Stetson to add a mapper for arbitrary messages being received into the process that I accepted without a second thought. </p>
<pre><code class="language-haskell">

<span class="hljs-title">nativeMapper</span> :: <span class="hljs-keyword">forall</span> msg. msg -&gt; <span class="hljs-type">ExternalMessage</span>

<span class="hljs-title">_</span> &lt;- (<span class="hljs-type">Stetson</span>.registerMapper $ nativeMapper <span class="hljs-type">SomeConstructor</span> <span class="hljs-type">SomeOtherConstructor</span>)
<span class="hljs-title">_</span> &lt;- subscribeToNativeEvents
</code></pre>
<p>At some point a month ago, I was asked about the Loop handler and streaming in Stetson by a colleague, I gazed apon the abomination that was external mapping and realised that it was time to do a proper job of unifying these handlers (a single type for all of them, meaning the death of &#39;yeeha&#39; sadly), deleting the ability to register external mappers and providing the ability to switch from a Rest handler into a Loop handler as part of content negotiation.  The actual means of doing this isn&#39;t worth covering in this post, but the end result is that we now had three namespaces for building handlers over the top of a single record and a standard pattern of being supplied an &#39;emitter&#39; function in the init callback for Loop and WebSocket for sending typed messages into the handler.</p>
<h1 id="straight-rest">Straight Rest</h1>
<pre><code class="language-haskell">
<span class="hljs-title">rest</span> :: <span class="hljs-type">StetsonHandler</span> {}
<span class="hljs-type">Rest</span>.handler (\req -&gt; <span class="hljs-type">Rest</span>.initResult req {})
  # <span class="hljs-type">Rest</span>.serviceAvailable (\req s -&gt; <span class="hljs-type">Rest</span>.result true req s)
    # <span class="hljs-type">Rest</span>.allowedMethods (\req url -&gt; <span class="hljs-type">Rest</span>.result (<span class="hljs-type">Stetson</span>.<span class="hljs-type">HEAD</span> : <span class="hljs-type">Stetson</span>.<span class="hljs-type">GET</span> : <span class="hljs-type">Stetson</span>.<span class="hljs-type">OPTIONS</span> : nil) req s)
    # <span class="hljs-type">Rest</span>.contentTypesProvided (\s url -&gt; <span class="hljs-type">Rest</span>.result (jsonWriter : nil) req s)
</code></pre>
<h1 id="rest-into-loop">Rest into Loop</h1>
<pre><code class="language-haskell">
<span class="hljs-title">eventsFirehoseRest</span> :: <span class="hljs-type">StetsonHandler</span> <span class="hljs-type">EventsWsMsg</span> <span class="hljs-type">Unit</span>
<span class="hljs-title">eventsFirehoseRest</span> =
  <span class="hljs-type">Rest</span>.handler (\req -&gt; <span class="hljs-type">Rest</span>.initResult req unit)
    # <span class="hljs-type">Rest</span>.allowedMethods (\req state -&gt; <span class="hljs-type">Rest</span>.result (<span class="hljs-type">Stetson</span>.<span class="hljs-type">HEAD</span> : <span class="hljs-type">Stetson</span>.<span class="hljs-type">GET</span> : <span class="hljs-type">Stetson</span>.<span class="hljs-type">OPTIONS</span> : nil) req state)
    # <span class="hljs-type">Rest</span>.contentTypesProvided (\req state -&gt; <span class="hljs-type">Rest</span>.result (streamEvents : nil) req state)
    # <span class="hljs-type">Loop</span>.init (\emitter req state -&gt; <span class="hljs-keyword">do</span>
                              _ &lt;- <span class="hljs-type">SimpleBus</span>.subscribe <span class="hljs-type">BookLibrary</span>.bus $ <span class="hljs-type">BookMsg</span> &gt;&gt;&gt; emitter
                              pure state)
    # <span class="hljs-type">Loop</span>.info (\(<span class="hljs-type">BookMsg</span> msg) req state -&gt;  <span class="hljs-keyword">do</span>
          _ &lt;- streamBody (stringToBinary $ writeJSON msg) req
          pure $ <span class="hljs-type">LoopOk</span> req state)
    <span class="hljs-keyword">where</span>
          streamEvents = tuple2 <span class="hljs-string">&quot;application/json&quot;</span> (\req state -&gt; <span class="hljs-keyword">do</span>
                         req2 &lt;- streamReply (<span class="hljs-type">StatusCode</span> <span class="hljs-number">200</span>) <span class="hljs-type">Map</span>.empty req
                         <span class="hljs-type">Rest</span>.switchHandler <span class="hljs-type">LoopHandler</span> req2 state)
</code></pre>
<h1 id="straight-loop">Straight Loop</h1>
<pre><code class="language-haskell">
<span class="hljs-title">eventsFirehoseLoop</span> :: <span class="hljs-type">StetsonHandler</span> <span class="hljs-type">EventsWsMsg</span> {}
<span class="hljs-title">eventsFirehoseLoop</span> =
   <span class="hljs-type">Loop</span>.handler (\req -&gt; <span class="hljs-type">Loop</span>.initResult req {})
    # <span class="hljs-type">Loop</span>.init (\emitter req state -&gt; <span class="hljs-keyword">do</span>
                              _ &lt;- <span class="hljs-type">SimpleBus</span>.subscribe <span class="hljs-type">BookLibrary</span>.bus $ <span class="hljs-type">BookMsg</span> &gt;&gt;&gt; emitter
                              pure s{})
    # <span class="hljs-type">Loop</span>.info (\(<span class="hljs-type">BookMsg</span> msg) req s -&gt; <span class="hljs-keyword">do</span>
          _ &lt;- streamBody (stringToBinary $ writeJSON msg) req
          pure $ <span class="hljs-type">LoopOk</span> req s)
</code></pre>
<h1 id="websocket">WebSocket</h1>
<pre><code class="language-haskell">
<span class="hljs-title">eventsWs</span> :: <span class="hljs-type">StetsonHandler</span> <span class="hljs-type">EventsWsMsg</span> <span class="hljs-type">Unit</span>
<span class="hljs-title">eventsWs</span> =
  <span class="hljs-type">WebSocket</span>.handler (\req -&gt; <span class="hljs-type">WebSocket</span>.initResult req unit)
  # <span class="hljs-type">WebSocket</span>.init (\emitter req s -&gt;  <span class="hljs-keyword">do</span>
                              _ &lt;- <span class="hljs-type">SimpleBus</span>.subscribe <span class="hljs-type">BookLibrary</span>.bus $ <span class="hljs-type">BookMsg</span> &gt;&gt;&gt; emitter
                              pure $ <span class="hljs-type">Stetson</span>.<span class="hljs-type">NoReply</span> s
                             )
  # <span class="hljs-type">WebSocket</span>.handle (\frame state -&gt; pure $ <span class="hljs-type">Stetson</span>.<span class="hljs-type">NoReply</span> state)
  # <span class="hljs-type">WebSocket</span>.info (\(<span class="hljs-type">BookMsg</span> msg) state -&gt; pure $ <span class="hljs-type">Stetson</span>.<span class="hljs-type">Reply</span> ((<span class="hljs-type">TextFrame</span> $ writeJSON msg) : nil) state)
</code></pre>
<p>Similarly to the <a href="/entries/purerl-updates---arbitrary-messages-and-handle_info-in-gen-servers.html">last post</a>, doing this as an emitter function made sense on first pass, but this was swiftly replaced with a plain ol&#39; pid because it played nicer with monitors, existing APIs, etc.</p>
<h1 id="passing-in-a-pid-instead">Passing in a Pid instead</h1>
<pre><code class="language-haskell">
<span class="hljs-title">eventsWs</span> :: <span class="hljs-type">StetsonHandler</span> <span class="hljs-type">EventsWsMsg</span> <span class="hljs-type">Unit</span>
<span class="hljs-title">eventsWs</span> =
  <span class="hljs-type">WebSocket</span>.handler (\req -&gt; <span class="hljs-type">WebSocket</span>.initResult req unit)
  # <span class="hljs-type">WebSocket</span>.init (\self s -&gt;  <span class="hljs-keyword">do</span>
                              _ &lt;- <span class="hljs-type">SimpleBus</span>.subscribe <span class="hljs-type">BookLibrary</span>.bus $ <span class="hljs-type">BookMsg</span> &gt;&gt;&gt; send self
                              pure $ <span class="hljs-type">Stetson</span>.<span class="hljs-type">NoReply</span> s
                             )
  # <span class="hljs-type">WebSocket</span>.handle (\frame state -&gt; pure $ <span class="hljs-type">Stetson</span>.<span class="hljs-type">NoReply</span> state)
  # <span class="hljs-type">WebSocket</span>.info (\(<span class="hljs-type">BookMsg</span> msg) state -&gt; pure $ <span class="hljs-type">Stetson</span>.<span class="hljs-type">Reply</span> ((<span class="hljs-type">TextFrame</span> $ writeJSON msg) : nil) state)
</code></pre>
<p>This all said, requiring this pid to be passed in as an argument is quite awkward, having to stash it in state if we want to access it from outside of our init function etc, once again StateT was employed so that the API for Stetson and Pinto would be aligned.</p>
<pre><code class="language-haskell">
<span class="hljs-title">eventsWs</span> :: <span class="hljs-type">StetsonHandler</span> <span class="hljs-type">EventsWsMsg</span> <span class="hljs-type">Unit</span>
<span class="hljs-title">eventsWs</span> =
  <span class="hljs-type">WebSocket</span>.handler (\req -&gt; <span class="hljs-type">WebSocket</span>.initResult req unit)
  # <span class="hljs-type">WebSocket</span>.init (\req s -&gt;  <span class="hljs-keyword">do</span>
                              self &lt;- <span class="hljs-type">WebSocket</span>.self
                              _ &lt;- <span class="hljs-type">WebSocket</span>.lift $ <span class="hljs-type">SimpleBus</span>.subscribe <span class="hljs-type">BookLibrary</span>.bus $ <span class="hljs-type">BookMsg</span> &gt;&gt;&gt; send emitter
                              pure $ <span class="hljs-type">Stetson</span>.<span class="hljs-type">NoReply</span> s
                             )
  # <span class="hljs-type">WebSocket</span>.handle (\frame state -&gt; pure $ <span class="hljs-type">Stetson</span>.<span class="hljs-type">NoReply</span> state)
  # <span class="hljs-type">WebSocket</span>.info (\(<span class="hljs-type">BookMsg</span> msg) state -&gt; pure $ <span class="hljs-type">Stetson</span>.<span class="hljs-type">Reply</span> ((<span class="hljs-type">TextFrame</span> $ writeJSON msg) : nil) state)
</code></pre>
<p>This then allows Stetson to stash internal state in the underlying implementation and surface an API over this; indeed there is no way of accidentally calling the wrong &#39;self&#39; and sending messages to the wrong process much like in Pinto. Every callback takes place within a typed  context that enforces what &#39;state&#39; and &#39;msg&#39; are - pretty neat.</p>
]]></description><link>http://codeofrob.com/entries/purerl-updates---arbitrary-messages-and-stetson-handlers.html</link><guid isPermaLink="true">http://codeofrob.com/entries/purerl-updates---arbitrary-messages-and-stetson-handlers.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Tue, 07 Jul 2020 09:30:00 GMT</pubDate></item><item><title><![CDATA[Purerl Updates - Arbitrary messages and handle_info in gen servers]]></title><description><![CDATA[<p>A continuation of progress updates on Pinto/Stetson then..  </p>
<h1 id="previous-purerl-posts">Previous Purerl posts</h1>
<ul>
<li><a href="/entries/introducing-pinto-and-stetson---opinionated-purescript-bindings-to-otp-and-cowboy.html">Introduction to Pinto/Stetson - Opinionated Bindings to OTP/Cowboy</a></li>
<li><a href="/entries/the-structure-of-an-end-to-end-purescript-otp-project.html">The structure of an end-to-end purescript OTP project</a></li>
<li><a href="/entries/building-on-top-of-otp-with-purescript-with-pinto.html">Building on top of OTP with Purescript with Pinto</a></li>
<li><a href="/entries/building-a-purescript-web-server-with-stetson-and-pinto.html">Building a Purescript web server with Stetson and Pinto</a></li>
<li><a href="/entries/shared-code-twixt-purescript-server-and-client.html">Shared code twixt Purescript server and client</a></li>
<li><a href="/entries/purescript-interop-with-native-erlang---interacting-with-redis.html">Purescript interop with native Erlang, interaction with Redis</a></li>
</ul>
<h1 id="updates">Updates</h1>
<ul>
<li><a href="/entries/updates-to-pinto+stetson---purerl-in-progress.html">Nix overlays for Purerl/etc</a></li>
<li><a href="/entries/purerl-updates---typed-routes-in-stetson.html">Typed routing for Stetson</a></li>
<li>Arbitrary messages and handle_info in gen_servers</li>
<li>Monitors for arbitrary pids from Gen servers + Stetson handlers</li>
<li>WebSocket handlers in Stetson</li>
<li>Streaming handlers in Stetson</li>
<li>MessageRouting in Pinto to easily bind to legacy code that sends us messages</li>
</ul>
<h1 id="the-story-so-far">The story so far</h1>
<p>The code for dealing with handle_info was very hand-wavey and involved the creation and registration of mappers and receivers within the gen server itself. This also ended up abusing gen_server:cast in order to function correctly and it wasn&#39;t really obvious where messages were coming from. It was a ticking time bomb as far as supporting the increasing amounts of code we are writing in Purescript goes.</p>
<p>There was <em>some</em> good in this approach, in that the type of the Gen Server specified both the State of the Gen Server and the type of the Msg it would receive, and the handleInfo function could  be supplied  in Gen.init, forcibly typed with this server name.</p>
<pre><code class="language-haskell">
  <span class="hljs-class"><span class="hljs-keyword">data</span> <span class="hljs-type">Msg</span> = <span class="hljs-type">Tick</span> | <span class="hljs-type">SomethingHappened</span> <span class="hljs-type">String</span></span>

  <span class="hljs-class"><span class="hljs-keyword">type</span> <span class="hljs-type">State</span> = { 
      <span class="hljs-comment">-- some stuff </span>
  }</span>

  serverName :: <span class="hljs-type">ServerName</span> <span class="hljs-type">State</span> <span class="hljs-type">Msg</span>
  serverName = <span class="hljs-type">Local</span> $ atom <span class="hljs-string">&quot;my_server&quot;</span>

  startLink :: <span class="hljs-type">Effect</span> <span class="hljs-type">StartLinkResult</span>
  startLink = <span class="hljs-type">Gen</span>.startLink init handleInfo

  init :: <span class="hljs-type">Effect</span> <span class="hljs-type">State</span> 
  init = <span class="hljs-keyword">do</span>
    <span class="hljs-type">SomethingElse</span>.registerForEvents serverName <span class="hljs-type">SomethingHappened</span>
    pure {}


  handleInfo :: <span class="hljs-type">Msg</span> -&gt; <span class="hljs-type">Effect</span> (<span class="hljs-type">CastResult</span> <span class="hljs-type">State</span>)
  handleInfo msg = <span class="hljs-keyword">do</span>
    <span class="hljs-keyword">case</span> msg <span class="hljs-keyword">of</span>
      <span class="hljs-type">Tick</span> -&gt; doTIck
      <span class="hljs-type">SomethingHappened</span> id -&gt; handleSomething id

</code></pre>
<p>Having to provide serverName as part of the registration function is clunky AF, under the hood this places the responsibility of mapping messages to the external module and  there is a disconnect between <em>that</em> and the handleInfo we supplied  as part of startLink. </p>
<h1 id="a-first-pass-emitter-functions">A first pass, emitter functions</h1>
<p>The code was changed so that an emitter function would be extractable from within a gen server, this would be typed around ServerName automatically and only the right type of messages would be capable of being passed into it.</p>
<pre><code class="language-haskell">
<span class="hljs-title">init</span> :: <span class="hljs-type">Effect</span> <span class="hljs-type">State</span> = <span class="hljs-keyword">do</span>
<span class="hljs-title">init</span> = <span class="hljs-keyword">do</span>
    emitter &lt;- <span class="hljs-type">Gen</span>.emitter serverName
    <span class="hljs-type">SomethingElse</span>.registerForEvents $ emitter &lt;&lt;&lt; <span class="hljs-type">SomethingHappened</span>
    pure {}
</code></pre>
<p>This is somewhat an improvement, as it could at this point be assumed that anything passed into that function would automatically be the right type for handle_info and the mapping code from inside the gen server could be removed entirely. It requires the use of proxy processes to intercept messages, and I spent a day or two upgrading nearly all of our company Purescript over to this new model because it felt good.</p>
<p>It didn&#39;t feel <em>great</em> after doing that though, once again we&#39;re relying on convention to create that emitter with the right &#39;serverName&#39; and it&#39;s not very &#39;Erlang&#39;, in theory it also means that code could be written to send messages to arbitrary gen servers providing you have access to the serverName and thats a bit naff.</p>
<h1 id="second-pass-making-it-more-erlang">Second pass, making it more Erlang</h1>
<p>The type of &#39;emitter&#39; was changed to <em>Process Msg</em> (A welcome suggestion from <a href="@louispilfold">http://twitter.com/louispilfold</a> when I was putting code samples out for feedback). This maps under the hood to a new type of a plain ol&#39; pid and is therefore compatible automatically with classic Erlang APIs. (Specifically erlang monitors and such being a useful end-goal here).</p>
<pre><code class="language-haskell">
<span class="hljs-title">init</span> :: <span class="hljs-type">Effect</span> <span class="hljs-type">State</span> = <span class="hljs-keyword">do</span>
<span class="hljs-title">init</span> = <span class="hljs-keyword">do</span>
    self &lt;- <span class="hljs-type">Gen</span>.self serverName
    self ! <span class="hljs-type">DoSomeStuffAfterStartup</span>
    <span class="hljs-type">SomethingElse</span>.registerForEvents $ send self &lt;&lt;&lt; <span class="hljs-type">SomethingHappened</span>
    pure {}
</code></pre>
<p>This was still not ideal however, the call to Gen.self included a runtime check (below) to ensure that the caller was indeed the &quot;self&quot; we were looking at to prevent external clients from abusing the API (if you provide an API, it <em>will</em> be abused and I&#39;d already seen some &quot;interesting&quot; code already written around these APIs while I was upgrading just our own code!)</p>
<pre><code class="language-erlang">
<span class="hljs-function"><span class="hljs-title">selfImpl</span><span class="hljs-params">(Name)</span> -&gt;</span>
  <span class="hljs-keyword">fun</span>() -&gt;
    Pid  = where_is_name(Name),
    Self = erlang:self(),
    <span class="hljs-keyword">if</span>
      Self =:= Pid -&gt; Self;
      <span class="hljs-literal">true</span> -&gt;
        exit(Self, {error, &lt;&lt;<span class="hljs-string">&quot;Gen.self was called from an external process, this is not allowed&quot;</span>&gt;&gt;})
    <span class="hljs-keyword">end</span>
  <span class="hljs-keyword">end</span>.

</code></pre>
<h1 id="third-pass-making-it-more-purescript">Third pass, making it more Purescript</h1>
<p>Sod it, StateT it is. We&#39;d been discussing moving the Gen callbacks into a state monad since I first wrote Pinto, the only obstacle being that I didn&#39;t understand state monads, which sounds stupid on retrospect but it&#39;s the truth <em>shrug</em>. I read a few tutorials, had a mild &quot;aha&quot; moment and things became a bit clearer.</p>
<p>What we really want is that all the callbacks to automatically</p>
<ul>
<li>Be typed around ServerName, so that all calls to Pinto APIs automatically assume this type</li>
<li>have access to the &#39;internal&#39; state in the Pinto gen_server implementation, so no casts ever have to be made again</li>
</ul>
<p>We had quite a bit of code in Gen.purs (our gen server wrapper) that relied on making casts to modify its state, monitors and such - removing all of this was just a sensible idea -  the idea being that if the callbacks to client coded operated within the context of that state, it could be retrieved and modified (optionally) as part of those callbacks.</p>
<pre><code class="language-haskell">
<span class="hljs-title">startLink</span> :: <span class="hljs-type">Effect</span> <span class="hljs-type">StartLinkResult</span>
<span class="hljs-title">startLink</span> = <span class="hljs-type">Gen</span>.startLink init handleInfo

<span class="hljs-title">init</span> :: <span class="hljs-type">Gen</span>.<span class="hljs-type">Init</span> <span class="hljs-type">State</span> <span class="hljs-type">Msg</span>
<span class="hljs-title">init</span> = 
  self &lt;- <span class="hljs-type">Gen</span>.self
  <span class="hljs-type">Gen</span>.lift $ <span class="hljs-type">SomethingElse</span>.registerForEvents $ send self  &lt;&lt;&lt; <span class="hljs-type">SomethingHappened</span>
  pure {}

<span class="hljs-title">handleInfo</span> :: <span class="hljs-type">Msg</span> -&gt; <span class="hljs-type">State</span> -&gt; <span class="hljs-type">Gen</span>.<span class="hljs-type">HandleInfo</span> <span class="hljs-type">State</span> <span class="hljs-type">Msg</span>
<span class="hljs-title">handleInfo</span> msg state =
  <span class="hljs-keyword">case</span> msg <span class="hljs-keyword">of</span>
    <span class="hljs-type">Tick</span> -&gt; <span class="hljs-type">CastNoReply</span> &lt;$&gt; handleTick state
    <span class="hljs-type">SomethingHappened</span> ev -&gt; <span class="hljs-type">CastNoReply</span> &lt;$&gt; handleSomethingHappened ev state

</code></pre>
<p>On the surface of this it isn&#39;t that different, but we&#39;ve done away with the need to constantly refer to &#39;serverName&#39; because we&#39;re operating in the context of a state monad (Gen.Init and Gen.HandleInfo are type aliass to help refer to the fairly wordy type used behind the scenes in Pinto).</p>
<p>Gen.self doesn&#39;t need to do anything other than pull state out of that state monad and return it to the client code (implementation below), this means that unless your code is being executed in the context of the state monad (IE: the gen server) you can&#39;t call it and the runtime checks and side effects can go away.</p>
<pre><code class="language-haskell">
  self :: <span class="hljs-keyword">forall</span> state msg. <span class="hljs-type">StateT</span> (<span class="hljs-type">GenContext</span> state msg) <span class="hljs-type">Effect</span> (<span class="hljs-type">Process</span> msg)
  self = <span class="hljs-keyword">do</span>
    <span class="hljs-type">GenContext</span> { pid } &lt;- <span class="hljs-type">State</span>.get
    pure pid
</code></pre>
<p>Similarly, Gen.Cast and Gen.Call are provided for <em>those</em> callbacks too, and all code executed  within the context of a Pinto Genserver has access to the internal state via the API so in theory things like trapExit/handleInfo/config can be modified safely from within that context without doing weird things around async casts back to that gen server.</p>
<p>That&#39;s a lot of words to say that Gen Servers and arbitrary messages are now very pretty indeed in Purerl. Example below of a gen server subscribing to a message bus from the <a href="https://github.com/id3as/demo-ps/blob/master/server/src/HandleInfoExample.purs">demo_ps</a> web project. You&#39;ll note that the actual API used in startLink has evolved to include a builder for setting the initial handlers/etc - there are a number of optional things to tweak about a gen server and it made sense to do this rather than accept an endlessly growing list of arguments.</p>
<pre><code class="language-haskell">
<span class="hljs-class"><span class="hljs-keyword">type</span> <span class="hljs-type">BookWatchingStartArgs</span> = {}</span>
<span class="hljs-class"><span class="hljs-keyword">type</span> <span class="hljs-type">State</span> = {}</span>

<span class="hljs-class"><span class="hljs-keyword">data</span> <span class="hljs-type">Msg</span> = <span class="hljs-type">BookMsg</span> <span class="hljs-type">BookEvent</span> </span>

<span class="hljs-title">serverName</span> :: <span class="hljs-type">ServerName</span> <span class="hljs-type">State</span> <span class="hljs-type">Msg</span>
<span class="hljs-title">serverName</span> = <span class="hljs-type">Local</span> $ atom <span class="hljs-string">&quot;handle_info_example&quot;</span>

<span class="hljs-title">startLink</span> :: <span class="hljs-type">BookWatchingStartArgs</span> -&gt; <span class="hljs-type">Effect</span> <span class="hljs-type">StartLinkResult</span>
<span class="hljs-title">startLink</span> args =
  <span class="hljs-type">Gen</span>.buildStartLink serverName (init args) $ <span class="hljs-type">Gen</span>.defaultStartLink { handleInfo = handleInfo }

<span class="hljs-title">init</span> :: <span class="hljs-type">BookWatchingStartArgs</span> -&gt; <span class="hljs-type">Gen</span>.<span class="hljs-type">Init</span> <span class="hljs-type">State</span> <span class="hljs-type">Msg</span>
<span class="hljs-title">init</span> args = <span class="hljs-keyword">do</span>
  self &lt;- <span class="hljs-type">Gen</span>.self
  _ &lt;- <span class="hljs-type">Gen</span>.lift $ <span class="hljs-type">SimpleBus</span>.subscribe <span class="hljs-type">BookLibrary</span>.bus $ <span class="hljs-type">BookMsg</span> &gt;&gt;&gt; send self
  pure $ {}

<span class="hljs-title">handleInfo</span> :: <span class="hljs-type">Msg</span> -&gt; <span class="hljs-type">State</span> -&gt; <span class="hljs-type">Gen</span>.<span class="hljs-type">HandleInfo</span> <span class="hljs-type">State</span> <span class="hljs-type">Msg</span>
<span class="hljs-title">handleInfo</span> msg state = <span class="hljs-keyword">do</span>
  <span class="hljs-keyword">case</span> msg <span class="hljs-keyword">of</span>
    <span class="hljs-type">BookMsg</span> bookEvent -&gt; 
      <span class="hljs-type">Gen</span>.lift $ handleBookEvent bookEvent state

<span class="hljs-title">handleBookEvent</span> :: <span class="hljs-type">BookEvent</span> -&gt; <span class="hljs-type">State</span> -&gt; <span class="hljs-type">Effect</span> (<span class="hljs-type">CastResult</span> <span class="hljs-type">State</span>)
<span class="hljs-title">handleBookEvent</span> ev state =
  <span class="hljs-keyword">case</span> ev <span class="hljs-keyword">of</span>
    <span class="hljs-type">BookCreated</span> isbn -&gt; <span class="hljs-keyword">do</span>
      _ &lt;- <span class="hljs-type">Logger</span>.info1 <span class="hljs-string">&quot;Book created ~p&quot;</span> isbn
      pure $ <span class="hljs-type">CastNoReply</span> state
    <span class="hljs-type">BookDeleted</span> isbn -&gt; <span class="hljs-keyword">do</span>
      _ &lt;- <span class="hljs-type">Logger</span>.info1 <span class="hljs-string">&quot;Book deleted ~p&quot;</span> isbn
      pure $ <span class="hljs-type">CastNoReply</span> state
    <span class="hljs-type">BookUpdated</span> isbn -&gt; <span class="hljs-keyword">do</span>
      _ &lt;- <span class="hljs-type">Logger</span>.info1 <span class="hljs-string">&quot;Book updated ~p&quot;</span> isbn
      pure $ <span class="hljs-type">CastNoReply</span> state
</code></pre>
<p>I&#39;ll insert another item to the list of &#39;new things&#39; to the bullet points currently being traversed as my next post will be about the corresponding message handling implementation in <a href="https://github.com/id3as/purescript-erl-stetson">Stetson</a>, which unsurprisingly uses the State monad to improve our lives there as well. Once you learn how to use a hammer, everything looks like a nail I guess.</p>
]]></description><link>http://codeofrob.com/entries/purerl-updates---arbitrary-messages-and-handle_info-in-gen-servers.html</link><guid isPermaLink="true">http://codeofrob.com/entries/purerl-updates---arbitrary-messages-and-handle_info-in-gen-servers.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Thu, 02 Jul 2020 09:30:00 GMT</pubDate></item></channel></rss>