<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Rob Ashton's blog]]></title><description><![CDATA[Software development dumping ground]]></description><link>http://codeofrob.com</link><image><url>http://codeofrob.com/img/cover.jpg</url><title>Rob Ashton&apos;s blog</title><link>http://codeofrob.com</link></image><generator>NodeJS RSS Module</generator><lastBuildDate>Fri, 08 Mar 2013 18:15:10 GMT</lastBuildDate><atom:link href="http://feeds.feedburner.com/robashton" rel="self" type="application/rss+xml"/><item><title><![CDATA[The price of abstraction - using LevelDB in RavenDB]]></title><description><![CDATA[<p>So, in our little foray into LevelDB, we've uncovered the following:</p>

<ul>
<li>We can do <a href="/entries/basic-operations-with-leveldb.htm">atomic writes</a></li>
<li>We can have <a href="/entries/transactional-guarantees-on-top-of-leveldb.html">consistent reads</a>, </li>
<li>We can create a <a href="/entries/writing-a-transaction-manager-on-top-of-leveldb.html">transaction manager</a> on top of LevelDB </li>
<li>We can can <a href="/entries/streaming-large-values-from-leveldb.html">stream data out of LevelDB</a></li>
<li>And we can create <a href="/entries/secondary-indexes-on-top-of-leveldb.html">secondary indexes</a></li>
</ul>

<p>With this, I set about creating a fork of RavenDB with a starting implementation of LevelDB for document storage.</p>

<p>I grabbed the <a href="https://github.com/meebey/leveldb-sharp">C# bindings</a> from Github and set about re-implementing my in-memory transaction manager on top of them.</p>

<p>The first thing I notice about the C# bindings is that the API looks like this:</p>

<pre><code>string value = db.Get(string key)
db.Put(string key, string value)
</code></pre>

<p>This is not what <a href="/entries/streaming-large-values-from-leveldb.html">RavenDB wants</a>, what RavenDB wants is a MemoryStream or at the very least a ByteArray - as it is not in the business of pushing large strings around the place in managed memory.</p>

<p>No problem, I fork the original project and set about addding Byte array support and then notice something.</p>

<p><em>The bindings are using the C API for LevelDB</em> (which makes sense if you're going to support cross platform C#), but more importantly the C API looks something like this:</p>

<pre><code>char* leveldb_get(char* key)
</code></pre>

<p>See that return result? That's a malloc, what does the LevelDB C# code do?</p>

<pre><code> var value = Marshal.PtrToStringAnsi(valuePtr, (int) valueLength);
 return value;
</code></pre>

<p>Ignoring the <a href="https://github.com/meebey/leveldb-sharp/issues/2">memory leak</a>, as it's a known issue and an easy mistake to make, we can see we're copying the data out of the unmanaged array into a managed string via the PtrToStringAnsi call.</p>

<p>What does the C api do?</p>

<pre><code>Status s = db-&gt;rep-&gt;Get(options-&gt;rep, Slice(key, keylen), &amp;tmp);
if (s.ok()) {
  *vallen = tmp.size();
  result = CopyString(tmp);
} else {
  *vallen = 0;
  if (!s.IsNotFound()) {
    SaveError(errptr, s);
  }
}
return result;
</code></pre>

<p>And CopyString?</p>

<pre><code>static char* CopyString(const std::string&amp; str) {
  char* result = reinterpret_cast&lt;char*&gt;(malloc(sizeof(char) * str.size()));
  memcpy(result, str.data(), sizeof(char) * str.size());
  return result;
}
</code></pre>

<p><em>urk</em></p>

<p>In order to get a stored document from LevelDB, the following operations are performed:</p>

<pre><code>- Internally, LevelDB will load the document into memory
- Internally, LevelDB will copy that memory into a string on a Get call
- The C API copies that memory into a mallocced byte array
- The C# API copies that memory into a managed string (probably on the Large Object Heap)
</code></pre>

<p>What we have here, is a C# wrapper around a C API that is a wrapper around a C++ API, and each of them is trying to be semantically equivalent to the underlying API - and the abstraction is <em>killing us</em>.</p>

<p>For small values, this probably isn't a big deal, but RavenDB documents can be any size, and RavenDB documents often <em>are</em> any size - ideally what we want is</p>

<pre><code>MemoryStream stream = db.Get(string key)
</code></pre>

<p>Where that stream reads from the start of an unmanaged array to the end of that unmanaged array and then disposes it.</p>

<p>In essence, the job of getting LevelDB into RavenDB is going to involve writing a new C API for LevelDB specific to our needs, then writing a C# API around that API specific to our needs.</p>

<p>Thankfully we need about 10% of the exposed functionality in LevelDB so our job isn't as hard as "writing a set of managed bindings for LevelDB", but this means that I won't have time to do this before I leave Israel.</p>

<p>I put my sad face on and get to work on something more manageable for my final day of work :-(</p>]]></description><link>http://codeofrob.com/entries/the-price-of-abstraction---using-leveldb-in-ravendb.html</link><guid isPermaLink="true">http://codeofrob.com/entries/the-price-of-abstraction---using-leveldb-in-ravendb.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Fri, 08 Mar 2013 09:00:00 GMT</pubDate></item><item><title><![CDATA[Secondary indexes on top of LevelDB]]></title><description><![CDATA[<p>Got <a href="/entries/basic-operations-with-leveldb.htm">atomic writes</a>, <a href="/entries/transactional-guarantees-on-top-of-leveldb.html">consistent reads</a>, <a href="/entries/writing-a-transaction-manager-on-top-of-leveldb.html">transactions</a> and we can <a href="/entries/streaming-large-values-from-leveldb.html">do streaming</a> if we need to - now if we can do secondary indexes we can move forwards with creating a storage implementation for RavenDB.</p>

<p>An example of a couple of indexes we need in RavenDB is:</p>

<ul>
<li>The document itself stored by key (primary index)</li>
<li>The document key stored by etag (secondary index)</li>
</ul>

<p>Whenever we write a document in RavenDB, we want to</p>

<ul>
<li>Remove the old Etag (or at least update it)</li>
<li>Write the document against its key</li>
<li>Write a new etag for the document</li>
</ul>

<p>(Actually, a bit more goes on than this as we store meta data for headers etc, but this will do for now)</p>

<p>The operation we want to do when indexing is</p>

<ul>
<li>What are all the new etags since last we indexed?</li>
<li>Loop through each of these, load the document and index it</li>
</ul>

<p>Etags are sortable by design, in my rough and ready document database the etag is just an integer that I increase every time I add a new document.</p>

<p>Now, when writing and reading from the database, we need to be able to differentiate between types of key in the database, and ordering for each of these types is set by the key.</p>

<p><em>For example, reading a document</em></p>

<pre><code>#define DOCPREFIX "/docs/"
#define ETAGPREFIX "/etags/"


void Get(std::string id, std::string* doc) {
  std::stringstream st;
  st &lt;&lt; DOCPREFIX;
  st &lt;&lt; id;
  std::string key = st.str();
  this-&gt;lastStatus = this-&gt;db-&gt;Get(leveldb::ReadOptions(), key, doc);
}
</code></pre>

<p>So I'm sticking a prefix in front of each key to denote what sort of key it is, yay.</p>

<p>That means that my secondary index of etags will end up looking like this</p>

<pre><code>/etags/0 -&gt; /docs/1
/etags/1 -&gt; /docs/5
/etags/2 -&gt; /docs/4
/etags/3 -&gt; /docs/6
/etags/4 -&gt; /docs/3
/etags/6 -&gt; /docs/7
/etags/8 -&gt; /docs/12
/etags/9 -&gt; /docs/5
</code></pre>

<p>When I want to index documents newer than /etags/5, I can do</p>

<pre><code>int IndexNewDocumentsNewerThan(int etag) {
  std::stringstream st;
  st &lt;&lt; ETAGPREFIX;
  st &lt;&lt; etag;
  std::string startkey = st.str();

  leveldb::Iterator* it = this-&gt;db-&gt;NewIterator(leveldb::ReadOptions());
  for (it-&gt;Seek(startkey); it-&gt;Valid() &amp;&amp; this-&gt;IsEtagKey(it-&gt;key().ToString()); it-&gt;Next()) {
    std::string document = it-&gt;value().ToString();
    IndexDocument(value);
  }
  delete it;
}
</code></pre>

<p>Building up other indexes is a trivial matter of making sure we have a suitable ordering in place, and queries are simply a matter of building up these keys appropriately.</p>

<p>With all of this, I think we have enough information to go and build persistence for RavenDB on top of LevelDB (and then test test test, performance performance performance etc).</p>

<p>If you want to check out the spikes and playarounds with LevelDB, you can find them on my <a href="https://github.com/robashton/leveldb-play">Github</a></p>]]></description><link>http://codeofrob.com/entries/secondary-indexes-on-top-of-leveldb.html</link><guid isPermaLink="true">http://codeofrob.com/entries/secondary-indexes-on-top-of-leveldb.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Thu, 07 Mar 2013 09:00:00 GMT</pubDate></item><item><title><![CDATA[Streaming large values from LevelDB]]></title><description><![CDATA[<p>We've uncovered that we can do <a href="/entries/basic-operations-with-leveldb.htm">atomic writes</a>, <a href="/entries/transactional-guarantees-on-top-of-leveldb.html">consistent reads</a> and <a href="/entries/writing-a-transaction-manager-on-top-of-leveldb.html">implement transactions</a> on top of LevelDB, but we've got another need in RavenDB which is that we have large objects (documents) inside RavenDB and we don't always want to load them into managed memory as a whole unit (instead, we want to stream them into a set of smaller objects)</p>

<p>Why don't we want to load them into memory in one go? Because large objects can wreck havoc with the garbage collector (the large object heap), and we also have streaming APIs in RavenDB that allow us to limit this sort of thing that we'd like to honour.</p>

<p>So, back to C++ as we answer the question "What about large objects inside LevelDB"?</p>

<p>Looking at the APIs in our first entry, we can see that "Get" will load entire value into memory ala this:</p>

<pre><code>db-&gt;Get(leveldb::ReadOptions(), key, &amp;document);
</code></pre>

<p>We <em>could</em> simply read bytes from this document (remember, it's an std::string) and marshal them through to C# bit by bit, but that means copying the whole document out of the store before doing this. This isn't a big deal as it's still completely on the native side at this point, but it's still an expense I'd prefer to skip if we're reading more than a few documents.</p>

<p>That's where Slice comes in.</p>

<pre><code>  leveldb::Iterator* it = this-&gt;db-&gt;NewIterator(leveldb::ReadOptions());
  for (it-&gt;Seek(start_key); it-&gt;Valid() &amp;&amp; it-&gt;key().ToString() &lt;= end_key; it-&gt;Next()) {
    Slice* slice = it-&gt;value();

    // Do stuff with a slice
  }
</code></pre>

<p>This is quite cool, LevelDB gives us the notion of a "Slice", which is merely a pointer to the beginning of the value, and the length of the value.</p>

<p>A "Get" operation merely copies this data into an std::string, and calling slice->ToString() will do this as well.</p>

<p>What we can do is </p>

<pre><code>char buffer[BUFFER_SIZE];
memcpy(slice-&gt;data(), buffer, BUFFER_SIZE);
</code></pre>

<p>Rinse and repeat, we can actually do this within an enumerator and send appropriately sized chunks aross to C# to be processed without copying the whole thing out at once.  This could be useful when iterating over a set of documents, although most likely simply copying into the string and using string.c_str() to get the pointer to the copied data will give us easier code - we'll see how that works out when it comes to actually writing the C# we need.</p>

<p>Anyway, now we just need support for secondary indexes, and for this I'll write a rudimentary document store.</p>]]></description><link>http://codeofrob.com/entries/streaming-large-values-from-leveldb.html</link><guid isPermaLink="true">http://codeofrob.com/entries/streaming-large-values-from-leveldb.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Wed, 06 Mar 2013 09:00:00 GMT</pubDate></item><item><title><![CDATA[Uncle Bob's viewpoint considered harmful]]></title><description><![CDATA[<p><strong>oof</strong></p>

<p>I just felt a disturbance in the force, it's as if a thousand angry geeks just vented their nerdrage on Twitter because of a <a href="http://blog.8thlight.com/uncle-bob/2013/03/05/TheStartUpTrap.html">blog post by Uncle Bob</a> and a response by <a href="http://news.ycombinator.com/item?id=5328721">@Nate Kohari</a>.</p>

<blockquote class="twitter-tweet"><p>It looks like I've pissed off everyone at 8th Light. Fortunately I can't hear the whining over the sound of me shipping code.</p>&mdash; Nate Kohari (@nkohari) <a href="https://twitter.com/nkohari/status/309028034755825665">March 5, 2013</a></blockquote>

<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>Ouch.</p>

<p><strong>So what kind of tests do you write Rob?</strong></p>

<p>Meh</p>

<p><strong>When do you write tests Rob?</strong></p>

<p>Meh</p>

<p><strong>Do you write tests Rob?</strong></p>

<p>Meh</p>

<p><strong>What</strong></p>

<p>Ok, so I write tests - I usually can't be bothered having this conversation though, because years of having "TDD" forced down everybody's throats has left a bitter divide between developers who crave tests with every ounce of their being, and developers who really don't like tests and wading between them is opening myself up for another few hours of argument when I'd prefer to just be building stuff.</p>

<p><em>I'm currently wandering around though, and have the time for this sort of thing.</em></p>

<p>Classic "<em>make no assumptions</em>" TDD is dead, that ship has sailed - it is over.  The stance it put forth however had the desired effect; which was to get everybody thinking about the code they wrote before they wrote it and write some tests around that code at some point in that code's lifetime so that iterating on that code in the future wouldn't be painful.</p>

<p><em>painful</em></p>

<p>That's the word. <strong>pain</strong>, let's say it again - <strong>pain</strong>, if it hurts - do something about it. </p>

<ul>
<li><p>Do you have too many tests that you are regularly throwing away with the failed feature? Stop writing so many tests around your product - you're likely experimenting and you're best off adopting a "spike and stabilise" approach to development.</p></li>
<li><p>Do you find it hard to make changes because lots of things break due to harmful coupling? Write some more tests around that part of the system, practise a test-first approach on new code and feel your way around those coupling issues.</p></li>
<li><p>Are you performing a song and dance around a feature because you want to do fine-grained unit testing and it means putting reams of code where your direct controller CRUD would suffice? Stop, stick with the end-to-end tests and perhaps use an in-memory repo to keep them fast enough.</p></li>
</ul>

<p>Listening to the pain is the only sane way to do software development, blindly listening to far-right viewpoints like Uncle Bobs will get you so far, and listening to far-left viewpoints which advocate throwing caution to the wind will probably lead you to failure. The idea that tests are a waste of time is a dangerous one if given time to grow in an echo chamber, but the idea that TDD is the absolute is just as dangerous.</p>

<p>Most of the time going with <em>make no assumptions</em> TDD is a waste of your client's money, and most of the time writing no tests at all will come back to bite you in the future - <em>got it</em>.</p>

<p>TDD as you mean it is a great academic exercise for your free time or weekends, TDD as you mean it is a great marketing term if you're one of those consultants who consult on TDD, and recruiters definitely love to see it on your CV but TDD is not usually how we build great software on time.</p>

<p><strong>So what kind of tests <em>do</em> you write Rob?</strong></p>

<ul>
<li>I usually spike and stabilise on new features which are likely to be canned, this means not making undue effort early on on code likely to be thrown away but allows the feature to get out as quickly as possible for feedback</li>
<li>I usually start off with end-to-end tests with an in-memory representation of anything that is slow (the database is an implementation detail etc)</li>
<li>I drop down to unit tests if I have complicated logic (surprisingly, most systems just have crud, so this is not necessary)</li>
<li>Workflows are usually tested from the UI as part of the end-to-end suite as it's what the user sees</li>
<li>Any combination of the above</li>
</ul>

<p><strong>Summary</strong></p>

<p>There are two types of pragmatist in this world when it comes to testing, those that use pragmatism as an excuse not to do the right thing, and me.</p>]]></description><link>http://codeofrob.com/entries/uncle-bobs-viewpoint-considered-harmful.html</link><guid isPermaLink="true">http://codeofrob.com/entries/uncle-bobs-viewpoint-considered-harmful.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Wed, 06 Mar 2013 08:00:00 GMT</pubDate></item><item><title><![CDATA[Writing a transaction manager on top of LevelDB]]></title><description><![CDATA[<p>So we can do <a href="/entries/transactional-guarantees-on-top-of-leveldb.html">consistent reads</a> and <a href="/entries/basic-operations-with-leveldb.html">batched writes</a>, but consistency across the two of these isn't built in.</p>

<p>Turns out this isn't a big deal and doesn't require an awful lot of work to build in on top of LevelDB. </p>

<p>This might actually be one of the strengths of LevelDB, that it does some of the more technically challenging stuff (putting all the levels and atomicity into the DB for the experts), but doesn't make decisions like what sort of guarantees you need in your database for you.</p>

<p>So, into the C# for a little experiment I wrote a very rudimentary transaction manager on top of ConcurrentDictionary.</p>

<pre><code>var myStore = new Storage(leveldb);
myStore.Batch(accessor =&gt; {
  var doc = accessor.Get("key1");
  accessor.Put("key2", "Hello");
});
</code></pre>

<p>What does this look like?</p>

<pre><code>public void Batch(Action&lt;StorageAccessor&gt; actions) {
  var transaction = this.CreateTransaction();
  var accessor = new StorageAccessor(this, transaction);
  try {
    actions(accessor);
  } catch(Exception ex) {
    this.RollbackTransaction(transaction);
    throw;
  }
  this.CommitTransaction(transaction);
}
</code></pre>

<p>Where the accessor simply calls the Get/Put/Delete methods on the storage and passes in the transaction associated with it.</p>

<pre><code>public class StorageAccessor {
  private Storage storage;
  private StorageTransaction transaction;

  public StorageAccessor(Storage storage, StorageTransaction transaction) {
    this.storage = storage;
    this.transaction = transaction;
  }

  public Object Get(string id) {
    return this.storage.Get(id, this.transaction);
  }

  public void Put(string id, Object obj) {
    this.storage.Put(id, obj, this.transaction);
  }

  public void Delete(string id) {
    this.storage.Delete(id, this.transaction);
  }
}
</code></pre>

<p>Where we then make a check to see if another transaction has modified that key in the meantime</p>

<pre><code>public void Put(string id, Object obj, StorageTransaction transaction) {
  this.keysToTransactionId.AddOrUpdate(id, (key) =&gt; {
      transaction.AddOperation(storage =&gt; storage.Put(id, obj));
      return transaction.Id;
    }, 
    (key, oldValue) =&gt; {
    // NOTE: This doesn't handle the transaction doing multiple operations on the same key
    throw new Exception("This should be a concurrency exception but I'm lazy");
  });
}
</code></pre>

<p>Obviously a real implementation needs to do things like keep modified information around so reads can go via that before hitting the LevelDB snapshot, but in essence you can build a pretty simple transaction manager providing your underlying storage does the hard work of managing atomicity of operations.</p>

<p>Next up, back to the C++ as I look at what we're doing do about large values in RavenDB documents.</p>]]></description><link>http://codeofrob.com/entries/writing-a-transaction-manager-on-top-of-leveldb.html</link><guid isPermaLink="true">http://codeofrob.com/entries/writing-a-transaction-manager-on-top-of-leveldb.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Tue, 05 Mar 2013 09:00:00 GMT</pubDate></item><item><title><![CDATA[Transactional guarantees on top of LevelDB]]></title><description><![CDATA[<p>So, I'm <a href="/entries/basic-operations-with-leveldb.html">experimenting with LevelDB</a> and I've discovered that it does atomic writes both on single operations and batches of operations.</p>

<p>This isn't actually all we need in RavenDB, as we need to be able to support multiple reads and writes - potentially over a period of time.</p>

<p>We can't just be reading from a database that might be changing under our feet all of the time, and it turns out that LevelDB gives us the ability to read from a particular version of the database.</p>

<p>When beginning a long running operation that needs a consistent read-state, we can create a snapshot and use this in all of our reads to ensure we have a consistent view of the database.</p>

<pre><code>// Create a snapshot at the beginning of a sequence of operations
leveldb::Snapshot* snapshot = db-&gt;GetSnapshot();


// For each read operation we can use this snapshot
leveldb::ReadOptions options;
options.snapshot = snapshot;

db-&gt;Get(options, "key", &amp;document);
</code></pre>

<p>That solves that problem then, although it leaves another question in the air - which is how LevelDB handles multiple writers modifying the same key.</p>

<p>Consider a thread coming along and beginning an operation</p>

<pre><code>tx1 = store-&gt;BeginOperation();
</code></pre>

<p>And another thread beginning an operation</p>

<pre><code>tx2 = store-&gt;BeginOperation();
</code></pre>

<p>And then</p>

<pre><code>// Thread one deletes a key
store-&gt;Delete("key1", tx1);

// Thread two Writes to that key
store-&gt;Put("key1", tx2);
</code></pre>

<p>By default (at least as I understand) LevelDB will happily accept these operations as it doesn't have any concurrency control.</p>

<p>Happily, this is easy enough to work through as we'll see in the next entry.</p>]]></description><link>http://codeofrob.com/entries/transactional-guarantees-on-top-of-leveldb.html</link><guid isPermaLink="true">http://codeofrob.com/entries/transactional-guarantees-on-top-of-leveldb.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Mon, 04 Mar 2013 09:00:00 GMT</pubDate></item><item><title><![CDATA[Basic operations with LevelDB]]></title><description><![CDATA[<p>So, in <a href="/entries/investigating-ravendb-on-mono-for-reals.html">trying out LevelDB</a>, I need to work out the basics, and then apply those learnings to whether the functionality exposed is compatible with RavenDB's storage needs.</p>

<p>I spent a few hours in the car on the way to and back from the Dead Sea and this seemed like an ideal time to crack out the g++ and write some C++ on top of LevelDB.</p>

<p>The first thing I did was download <a href="https://code.google.com/p/leveldb/">the source</a> and un-pack it, I also grabbed <a href="http://twitter.com/kellabyte">@kellabytes</a>'s '<a href="https://github.com/kellabyte/Dazzle">Dazzle</a>' source code as an 'RTFM' back-up.</p>

<p>What I really like is that the header files for LevelDB are the best means of documentation, I forgot I liked this about C++, I now remember - all the learnings done ended up being done by just reading the source code, pretty neat.</p>

<p>So, what do we have?</p>

<p><em>Opening a database</em></p>

<pre><code>leveldb::DB* db;
leveldb::DB::Open(options, "play/testdb", &amp;db);
</code></pre>

<p><em>Putting something in</em></p>

<pre><code>status = db-&gt;Put(leveldb::WriteOptions(), "Key", "Hello World");
</code></pre>

<p><em>Getting something out</em></p>

<pre><code>std::string document;
store-&gt;Get(leveldb::ReadOptions(), "key", &amp;document);
</code></pre>

<p><em>Deleting something</em></p>

<pre><code>store-&gt;Delete(leveldb::WriteOptions(), "key");
</code></pre>

<p><em>Squeeeeeeee</em></p>

<p>I love how simple that is, and that each of these is a safe operation, important to note the following at this point:</p>

<ul>
<li>I'm passing in strings to those put/get operations</li>
<li>LevelDB is copying to/from those strings</li>
<li>Because we're using strings, scope determines the release of memory</li>
</ul>

<p>This is elegant, Put actually takes a "Slice" type too, but that's implicitly convertable from a string and therefore this works nicely. </p>

<p>I'll cover Slice on its own as it's an interesting notion if I understand it correctly.</p>

<p>Importantly for RavenDB, we need to be able to write multiple operations in an atomic fashion, LevelDB appears to accommodate for this neatly.</p>

<pre><code> leveldb::WriteBatch batch;
 batch.Delete("key");
 batch.Put("key2", value);
 db-&gt;Write(leveldb::WriteOptions(), &amp;batch);
</code></pre>

<p>LevelDB can actually operate in async or synchronous mode, but because Raven makes gaurantees about writes having happened I can't think we'd be able to use async mode (because there isn't any way to know when these writes are finished to my knowledge).</p>

<p>These guarantees aren't actually enough for RavenDB, and I'll cover the reasons for that in the next entry.</p>]]></description><link>http://codeofrob.com/entries/basic-operations-with-leveldb.html</link><guid isPermaLink="true">http://codeofrob.com/entries/basic-operations-with-leveldb.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Fri, 01 Mar 2013 09:00:00 GMT</pubDate></item><item><title><![CDATA[Investigating RavenDB on Mono for reals]]></title><description><![CDATA[<p>I've been down <a href="/entries/ravendb-on-mono.html">this road before</a> with a certain amount of success, and indeed I even did several talks entirely in linux about RavenDB complete with working demos (although some folk might doubt that story).</p>

<p>So what gives then? How come RavenDB doesn't work on Mono out of the box now a couple of years later? Well - the short answer is that it never did. </p>

<p>When I got it working on Mono in the past it was on top of "Munin", a managed storage engine that <a href="http://twitter.com/ayende">@ayende</a> and I hacked up in a hotel lobby in the small hours of the morning that "mostly works" but has never entirely been recommended for production.</p>

<p>That's not to say we haven't got a lot of mileage out of it, as indeed it has been used in pretty much everyone's test suites since its conception and has faithfully done as asked all that time; however, it is clear that for <em>true</em> cross-platform happiness, we could do with a replacement for Esent for platforms that don't support it, and re-inventing the wheel ourselves when so many other people have done work on this for us doesn't make a lot of sense.</p>

<p>The other problem is of course that RavenDB was developed on top of the .NET runtime, using the .NET compiler chain - and support for some of the features used by RavenDB make compilation on mono an hilariously frustrating process of bug fixes and pull requests (which then means it won't work OOB on versions of mono packaged with popular Linux distros). </p>

<p>I'm not going near that with a barge pole, although I'm sure if somebody wants to sit there and get RavenDB simply <em>building</em> on Mono, it'll only take a few hours of workarounds which will be happily accepted as a pull request.</p>

<p>With that, this series of blog posts is <strong>not</strong> going to finish with an entry saying "It now works on Mono", because that's going to take a couple of weeks' work and I haven't got a couple of weeks. However, the breadcrumbs and work completed so far should be good enough for anybody with the spare time and inclination to finish that work and get it working for reals.</p>

<p>So, we've been doing some investigations, and in summary it looks like LevelDB is a good option for our usage patterns - I'm going to spend a couple of entries going over the little spikes I wrote to verify that, and then show the beginnings of the work inside RavenDB itself to kick this off with.</p>

<p>With a proper cross-platform persistence solution, and a few hours tinkering with the build - RavenDB will work on Mono, allow me to help lay some foundations...</p>]]></description><link>http://codeofrob.com/entries/investigating-ravendb-on-mono-for-reals.html</link><guid isPermaLink="true">http://codeofrob.com/entries/investigating-ravendb-on-mono-for-reals.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Thu, 28 Feb 2013 09:00:00 GMT</pubDate></item><item><title><![CDATA[Custom query inputs in RavenDB]]></title><description><![CDATA[<p>Back on the list of things "added to RavenDB" <a href="/entries/working-at-hibernating-rhinos.html">during my time at Hibernating Rhinos</a>, ever wished you could pass custom arguments to the transformer when performing a query in RavenDB?</p>

<p>Well, this has been asked for a number of times and since <a href="/entries/ravendb-resulttransformers---a-new-way-of-looking-at-things.html'">splitting out results-transformers</a> into their own process, it has become much easier to add this functionality.</p>

<p>What does this look like?</p>

<p>Well, say we have a result transformer that takes Ponies and creates unicorns, only our database doesn't know about horns - let's see what we can do here.</p>

<pre><code>public class Unicorn {
  public string Name { get; set; }
  public string Colour { get; set; }
  public string CutieMark { get; set; }
  public int Hornsize { get; set; }
}

public class PoniesIntoUnicorns : AbstractTransformerCreationTask&lt;Pony&gt; {
  public PoniesIntoUnicorns() {
    Transform = ponies =&gt; from pony in ponies
                          select new {
                            pony.Name,
                            pony.Colour,
                            pony.CutieMark,
                            Hornsize = pony.Size * Query["hornscalefactor"]
                          }
  }
}
</code></pre>

<p>Okay, contrived example but this feature isn't for me and my pony database, it's for you and your requirements, and you know it's you I'm talking about because you're looking at the above and going <em>finally, I've been waiting for this</em>.</p>

<p>How do we use the above?</p>

<pre><code>session.Query&lt;Pony&gt;()
       .Where(pony =&gt; pony.Colour === "purple")
       .TransformWith&lt;PoniesIntoUnicorns, Unicorn&gt;()
       .AddQueryInput("hornscalefactor", 0.1)
       .ToArray();
</code></pre>

<p>Pretty simple and effective, glad I could oblige :)</p>

<p>Oh yeah, it works for Load too</p>

<pre><code>var unicorn = session.Load&lt;PoniesIntoUnicorns, Pony&gt;(
              x=&gt; x.AddQueryInput("hornscalefactor", 0.1))
</code></pre>

<p>Not the tidiest API in the world, but I'm sure it will improve as people actually use it.</p>]]></description><link>http://codeofrob.com/entries/custom-query-inputs-in-ravendb.html</link><guid isPermaLink="true">http://codeofrob.com/entries/custom-query-inputs-in-ravendb.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Wed, 27 Feb 2013 09:00:00 GMT</pubDate></item><item><title><![CDATA[Talking at DevSum 2013]]></title><description><![CDATA[<p>I've been told I need to tell people when I'm doing talks, so consider this my attempt at making this so.</p>

<p>I'll be talking at <a href="http://devsum.se/">Devsum 2013</a> on the 29th/30th of May on the <a href="http://devsum.se/talare/rob-ashton/">pragmatic outside-in testing of ASP.NET MVC applications</a></p>

<p>I went to Devsum last year and had a lot of fun, last year the venue was just down the road from an Irish Pub and that pretty much put the icing on the cake. Let me know if you're going to be there and we can organise a guinness driven get-together.</p>

<p>Hoping to announce a few other talks here soon, I finish travelling and working around the world at the end of May and I'm available.</p>]]></description><link>http://codeofrob.com/entries/talking-at-devsum-2013.html</link><guid isPermaLink="true">http://codeofrob.com/entries/talking-at-devsum-2013.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Tue, 26 Feb 2013 09:00:00 GMT</pubDate></item></channel></rss>