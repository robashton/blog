<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Rob Ashton's blog]]></title><description><![CDATA[Software development dumping ground]]></description><link>http://codeofrob.com</link><image><url>http://codeofrob.com/img/cover.jpg</url><title>Rob Ashton&apos;s blog</title><link>http://codeofrob.com</link></image><generator>NodeJS RSS Module</generator><lastBuildDate>Fri, 05 Apr 2013 14:29:29 GMT</lastBuildDate><atom:link href="http://feeds.feedburner.com/robashton" rel="self" type="application/rss+xml"/><item><title><![CDATA[Writing an OData parser - starting at the beginning]]></title><description><![CDATA[<p>I'm going off what is specified in both the OData spec and the OData URI conventions document.</p>

<p>This is a bit annoying, because it seems like URI conventions are just that, conventions - and people are free to do what they want (I haven't looked at the metadata spec yet so I'm not sure how discoverable this customisability is, I guess I'll get there during my time on this task)</p>

<p>What I think I can start with, is parsing the following basics</p>

<ul>
<li>The service root itself (http://example.com/service/odata.svc for example)</li>
<li>An entity at this root ( /model )</li>
<li>An entity with a key ( /model(1) )</li>
</ul>

<p><strong>How I'll develop this</strong></p>

<p>This is yet another task I'll probably write tests for as I go so I can document how far I've gotten and have a safety net as I no doubt make lots of mistakes.</p>

<p>I'll copy and paste code from the old OData parser as I need it and as I write the tests to support it, in this even the legacy code will end up with coverage.</p>

<p>In this way, I hope to be able to hand this over to Rulemotion in its semi-complete state but with a nice document (the tests) explaining what is covered so far.</p>

<p><strong>My first few tests</strong></p>

<p>I'll not bother covering the order in which I do this, as it's pretty similar to how I did the JSON parser, except I can make a few more assumptions because I know a but more about how OMeta works.</p>

<pre><code>function test(input, entry, expectation) {
  describe("Parsing " + input, function() {
    var parser = ODataParser.createInstance()
    var result = parser.matchAll(input, entry)
    expectation(result)
  });
}

test("/", "OData", function(result) {
  it("Service root should have no model", function() {
     assert.equal(result.resource, null)
  })
})

test("/model", "OData", function(result) {
  it("should have the resource specified", function() {
     assert.equal(result.resource, 'model')
  })
})

test("/model(1)", "OData", function(result) {
  it("should have the resource specified", function() {
     assert.equal(result.resource, 'model')
  })
  it("should have the key specified for the source", function() {
     assert.equal(result.key, '1')
  })
})
</code></pre>

<p>After the first couple of tests, setting up the parser etc was a ball-ache so I fixed it.</p>

<p>I'm missing out the bit where I specify what the service root is, I'll come back to it later as I'm more interested in parsing the path itself.</p>

<p>This is what I wrote to support the above tests:</p>

<pre><code>ometa ODataParser 
  Number = &lt;digit+&gt;:d -&gt; parseInt(d, 10),

  OData = (
      PathSegment
    | '/'
  ) 
  ,

  PathSegment = 
      '/'
        ResourceName:resource
        (
          ("(" Number:key ")")?
        ) -&gt; { resource: resource, key: key }
      ,

  ResourcePart =
    &lt;    (   letter
      |    '_'
      )+
    &gt;:resourcePart
    -&gt; resourcePart.replace(new RegExp('_', 'g'), ' '),

  ResourceName =
    &lt;    ResourcePart
      (    '-'
        ResourcePart
      )*
    &gt;
}
</code></pre>

<p>Things of note</p>

<ul>
<li>I'm currently returning the model from PathSegment as { resource: resource, key: key }, I'll end up making something else for this I think</li>
<li>The "key" is optional, if it's not there then it will simply be undefined, this is what that question mark is for after those braces</li>
<li>The ResourcePart and ResourceName are copied from the old code and simply convert underscores into spaces, I haven't bothered writing tests for this as I've not checked what ODatas rules are for entity names yet (It's likely to be a bit more complicated than "any text at all")</li>
</ul>

<p>This is all very rudimentary, now - looking at the URI conventions, they seem to support arbitrary paths into object relationships like so:</p>

<pre><code>/model(1)/children(1)/otherchildren(1)/field
</code></pre>

<p>This suggests I probably want to recurse in order to build up this sequence</p>

<pre><code>test("/model(1)/child", "OData", function(result) {
  it("should have the resource specified", function() {
     assert.equal(result.resource, 'model')
  })
  it("should have the key specified for the resource", function() {
     assert.equal(result.key, '1')
  })
  it("should have the child specified", function() {
     assert.equal(result.resource.next, 'child')
  })
})
</code></pre>

<p>Not sure if this is an appropriate representation, but it'll do for now until I find out how we're going to be consuming this model.</p>

<p>Having just arrived at the hotel and written this all on the boat, I'll defer having a look at how to do this until tomorrow, I've already passed the balmers peak.</p>]]></description><link>http://codeofrob.com/entries/writing-an-odata-parser---starting-at-the-beginning.html</link><guid isPermaLink="true">http://codeofrob.com/entries/writing-an-odata-parser---starting-at-the-beginning.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Fri, 05 Apr 2013 09:30:00 GMT</pubDate></item><item><title><![CDATA[Building an OData parser in OMeta]]></title><description><![CDATA[<p>Now I've got myself up to speed in OMeta (which took me an embarassingly difficult couple of days!), I can begin work on a "programming task" for the rest of my stay at Rulemotion's office in Athens.</p>

<p><strong>First off, the context</strong></p>

<p>Rulemotion is doing some pretty cool stuff with SBVR relating to Model First Development. It goes a little bit like this:</p>

<ul>
<li>Write about your system in SBVR</li>
<li>Use OMeta to parse this information</li>
<li>Generate a database with validation/constraints</li>
<li>Generate an OData API over the top of this system</li>
</ul>

<p>It's a little <em>more</em> than that, but the discussion of "what the products are" would take a few blog posts in themselves and the task of going over this topic is more related to the other half of my job whilst here at Rulemotion (plus, I'm neither employed or qualified to write marketing materials!).</p>

<p>What I <em>can</em> say is that the general trend at Rulemotion seems to be "Use OMeta for parsing things", and in doing this they automatically get a lot of things for free (chiefly, the editor that they're using/creating for supporting development gets its auto-completion and highlighting from OMeta, which is pretty super.)</p>

<p>What we actually have at the moment is a situation that has evolved like so:</p>

<ul>
<li>There was a custom HTTP API</li>
<li>They moved to OData</li>
<li>They wrote a partial OData parser on top of a lot of their other code</li>
<li>This OData parser is dependent on the underlying model generated by SBVR</li>
<li>It would be preferable if the OData parser was independent and more complete in itself</li>
<li>Then the model generated by the parser can be consumed by their code and related to the model</li>
</ul>

<p>There is also hope to open source the OData parser too, which is also super cool.</p>

<p><strong>Where do I come in?</strong></p>

<ul>
<li>Well, my first task was to get up to speed on OMeta, I can tick that one off well enough!</li>
<li>My next task is to add some features to the existing OData parser, I manage this after stepping through the generated JS to understand more about what is going on and writing some tests around what seems to be desired</li>
<li>At this point there is a decision to make about how to proceed</li>
</ul>

<p>It is going to start being annoying building up a pile of OData parsing code that is coupled to the underlying model, so I'm going to give writing the OData parser a go, and at least try to reach feature parity with what they have now (I can re-use quite a lot of the code).</p>

<p>There is a little bit of discussion based around a semi-facetious remark I made about using OMeta to parse the ABNF for OData into OMeta, but while this would provide a more "complete" solution, would still involve having to write the semantic output code and might not necessarily be the best use of time given they only need a subset of OData.</p>

<p>So I've decided to give writing an OData parser a go over the weekend and see how far I get, I'll be holidaying it up on a Greek island off the coast of Athens and applying Mojito Driven Development so I make no promises about productivity...</p>]]></description><link>http://codeofrob.com/entries/building-an-odata-parser-in-ometa.html</link><guid isPermaLink="true">http://codeofrob.com/entries/building-an-odata-parser-in-ometa.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Thu, 04 Apr 2013 09:30:00 GMT</pubDate></item><item><title><![CDATA[Building a basic JSON parser in OMeta]]></title><description><![CDATA[<p>I learn by doing, not by reading books, this is bit of a disability when first encountering something <em>completely</em> new because it's hard to start until you know something but learning something when you can't learn by reading is a real pain!</p>

<p>Nevertheless, I decided the best way to crack the nut of OMeta would be to write something to read JSON (it has already been done so I've got something to refer to if I get stuck), and I don't need to make it complete - just functional enough to say "okay, I can do something more real in OMeta now"</p>

<p>Oh, and because this is something new and different, my spike into it will involve feeling my way through the problem with tests. I can't do it all in-line hardcore TDD-style in the test functions and extract because of the OMeta compilation phase - but it should be good enough for fast feedback while learning.</p>

<p>So first off, I want to know how to match an empty object</p>

<pre><code>{}
</code></pre>

<p>That's my empty object there, the simplest possible blob of JSON I can come up with :-)</p>

<pre><code>describe("matching an empty object", function() {
  var input = "{}"
  var result = JsonReader.matchAll(input, "obj")
  assert.deepEqual(result, {})
})
</code></pre>

<p>So how do I match it?</p>

<pre><code>ometa JsonReader {
  obj = "{}" -&gt; {}
}
</code></pre>

<p>How about matching a single numerical value?</p>

<pre><code>describe("matching a single numerical value", function() {
  var input = "400"
  var result = jsonreader.matchall(input, "num")
  assert.equal(result, 400)
})
</code></pre>

<p>This won't work for long, but it's the easiest solution - match <em>anything</em> over and over again, then parse the result as an integer</p>

<pre><code>ometa JsonReader {
  num = anything+:x -&gt; parseInt(x.join(''), 10),
  obj = "{}" -&gt; {}
}
</code></pre>

<p>How about matching some text? we need to match quoted text as it can be both a value or a key.</p>

<pre><code>describe("Matching a single string value", function() {
  var input = '"hello"'
  var result = JsonReader.matchAll(input, "str")
  assert.equal(result, "hello")
})
</code></pre>

<p>Opting for the simplest I can think of, <em>match anything and them remove the quotes.</em></p>

<pre><code>ometa JsonReader {
  str = anything+:x -&gt; x.join('').replace(/\"/g, ''),
  num = anything+:x -&gt; parseInt(x.join(''), 10),
  obj = "{}" -&gt; {}
}
</code></pre>

<p>This will come back to bite me I'm sure - how about matching a basic key value pair?</p>

<pre><code>describe("Matching a key value pair with a numerical value", function() {
  var input = '"foo":1337'
  var result = JsonReader.matchAll(input, "kvp")
  assert.deepEqual(result, [ "foo", 1337 ])
})
</code></pre>

<p>Can we try this? <em>Match a string, then a colon, then a number</em></p>

<pre><code>ometa JsonReader {
  kvp = str:k ":" num:v -&gt; [ k, v ],
  str = anything+:x -&gt; x.join('').replace(/\"/g, ''),
  num = anything+:x -&gt; parseInt(x.join(''), 10),
  obj = "{}" -&gt; {}
}
</code></pre>

<p>Well we can, but it won't pass because we're being lazy and matching "anything" for both strings and numbers. (We're being greedy and it'll try and match quotes and braces and then fail) </p>

<p>Back to the drawing board, can we specify what we mean by an expected character in a string?</p>

<pre><code>describe("Matching the character 'a'", function() {
  var input = 'a'
  var result = JsonReader.matchAll(input, "char")
  assert.deepEqual('a', result)
})
</code></pre>

<p>Well yes, there is only one! So I'll cheat and just match <em>anything</em> again :P</p>

<pre><code>ometa JsonReader {
  char = anything:x -&gt; x,
  kvp = str:k ":" num:v -&gt; [ k, v ],
  str = anything+:x -&gt; x.join('').replace(/\"/g, ''),
  num = anything+:x -&gt; parseInt(x.join(''), 10),
  obj = "{}" -&gt; {}
}
</code></pre>

<p>How about not managing to map something that isn't a character? <em>anything</em> isn't going to work remember!</p>

<pre><code>describe("Character matching doesn't match quotes", function() {
  var input = '"'
    , thrown = null
  try {
    JsonReader.matchAll(input, "char")
  } catch(ex) {
    thrown = ex
  }
  assert.notEqual(thrown, null)
})
</code></pre>

<p>Not too hard, just <em>check that we haven't got a double quote and then match anything else</em>:</p>

<pre><code>ometa JsonReader {
  char =  (
            ~'"'
            anything
          ):x -&gt; x,
  kvp = str:k ":" num:v -&gt; [ k, v ],
  str = anything+:x -&gt; x.join('').replace(/\"/g, ''),
  num = anything+:x -&gt; parseInt(x.join(''), 10),
  obj = "{}" -&gt; {}
}
</code></pre>

<p>How about going back to that original test with the string matching?</p>

<pre><code>describe("Matching a single string value", function() {
  var input = '"hello"'
  var result = JsonReader.matchAll(input, "str")
  assert.equal(result, "hello")
})
</code></pre>

<p>What happens if we come to a quote in the middle of the data?</p>

<pre><code>describe("Matching a string value with pre-mature quote", function() {
  var input = '"hel"lo"'
  var result = JsonReader.matchAll(input, "str")
  assert.equal(result, "hel")
})
</code></pre>

<p>If we say that rather than matching "anything" for str, we want to match <em>only a collection of "char" with quotes around it</em> then..</p>

<pre><code>ometa JsonReader {
  char =  (
            ~'"'
            anything
          ):x -&gt; x,
  kvp = str:k ":" num:v -&gt; [ k, v ],
  str = '"' char+:x '"' -&gt; x.join(''),
  num = anything+:x -&gt; parseInt(x.join(''), 10),
  obj = "{}" -&gt; {}
}
</code></pre>

<p>Now can we run that key-value one again?</p>

<pre><code>describe("Matching a key value pair with a numerical value", function() {
  var input = '"foo":1337'
  var result = JsonReader.matchAll(input, "kvp")
  assert.deepEqual(result, [ "foo", 1337 ])
})
</code></pre>

<p>Huzzah. Functional.</p>

<p>How about a key-value with a string value?</p>

<pre><code>describe("Matching a key value pair with a string value", function() {
  var input = '"foo":"bar"'
  var result = JsonReader.matchAll(input, "kvp")
  assert.deepEqual(result, [ "foo", "bar" ])
})
</code></pre>

<p>Well this is where the our parsing language shines, as we can specify <em>"any valid value", where "value -> num | str"</em></p>

<pre><code>ometa JsonReader {
  char =  (
            ~'"'
            anything
          ):x -&gt; x,
  value = (
            num
          | str
          ):x -&gt; x,
  kvp = str:k ":" value:v -&gt; [ k, v ],
  str = '"' char+:x '"' -&gt; x.join(''),
  num = anything+:x -&gt; parseInt(x.join(''), 10),
  obj = "{}" -&gt; {}
}
</code></pre>

<p>Actually, this probably means that matching objects shouldn't be too hard.</p>

<pre><code>describe("Matching a key value pair with an object value", function() {
  var input = '"foo":{}'
  var result = JsonReader.matchAll(input, "kvp")
  assert.deepEqual(result, [ "foo", {}])
}) 
</code></pre>

<p>Just add the obj to the list of possible values expected (<em>note it comes before number because number is still really greedy!</em>)</p>

<pre><code>ometa JsonReader {
  char =  (
            ~'"'
            anything
          ):x -&gt; x,
  value = (
            str
          | obj
          | num
          ):x -&gt; x,
  kvp = str:k ":" value:v -&gt; [ k, v ],
  str = '"' char+:x '"' -&gt; x.join(''),
  num = anything+:x -&gt; parseInt(x.join(''), 10),
  obj = "{}" -&gt; {}
}
</code></pre>

<p>Now, this obj definition isn't actually correct - because objects contain one or more key value pairs.</p>

<pre><code>describe("Matching an object with a single key value pair", function() {
  var input = '{"foo":1337}'
  var result = JsonReader.matchAll(input, "obj")
  assert.deepEqual(result, { foo: 1337 })
})
</code></pre>

<p>This won't actually work because as mentioned above, 'num' is still greedy and needs changing so it only matches numerical digits.</p>

<pre><code>describe("Matching a number with an early termination", function() {
  var input = '133"7'
  var result = JsonReader.matchAll(input, "num")
  assert.equal(result, 133)
})
</code></pre>

<p>Turns out that there is a built-in called 'digit' to help us with this ( there is probably a built-in for strings too, but documentation for OMeta is not very good and I didn't see it)</p>

<pre><code>ometa JsonReader {
  char =  (
            ~'"'
            anything
          ):x -&gt; x,
  value = (
            str
          | obj
          | num
          ):x -&gt; x,
  kvp = str:k ":" value:v -&gt; [ k, v ],
  str = '"' char+:x '"' -&gt; x.join(''),
  num = &lt;digit+&gt;:x -&gt; parseInt(x, 10),
  obj = "{}" -&gt; {}
}
</code></pre>

<p>Groovy, now perhaps we can try that test again:</p>

<pre><code>describe("Matching an object with a single key value pair", function() {
  var input = '{"foo":1337}'
  var result = JsonReader.matchAll(input, "obj")
  assert.deepEqual(result, { foo: 1337 })
})
</code></pre>

<p>I decide to call an external function to help build up the object which has key/value pairs - that's the <em>makeObject(args):output</em> bit:</p>

<pre><code>ometa JsonReader {
  char =  (
            ~'"'
            anything
          ):x -&gt; x,
  value = (
            str
          | obj
          | num
          ):x -&gt; x,
  kvp = str:k ":" value:v -&gt; [ k, v ],
  str = '"' char+:x '"' -&gt; x.join(''),
  num = &lt;digit+&gt;:x -&gt; parseInt(x, 10),
  obj = "{}" -&gt; {}
      | "{" (
              kvp:kv 
              makeObject(kv):output
            ):output
        "}" -&gt; output
}

JsonReader.makeObject = function(kv) {
  var obj = {}
  obj[kv[0]] = kv[1]
  return obj
}
</code></pre>

<p>Finally, how about lists of arguments?</p>

<pre><code>describe("Matching an object with multiple key value pairs", function() {
  var input = '{"foo":43,"bar":343}'
  var result = JsonReader.matchAll(input, "obj")
  assert.deepEqual(result, { foo: 1337, bar: 343 })
})
</code></pre>

<p>Turns out that there is another construct in OMeta for this scenario (The best way to find these constructs is to read the ometa-js source - sorry.) <em>listOf</em></p>

<pre><code>ometa JsonReader {
  char =  (
            ~'"'
            anything
          ):x -&gt; x,
  value = (
            str
          | obj
          | num
          ):x -&gt; x,
  kvp = str:k ":" value:v -&gt; [ k, v ],
  str = '"' char+:x '"' -&gt; x.join(''),
  num = &lt;digit+&gt;:x -&gt; parseInt(x, 10),
  obj = "{}" -&gt; {}
      | "{" (
              listOf(`kvp, ','):kvs
              makeObject(kvs):output
            )
        "}" -&gt; output
}

JsonReader.makeObject = function(kvs) {
  var obj = {}
  for(var i = 0 ; i &lt; kvs.length; i++) {
    var kv = kvs[i]
    obj[kv[0]] = kv[1]
  }
  return obj
}
</code></pre>

<p><strong>Summary</strong></p>

<p>It's pretty trivial to build up a parser for something if you built up a test suite as you go and have some way of</p>

<ul>
<li>Seeing examples for the syntax that OMeta supports</li>
<li>Seeing examples for the built-ins inside OMeta</li>
</ul>

<p>With this behind me, I can actually go and look at the source code of this company and see about doing something useful from this learning!</p>]]></description><link>http://codeofrob.com/entries/building-a-basic-json-parser-in-ometa.html</link><guid isPermaLink="true">http://codeofrob.com/entries/building-a-basic-json-parser-in-ometa.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Wed, 03 Apr 2013 09:30:00 GMT</pubDate></item><item><title><![CDATA[Learning OMeta in Greece]]></title><description><![CDATA[<p>This week I'm in Athens (well, probably a month or so ago now there is a massive blog post queue), working for <a href="http://rulemotion.com/">Rulemotion</a> who use these technologies amongst others:</p>

<ul>
<li>OMeta</li>
<li>SBVR</li>
<li>JS</li>
<li>CoffeeScript</li>
<li>NodeJS</li>
<li>OData</li>
</ul>

<p>My job in Athens is two fold:</p>

<ul>
<li>Write some OMeta stuff somewhere (They have a specific project/task - don't worry)</li>
<li>Look at the overall project and give my honest feedback</li>
</ul>

<p>Well, giving honest feedback is something I can do - however my relationship with OMeta and SBVR is that I've never heard of them.</p>

<p><strong>It looks like I have some learning to do</strong></p>

<p>So what IS OMeta? It turns out that I do have a little experience in this area because OMeta is a expression parsing language, and like most people I've written a few parsers and compilers in my few years as a software developer.</p>

<p>OMeta is a bit different in that it had a specific goal - chiefly that of making it fast to prototype new languages/parsing constructs, and indeed it can be used to do pretty much the whole chain in the compilation process (parsing, intepreting and compilation).</p>

<p>You can read the <a href="http://www.vpri.org/pdf/tr2008003_experimenting.pdf">original paper</a>, it's the best source of information apart from just reading the implementation.</p>

<p><em>I'm not going to do a blog series on this, just wanted to throw up some stuff as I learned it :)</em></p>

<p><strong>What I'm using</strong></p>

<p>I have <a href="https://github.com/Page-/ometa-js">OMeta-JS</a>, and I'm doing most of my playing in the web browser with <a href="https://github.com/Page-/ometa-js/tree/highlighting/examples/highlighting">An OMeta parsing Ometa demo</a>.</p>

<p>If I make some OMeta in the textbox and then go to the other textbox, I can copy and paste the JS into a repl and play around, it's not the most super effective way of working but I suspect this demo will be improved on to make it easier.</p>

<p><strong>So again, what is OMeta?</strong></p>

<p>I told you, it's a parsing language, check out the following:</p>

<pre><code>ometa MyParser {
  greeting = "Hello"
}
</code></pre>

<p>If I compile this, I'll get the following:</p>

<pre><code>var MyParser = OMeta._extend({
  greeting: function() {
      var $elf = this, _fromIdx = this.input.idx;
      return this._applyWithArgs("token", "Hello");
  }
});
</code></pre>

<p>Which I can use in some code</p>

<pre><code>MyParser.matchAll("Hello", "greeting")    : Success
MyParser.matchAll("Goodbye", "greeting")  : Failure
</code></pre>

<p>What I can also do is transform these expressions into other expressions</p>

<pre><code>ometa MyParser {
  greeting = "Hello" -&gt; "Howdy"
}

MyParser.matchAll("Hello", "greeting")    : "Howdy"
</code></pre>

<p>And I can also build up matches out of other matches</p>

<pre><code>ometa MyParser {
  greeting = "Hello",
  bob      = "Bob",
  sentence = greeting bob
}

MyParser.matchAll("Hello Bob", "sentence")  : Success
MyParser.matchAll("Hello James", "sentence")  : Failure
MyParser.matchAll("Bob", "bob")  : Success
</code></pre>

<p>And this means I can build up transformations from simple expressions:</p>

<pre><code>ometa MyParser {
  greeting = "Hello" -&gt; "Howdy ",
  bob      = "Bob"   -&gt; "Bobby"
  sentence = greeting:g bob:b -&gt; (g + b)
}

MyParser.matchAll("Hello Bob", "sentence")  : "Howdy Bobby"
MyParser.matchAll("Hello James", "sentence")  : Failure
</code></pre>

<p>Now obviously we don't use this language for parsing daft sentences like the above, what we do is use it to build up expectations around structures such as program code.</p>

<pre><code>ometa CParser {
  type    = identifier,
  argList = listOf(arg, ","),
  methodBody = "{" statementList "}"
  program = type "main("  argList ")" methodBody
}
</code></pre>

<p>And so on...</p>

<p>Now this is getting ahead of myself, let me write about how I got to grips with OMeta by writing a JSON parser...</p>]]></description><link>http://codeofrob.com/entries/learning-ometa-in-greece.html</link><guid isPermaLink="true">http://codeofrob.com/entries/learning-ometa-in-greece.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Tue, 02 Apr 2013 09:30:00 GMT</pubDate></item><item><title><![CDATA[I am interested in talking about work]]></title><description><![CDATA[<p>By the time July rolls around, I will have spent half a year travelling around <a href="/entries/i-am-not-looking-for-a-job.html">working for free</a>, learning new technologies, picking up new skills and in general becoming a <a href="/entries/a-note-on-working-hours-and-working-at-home.html">happier person</a>.</p>

<p>I can't keep doing this forever, aside from the slow drip of money out of my bank accounts I already miss having a permanent base to go back to.</p>

<p>So, from July I'll need to start earning a regular or semi-regular income once more, and it is therefore time to publicise let the world know I'm ready for it again.</p>

<p><strong>Who am I?</strong></p>

<p>I have spent the past decade delivering software primarily in the Microsoft space with a focus on C# and JS. I mostly end up in charge of the software projects I'm involved with and have a focus on keeping the team's options open and able to react to change.</p>

<p>I am a regular on the conference circuit and tend to talk about testing, javascripts and pragmatic development in .NET.</p>

<p>I write code, I write a <em>lot</em> of code, I have the ability to learn things <em>quickly</em> and start delivering early on after starting a new role and I am an enthusiastic sharer of ideas and opinions.</p>

<p><strong>What I'm offering</strong></p>

<p>I am offering my nimble fingers and active mind to anyone in the world, I can...</p>

<ul>
<li>Help build features, and teach at the same time</li>
<li>Run workshops on JS (back-end or front-end), ASP.NET MVC and RavenDB</li>
<li>Improve your testing strategies</li>
<li>Help remove obstacles from your development process!</li>
<li>Help you move forward from your legacy issues (Silverlight to ...?)</li>
<li>Advise on your long-term architectural problems/goals</li>
</ul>

<p>I am good at shipping code, I am knowledgable in many areas of .NET development and have spent a lot of time using and developing against alternative databases for the past few years.</p>

<p><strong>What I looking for</strong></p>

<p>I am flexible, I ask for you to be the same. I am always happy to travel on-site providing my expenses are covered, but I think I am going to start renting a house again and will probably want to be spending some time there in between trips.</p>

<p>I am most likely looking for short-term consultancy work (4-5 days) and short-term development work (2-8 weeks). </p>

<p>I am looking for work in any country; as mentioned above I'm happy to travel.</p>

<p>My rates are flexible and tend to vary depending on country, convenience and company. I am looking to sustain a lifestyle and can therefore offer flexibility.</p>

<p><strong>So</strong></p>

<p>Do get in touch at <a href="mailto:robashton@codeofrob.com">robashton@codeofrob.com</a> and ask me for availability from July onwards, I am already starting to fill my calendar with paid work from that date onwards so will likely need advance notice for anything more than a week or so.</p>]]></description><link>http://codeofrob.com/entries/i-am-interested-in-talking-about-work.html</link><guid isPermaLink="true">http://codeofrob.com/entries/i-am-interested-in-talking-about-work.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Mon, 01 Apr 2013 09:30:00 GMT</pubDate></item><item><title><![CDATA[My relationship with SOLID - The overloaded D]]></title><description><![CDATA[<p>My week of SOLID: so far:</p>

<ul>
<li><a href="/entries/my-relationship-with-solid---starting-with-s.html">S</a>ingle responsibility</li>
<li><a href="/entries/my-relationship-with-solid---the-big-o.html">O</a>pen closed</li>
<li><a href="/entries/my-relationship-with-solid---the-misunderstood-l.html">L</a>iskov substitution</li>
<li><a href="/entries/my-relationship-with-solid---seeing-i-to-i.html">I</a>nterface segregation</li>
<li><a href="#">D</a>ependency inversion</li>
</ul>

<p>We've reached D, and that's where the wave we started with L finally hits the shore and materialises into something we can use.</p>

<p><blockquote>
A. High-level modules should not depend on low-level modules. Both should depend on abstractions.
  </blockquote></p>

<p><blockquote>
B. Abstractions should not depend upon details. Details should depend upon abstractions.
  </blockquote></p>

<p>We've been using that Stream example for the last couple of blog entries and we'll pull that out one last time as an example:</p>

<p>Stream is an example of a <em>low level module</em>, and we can consider IStream to be an abstraction of that, which <em>high level modules</em> can consume</p>

<pre><code>public interface IStream {
  void Read();
  void Write();
  void Seek();
}

public class Stream : IStream { etc }
</code></pre>

<p>However, this is still not really an abstraction, the abstraction is a lie because it is an exact mirroring of the actual Stream object. (this is okay sometimes as we'll see below)</p>

<p><strong>How I like to code sometimes</strong></p>

<p>I'm writing a high level module, let's say it's a Controller, and it needs to construct a view model for return to the outside world for rendering in a View of some sort. </p>

<p>Let's mix it up a little bit and do it in NodeJS like I'm pretty much doing everywhere at the moment.</p>

<pre><code>app.get('/ponies', function(req, res) {

})
</code></pre>

<p>I don't like to typically make decisions in a long-term project about persistence until I know more about my use cases, and indeed in this case I don't even want to care that persistence is even happening - instead, what I'll usually do is say to the outside world, <em>I need something that houses ponies</em></p>

<pre><code>module.exports = function(app, stable) {
    app.get('/ponies', function(req, res) {
      var ponies = stable.availablePonies(req.params.count, req.params.pagesize, req.params.page)
      res.send(ponies)
    })
}
</code></pre>

<p>What is stable? Well, stable was instantiated by the Application Root and I don't care what stable is, a basic implementation (and indeed the one I start off with is)</p>

<pre><code>stable = new InMemoryStable()
</code></pre>

<p>This allows me to write features quickly, allows me to write functional tests that are relatively fast and allows me to iterate on these things quickly and defer the decision about persistence until it becomes necessary to deal with it (if it does at all)</p>

<p>The important point here is that implicit interface being defined here (I'm in JS, we haven't got a real interface), let's call it "IHousePonies" has been dictated by the high level module and it doesn't care what the low level code behind that interface does.</p>

<p>That's an inversion of responsibility and it's a good one too because it means I'm unlikely to run into violations of <a href="/entries/my-relationship-with-solid---seeing-i-to-i.html">ISP</a> because the high level module <em>requires</em> that the functionality on that interface be present on all implementations of that interface.</p>

<p>This is close-ish to what some people would describe as using <a href="http://martinfowler.com/bliki/RoleInterface.html">role interfaces</a> which are worth reading up on. Certainly when I'm putting my software-engineering hat on and I'm working on a project where things are likely to be complex, there are likely to be quite a few moving parts and code quality is important I'll lean in this direction.</p>

<p><strong>How I like to code other times</strong></p>

<p>If I'm in .NET helping build the standard ASP.NET MVC application that clients tend to want, I'll pull out RavenDB which has an in-memory mode and use its interfaces directly across my application. They are an abstraction already and that abstraction hides</p>

<ul>
<li>The use of RavenDB as an embedded database</li>
<li>The use of RavenDB as a remote HTTP database</li>
<li>The use of RavenDB as an in-memory database (for testing)</li>
</ul>

<p>Sticking our own abstraction around this makes very little sense, although if we have complicated domain logic we might end up with some coordinators between the controller and RavenDB.</p>

<p>In most cases the effort of building up abstractions over time from our high level controllers won't really have much of a pay off.</p>

<p>Of course, if elsewhere in that project I have the need to do something that isn't CRUD, then the use of my own abstractions will come into things because hiding the low level details from the high level consumers is still important. These abstractions can be built up over time as needed, rather than defining them all up front.</p>

<p><em>Insert any other similar technologies into the above scenarios and it's pretty much the same</em></p>

<p><strong>How any of this can relate to testing</strong></p>

<p>Well, if we're describing our dependencies on other modules as interfaces that we own, we can create in memory representations of those dependencies in the form of mocks, stubs or full-blown in-memory implementations (the latter being my personal preference in most cases)</p>

<p>Having ownership of the interfaces that we rely on means we can dictate the desired behaviour via tests (or in some languages code contracts), and it means that the tests we write for our module can make assumptions about how the code behind that abstraction is going to work.</p>

<p>Coupling this design principal with techniques such as <a href="http://en.wikipedia.org/wiki/Dependency_injection">Dependency Injection</a> means that we can make it very clear to the outside world from our module what abstractions we rely on, and allow the outside world to make decisions about what it is we are going to actually get.</p>

<p><strong>How it can all go wrong - attack of the killer interfaces</strong></p>

<p>What happens in a lot of projects is that we decide that the reason for DI is for testing, and isolation is really important so we need to have abstractions for everything, and almost everywhere we end up with</p>

<pre><code>public class Something
public interface ISomething
</code></pre>

<p>Because every class needs an interface in order to be mockable - we forget to apply the inversion part of dependency inversion and instead we just focus on dependencies for our tests.</p>

<p>This isn't helped by most popular IOC frameworks and their default convention that it'll automatically bind instances like the above for us.</p>

<p>This is awful, when we think about inverting the relationship between our high level modules and low level modules, we should be thinking about it in terms of pay-off and not dancing around for the sake of writing pointless low level tests for code with little real behaviour.</p>

<p>We should be limiting our abstractions to the tune of rather than thinking about everything at the class level, thinking about things at the module level (which could be a collection of classes that talk to each other and require some external data/input)</p>

<p><strong>SOLID - where did it all go wrong?</strong></p>

<p>I could go on about this till the cows come home, but I don't want to because I've got some stuff to build, so I'll leave it here with a final note:</p>

<p><strong>ALL</strong> of the SOLID principles are <em>great</em>, as a guideline for thinking about code as we write it, I'm not disagreeing with that at all. What I disagree with are statements that say "MUST" or "NOT ALLOWED" etc - because most developers are not master craftsmen (or whatever you call yourselves these days) and trying to make them write code as if they are is what leads to disaster.</p>

<p>Most code should be allowed to grow organically, and caution should be exercised in making sure that we don't end up with that big ball of mud that everybody fears - absolutely. Trying to avoid that big ball of mud by blindly following the SOLID principles leads to the creation of a big haystack where the actual functionality is a needle hidden somewhere underneath.</p>

<p><strong>fin</strong></p>]]></description><link>http://codeofrob.com/entries/my-relationship-with-solid---the-overloaded-d.html</link><guid isPermaLink="true">http://codeofrob.com/entries/my-relationship-with-solid---the-overloaded-d.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Fri, 29 Mar 2013 09:30:00 GMT</pubDate></item><item><title><![CDATA[My relationship with SOLID - Seeing I to I]]></title><description><![CDATA[<p>The <a href="http://en.wikipedia.org/wiki/Interface_segregation_principle">interface segregation principle</a> is slightly more relevant to the code that I write day to day than <a href="/entries/my-relationship-with-solid---the-misunderstood-l.html">Liskov</a>.</p>

<p><blockquote>
  The interface-segregation principle (ISP) states that no client should be forced to depend on methods it does not use. 
  </blockquote></p>

<p>I talked <a href="/entries/my-relationship-with-solid---the-misunderstood-l.html">yesterday</a> about the Stream class, and showed how</p>

<pre><code>public class Stream {
  public virtual bool CanRead { get; }
  public virtual bool CanWrite { get; }
  public virtual bool CanSeek { get; }

  public virtual void Read(Byte[] buffer, int offset, int amount) {}
  public virtual void Write(Byte[] buffer) {}
  public virtual void Seek(int offset){}
}
</code></pre>

<p>Wasn't necessarily a violation of Liskov because the variations in its behaviour were well described by those slightly uncomfortable properties.</p>

<p>However, those awkward properties definitely point towards a violation of the ISP. Why? Because we have an interface - (in this case, an implicit one dictated by the Stream base class) which looks like this:</p>

<pre><code>interface Stream {
  void Read(Byte[] buffer, int offset, int amount);
  void Write(Byte[] buffer);
  void Seek(int offset);
}
</code></pre>

<p>And yet not all Streams can do all of those things, hence we resort to those rather opaque properties.</p>

<p>Perhaps another way we'll often see violations of this in code (let's say we didn't have those properties) is the checking for specific types in methods that use the interface such as:</p>

<pre><code>if(stream is FileStream)
  stream.Write(bytes, 0, bytes.Length)
</code></pre>

<p><em>shudder</em>, this stuff be bad as not only do we open up ourselves for runtime crashes when a consumer passes in something we don't recognise but we're writing opaque behaviour into our code that'll confuse consumers of that code.</p>

<p><strong>Interface segregation to the rescue</strong></p>

<pre><code>public interface IRead {
  void Read(Byte[] buffer, int offset, int amount);
}

public interface IWrite {
  void Write(Byte[] buffer);
}

public interface ISeek {
  void Seek(int offset);
}
</code></pre>

<p>When we have methods that require something that Reads, we can pass in IRead, when we have methods that require something that Writes can pass in IWrite, and this is great, what if we need something that Reads <em>and</em> Writes</p>

<pre><code>public interface IReadAndWrite : IRead, IWrite {}
</code></pre>

<p>Okay, maybe we can do this, but what about something that Reads Writes and Seeks?</p>

<pre><code>public interface IReadAndWriteAndSeek : IRead, IWrite, ISeek {}
</code></pre>

<p>Now this is a bit contrived, but this is one of the reasons the .NET team made the decision to go with the CanRead/CanWrite approach beacuse otherwise we'd either simply revert to checks like</p>

<pre><code>if(Stream is IRead)
</code></pre>

<p>or have to do stuff with generics like</p>

<pre><code>void WriteToFile&lt;T&gt;(T stream, string filename) where T : IRead, IWrite, ISeek
</code></pre>

<p><em>shudder</em></p>

<p><strong>Framework Engineering</strong></p>

<p>If you're writing a framework, first off stop and don't do that... but okay, if you're writing a framework these are the compromises that you'll sometimes have to make - and that's okay.</p>

<p>Well described behaviour that's a little bit awkward is better than having a pile of interfaces that we have to dance around if we want to achieve something meaningful.</p>

<p>As mentioned yesterday, I actually don't mind the .NET teams decision to break ISP here because the usage of these streams would be much harder with the number of variations in behaviour a stream can actually have.</p>

<p>Tomorrow we'll look at why ISP is irrelevant in the grand scheme of things however, as we reach the final entry in this little brain-dump and talk about DI and how it encourages the use of role interfaces.</p>]]></description><link>http://codeofrob.com/entries/my-relationship-with-solid---seeing-i-to-i.html</link><guid isPermaLink="true">http://codeofrob.com/entries/my-relationship-with-solid---seeing-i-to-i.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Thu, 28 Mar 2013 09:30:00 GMT</pubDate></item><item><title><![CDATA[My relationship with SOLID - The misunderstood L]]></title><description><![CDATA[<p>I imagine my statement yesterday that OCP is "dead" will be the big bomb out of all of these blog entries, but nevertheless we push forward and look at the <a href="http://en.wikipedia.org/wiki/Liskov_substitution_principle">Liskov substitution principle</a></p>

<p><blockquote>
   If S is a subtype of T, then objects of type T may be replaced with objects of type S (i.e., objects of type S may be substituted for objects of type T) without altering any of the desirable properties of that program (correctness, task performed, etc.)
  </blockquote></p>

<p>This is one of those cases where things <em>just make sense</em>, and yet people always have a hard time describing exactly what it is. I'm probably not going to spend much time on it in this blog entry because it's really boring and I doubt I can do a better job of explaining it than anybody else.</p>

<p><em>Instead, my relationship with it..</em></p>

<p>Well, I'll start off by saying that day to day that Liskov means nothing to me, it's almost a rule that strictly applies itself to inheritance situations and because I'm primarily these days working in langauges that don't have any real native inheritance mechamisms (prototype chaining doesn't really count), this isn't something that affects me.</p>

<p>Hell, you know what? When I'm working in C# it is something that I don't run into because inheritance is generally something I don't use or take advantage of (because composition is usually simpler etc etc). You can't change the behaviour of an object through inheritance if you never use it.. right? :-)</p>

<p>Nevertheless, I want an example anyway, and I want one we're all familiar with so I'll hit up the .NET framework, and while I can remember vague instances of being annoyed about violations in UI frameworks like WinForms those days a long behind me and I can't remember any of them.</p>

<p>Indeed it's actually hard to think of any examples of this in the .NET framework which aren't actually a violation of our next guideline ("interface segregation"),  and throughout the "SOLID years" if you look at other people's writing on this subject, most writings about Liskov are actually about Interface Segregation.</p>

<p>So let's hit up a commonly quoted example that is almost a violation and talk about it a little bit.</p>

<p><em>The oft-quoted Stream example</em></p>

<p>Looking at the design principles that produced it, the reasoning is quite clear about why the .NET team went in the direction they did with this one, let's expand and use a simplified version of the Stream class.</p>

<pre><code>public class Stream {
  public virtual void Read(Byte[] buffer, int offset, int amount) {}
  public virtual void Write(Byte[] buffer) {}
  public virtual void Seek(int offset){}
}
</code></pre>

<p>Now, the default behaviour of this is to throw an exception on any of those methods, and derived instances can do proper implementations of these, like so</p>

<pre><code>public class FileStream : Stream {
  public override void Read(Byte[] buffer, int offset, int amount) { // Read from the file }
  public override void Write(Byte[] buffer) { // Write to the file }
  public override void Seek(int offset){ // Seek to a position within the file }
}
</code></pre>

<p>And maybe an implementation that reads from a HTTP request</p>

<pre><code>public class HttpStream : Stream {
  public override void Read(Byte[] buffer, int offset, int amount) { // Read from the file }
  public override void Write(Byte[] buffer) { throw new NotSupportedException(); }
  public override void Seek(int offset){  throw new NotSupportedException(); }
}
</code></pre>

<p>Now, at this point if we were to pass around the Stream object to a method like this</p>

<pre><code>public void ReadStreamIntoFile(string filename, Stream stream);
</code></pre>

<p>Then our two streams would work just fine. </p>

<p>However, if we were to pass the stream object into this method:</p>

<pre><code>public void WriteFileIntoStream(string filename, Stream stream);
</code></pre>

<p>The FileStream would function correctly, and the HttpStream would throw a NotSupportedException.</p>

<p>This is why the Stream class is often quoted as an example, the derived instances change the behaviour in program-breaking ways.</p>

<p><em>However</em></p>

<pre><code>public class Stream {
  public virtual bool CanRead { get; }
  public virtual bool CanWrite { get; }
  public virtual bool CanSeek { get; }

  public virtual void Read(Byte[] buffer, int offset, int amount) {}
  public virtual void Write(Byte[] buffer) {}
  public virtual void Seek(int offset){}
}
</code></pre>

<p>The behaviour as described, is that if those properties return true, then the methods are safe to call, if they return false, they're not safe to call.</p>

<p>It's opaque, and feels a bit wrong - but we don't necessarily have a violation of Liskov and we're happy on this front. This is a good example of where the pragmatics of developer usage have overidden the following of arbitrary software-design "rules".</p>

<p><strong>Back to my relationship with Liskov</strong></p>

<p>I do not have a relationship with the Liskov substitution principle because I don't generally write code that has any sort of inheritance chain within it, but if I did - sometimes I guess I'd end up in the situation like the above and that would be okay. I'm okay with that for the most part.</p>

<p><strong>Summary</strong></p>

<p>Liskov is ultimately pretty boring, and unless you're writing code with lots of inheritance it isn't really a problem. Don't write code with lots of inheritance and keep this problem away from you. Winning.</p>

<p>As a design principle, I totally agree with it - changing derived classes behaviour is annoying, don't do it. Okay, sorted.</p>]]></description><link>http://codeofrob.com/entries/my-relationship-with-solid---the-misunderstood-l.html</link><guid isPermaLink="true">http://codeofrob.com/entries/my-relationship-with-solid---the-misunderstood-l.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Wed, 27 Mar 2013 09:30:00 GMT</pubDate></item><item><title><![CDATA[My relationship with SOLID - The big O]]></title><description><![CDATA[<p><strong>Open closed is dead, long live Open closed</strong></p>

<p>I'm blogging about <a href="/entries/my-relationship-with-solid---starting-with-s.html">SOLID</a> for some reason, and now we're onto the beast that set me off:</p>

<p><strong>OCP</strong></p>

<p>Yikes</p>

<p><blockquote>
    They are “Open For Extension”. This means that the behavior of the module can be extended. That we can make the module behave in new and different ways as the requirements of the application change, or to meet the needs of new applications.
  </blockquote></p>

<p>and</p>

<p><blockquote>
    They are “Closed for Modiﬁcation”. The source code of such a module is inviolate. No one is allowed to make source code changes to it.
  </blockquote></p>

<p>Thanks <a href="https://docs.google.com/file/d/0BwhCYaYDn8EgN2M5MTkwM2EtNWFkZC00ZTI3LWFjZTUtNTFhZGZiYmUzODc1/edit?hl=en">Uncle Bob</a>, you're right, this <em>is</em> <a href="http://blog.8thlight.com/uncle-bob/2013/03/08/AnOpenAndClosedCase.html">over-stated</a>, and because it's so over stated, I believe it to be the cause of so many of the over-designed pieces of crap I've had to deal with in my career :-)</p>

<p>This is the conversation I imagine developers having with themselves when writing this stuff, I don't have to imagine too hard because I've been there too:</p>

<p><blockquote>
    What if somebody at some point wants to change this so they can have another behaviour for this value, I'd better use the strategy pattern here instead of this switch statement, but oh my word now I've done that what if somebody wants to use this code from some other system than this one, I'd better stick a command system in front of this and use double dispatch for handling them - but wait, what if other code needs to react from this and do something else, I'd better raise a pile of events, but what if those aren't enough I'd better make sure I split all this behaviours out into their own interfaces so they can be overridden and...
  </blockquote></p>

<p>And on it goes until suddenly what was a couple of classes behind a controller or presenter blow up into a mess of a hundred classes that all do the square root of diddly squat, but together manage to cause a lot of headaches for anybody coming across the code in the future.</p>

<p>Now, I'm sure this wasn't the intent behind these statements, and it sure isn't now - but you know what?</p>

<ul>
<li>I don't really care what the original sentiment was behind Uncle Bob's statement.</li>
<li>I don't really care what it is actually supposed to mean. </li>
<li>I <em>do</em> care that code that I come into contact with doesn't get in my way when I want to add a feature</li>
</ul>

<p>Here is my current thinking on the Big O. Let's make it stand for "Open", and remove the CP.</p>

<p><blockquote>
    Good code is code that another developer can change the behaviour of easily and clearly see the consequences of that change.
  </blockquote></p>

<p>The decisions made when designing a language can have implications on how we satiate this need.</p>

<p>We can look to <a href="http://twitter.com/jonskeet">@jonskeet</a>'s perfect language where "all classes are sealed by default, all methods are sealed by default, all extensibility points are explicitly defined", or we can look at any of the no-holds barred dynamic languages that let us get away with pretty much anything.</p>

<p><strong>Let's take option one</strong></p>

<p>Let's say we <em>do</em> make that call (because Jon is where all of this started, so taking his perspective will help see where he is coming from), then surely if we're going to follow OCP then we have to from the very beginning bake in these extension points on a <em>just in case</em> basis. </p>

<p>Woah! No!!! Stop right there. This is how we end up with the kind of code where we use the strategy pattern everywhere and have a million and one interfaces to describe the act of doing absolutely nothing of consequence at all.</p>

<p><em>The best code is code that can be changed easily</em>, code that is easy to read, code that keeps state close to the behaviour, code that that doesn't attempt magic tricks, code that anybody can read - this is the code meets this standard. </p>

<p><strong>Let's take option two</strong></p>

<p>Now we're in the magical happy land of dynamic languages, and we can just screw over any object by fiddling with its prototype, the rules have gone out of the window. This is the <em>land of possibility</em> people, and we have the <em>power to change things</em>.</p>

<p>Does this mean we haven't got to worry about OCP? Nay - this is not so. Having the ability to change anything is <em>fantastic</em>, people <em>don't</em> know everything when putting together a module, and having everything open by default means that while you wait for the project you're using to have an appropriate extension point you can hack around it and get on building your product.</p>

<p>However, explicitly defined extension points have clearly defined behaviours and are predictable - so are clearly desireable.</p>

<p><strong>Wait a second</strong></p>

<p>Getting off the subject of how languages can affect our decisions, we can look at how our programming culture can have an impact on it. </p>

<p>I find it intensely irritating that the languages that lean towards the "closed by default" design also seem to live in the environment where the code itself is also "closed by default", which means that either the framework authors have to build in extension points for everything imaginable or the users of that framework code have to suffer for it.</p>

<p>I find it intensely amusing that the languages that lean towards the "open by default" design also live in the environment where the code itself is open by default (this is the age of Github), which means that the people with the problem can come in, make the desired change and move on with their projects with barely any thought to it at all.</p>

<p>And this is where I go back on what I said in all the statements above, <em>this</em> is where OCP is now, times have changed since the original sentiment was uttered, we have a lot more open source now and the ideal is:</p>

<ul>
<li>Building small disposable modules that live in package systems and on Github/etc</li>
<li>Primarily building products out of these open source building blocks</li>
<li>These building blocks are either replacable in a few hours work, or easily extended via a pull request</li>
<li>These building blocks can be forked for your project and changes merged from upstream with little effort</li>
</ul>

<p>This changes the very face of things, this changes how we build software - and it means that a strict adherence to OCP becomes largely a thing restricted to stodgy enterprise environments that are slow moving, uncompetitive and slow to get things done (and they're welcome to it)</p>

<p>The true future of OCP is in building these open source little packages that are easily changed or forked, and in that we can find an elegance and simplicity that I find quite pleasing.</p>

<p>If we're forking a module and sticking a new version on it, we're saying that it is no longer the same as the old module, it is new code and the old code still exists too.</p>

<p><strong>Where OCP makes sense</strong></p>

<ul>
<li><p>An example of where this sort of thinking has relevance, is in technical patterns like event sourcing, where once an event becomes committed, it becomes preferable to create new versions of that event rather than modify the original (because events can't be changed, they already happened - etc)</p></li>
<li><p>An example of where this sort of thinking has relevance, is where you have code that starts being modified in the same way repeatedly (such as adding further conditions to an if statement), and this modification has cost and side-effects. Refactoring towards simplicity usually has the effect of removing this sort of repetitive action and indeed that code will become closed to modification over time as it matures.</p></li>
<li><p>Repetition actually has the key, if you're constantly touching  the same piece of code again and again over time, then that piece of code is probably, as <a href="http://twitter.com/gregyoung">@gregyoung</a> puts it, a "bug hive". If you have this sort of pattern in your source control then that code is likely brittle because it hasn't been refactored over time towards making it easier to extend without breaking stuff.</p></li>
<li><p>Coupled with a practical application of the single responsibility principle, we can say that if we're changing a piece of code too much either our requirements are just changing a lot (at which point you're not changing code you're throwing old code away and replacing it), or our code is trying to do too much.</p></li>
</ul>

<p><strong>Summary</strong></p>

<p>So, in the age of tiny disposable modules that do just one thing, OCP is dead (<em>wink</em>) - who'd have thunk it. <em>/dramatic oversimplification</em></p>]]></description><link>http://codeofrob.com/entries/my-relationship-with-solid---the-big-o.html</link><guid isPermaLink="true">http://codeofrob.com/entries/my-relationship-with-solid---the-big-o.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Tue, 26 Mar 2013 10:00:00 GMT</pubDate></item><item><title><![CDATA[My relationship with SOLID - Starting with S]]></title><description><![CDATA[<p>I saw a tweet by <a href="http://twitter.com/jonskeet">@jonskeet</a> the other day which caught my eye:</p>

<p><blockquote class="twitter-tweet"><p>(I know that doubting things like OCP is pretty close to heresy, but it's just <em>never</em> made sense to me.)</p>&mdash; Jon Skeet (@jonskeet) <a href="https://twitter.com/jonskeet/status/309911260701552640">March 8, 2013</a></blockquote>
  <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<p><em>Well okay then</em></p>

<p>Well, obviously I kinda agree with this sentiment if you go and hit up the <a href="http://blog.8thlight.com/uncle-bob/2013/03/08/AnOpenAndClosedCase.html">"Immature" 43 year old Uncle Bob's</a> statement which is as written:</p>

<p><blockquote>
    They are “Closed for Modiﬁcation”. The source code of such a module is inviolate. No one is allowed to make source code changes to it.
  </blockquote></p>

<p>But this got me thinking more widely on my relationship with SOLID as a whole and how that has changed over the years. It reminded me how many times (like the GoF patterns) I've seen an over-zealous and misunderstanding of these concepts <a href="/entries/the-fallacy-of-the-dreyfus-model-in-software-development.html">wreak havoc in codebases</a>.</p>

<p>I've been able to quote the "rules" from SOLID word for word this past half-decade quite easily, but my relationship and understanding of how these seemingly innocuous statements impact my code has changed over time much like <a href="/entries/uncle-bobs-viewpoint-considered-harmful.html">my relationship and understanding of TDD</a></p>

<p>So, for the next 5 entries, I will jot down my current relationship with SOLID without too much thought or proof-reading (<em>Okay, I lied, I got a few people to read these because I wanted to avoid pedantry</em> - thanks folk)</p>

<p><strong><a href="http://en.wikipedia.org/wiki/Single_responsibility_principle">The single responsibility principle</a></strong></p>

<p><blockquote>
    In object-oriented programming, the single responsibility principle states that every class should have a single responsibility, and that responsibility should be entirely encapsulated by the class. 
  </blockquote></p>

<p>This is the kind of statement that leads to <a href="http://ayende.com/blog/154177/limit-your-abstractions-so-what-is-the-whole-big-deal-about">this sort of mess</a>, </p>

<p><blockquote>
    "but omg my controller should only be routing and my service should only be servicing and my data layer should only be data-ing."
  </blockquote></p>

<p>And this is unfair, and yet the confusion continues because people see several parts to the whole idea that</p>

<p><blockquote>
    a class should only have one reason to change <br/>
    a  class should only have one responsibility
  </blockquote></p>

<p>The problem is that most of the time the abstractions people come up with to limit a classes responsibility are <em>horizontal</em> in nature. The true power of single responsibility however, lies in the vertical.</p>

<p>Perhaps this is because they're easier to conceptualise, and easy to write frameworks and patterns for so we can feel productive in our day job - but this is really not as it should be.</p>

<p>What do I mean by horizontal? "I'm going to have a layer of controllers, a layer of validation, a layer of services, a layer of business logic,and a layer of data access". "If I put it behind an interface then I'll be alright".</p>

<p>What do I mean by vertical? "I'm going to write a feature to to manage the availability of books in this library"</p>

<p>Frameworks like ASP.NET MVC don't help us with this and their by-default grouping of horizontal concerns across a project, it makes it too easy to carry on grouping the horizontal concerns across a large project and pretend we have a nice clean architecture because we have folders to put things on.</p>

<p><em>Your relationship with state</em></p>

<p>A lot of the time it boils down to state, OO and state have a bit of a confusing relationship, and most of our efforts should be devoted to minimising the amount of mutable state that is exposed to concerns that aren't directly related to each other.</p>

<p>Funnily enough, despite the confusion this is actually pretty easy to conceptualise via tooling and metrics, if your classes are cohesive, most of the methods on that class will touch most of the state in that object. </p>

<ul>
<li>If half of the methods touch half of the state, and the other half of the methods touch the other half of the state then there are clearly two classes.</li>
<li>If you're constantly having to pass visitors around the place to get at private state, or expose state through getters, then you should probably be looking at merging code because you've spread responsibility too thin</li>
<li>If you're constantly passing state around in bags to be mutated by several other things, then you likely have the responsibilties spread out over several layers and you should likely be deleting those layers and putting the code in an object that looks to protect access to that state.</li>
<li>If you haven't got a lot of directly mutable state, then something somewhere probably is being mutated (such as in the database) and following that chain to the source will yield in answers similar to the above.</li>
<li>If you have to jump through more than a couple of classes to find the state that your behaviour is trying to modify, you've gone too far - keeping your mutable state close to the actual behaviour being enacted on it is the road to success</li>
</ul>

<p>Having a horizontal layer whose responsibility is "modifying state in the database" is nonsensical and vague.</p>

<p>Having several objects whose responsibility is looking after the state for a particular feature and then persisting it (perhaps via another facillitiating object) has a lot more sense in terms of understandability and traceability.</p>

<p><em>A note note on orthoganal concerns</em></p>

<p>State based data persistence is not (usually) an orthogonal concern, neither is the workflow/routing sat in front of your MVC application - logging on the other hand can be, and authentication/authorisation can be too. </p>

<p>Clearly, you shouldn't be constantly modifying these vertical slices because of a change to your authentication scheme or logging system. Trying to classify too many things as being orthogonal to your core functionality however is what leads to layer soup, and care should always be taken not to do this.</p>

<p>You can discover these as you go, there is nothing wrong with littering your code with these concerns to begin with, and then as things get repetitive, pulling them out to make life easier. Premature attempts at trying to isolate these concerns is often the path to layer soup.</p>

<p><em>Upwards from the class level</em></p>

<p>Trying to make classes whose concerns are limited, whose reason to change are limited is all very well and good, but it's no good looking at things under a microscope and saying <em>Hey, things look okay</em>, when you sit back and have to go <em>what is this crap?</em> </p>

<p>Let me invoke the <a href="http://codeofrob.com/entries/lots-of-small-things.html">NodeConf drinking game</a>, a lot of the time it is much more valuable to think of your codebase as a collection of modules which are independently versioned and have clear boundaries set up between them.</p>

<p>Any of these small modules can start off by being a complete and utter mess, and if further work is required in that area you can either re-write the module in a few hours, or refactor it as you go (did you say <em>spike and stabilise</em> again Rob? I think I did)</p>

<p><em>That</em> is where single responsibility really means something, and you can save a lot of time in development by building these disposable building blocks when you're rapidly iterating on a product.</p>

<p><em>Summary</em></p>

<p>I seem to have started with the least controversial and one of the most harmful of rules, oh well...</p>

<p>Thus ends my brain dump on responsibility and the many routes to layer soup. Tomorrow I'll go the heart of the matter on OCP, and then wind down the rest of the week through the rest of the set.</p>]]></description><link>http://codeofrob.com/entries/my-relationship-with-solid---starting-with-s.html</link><guid isPermaLink="true">http://codeofrob.com/entries/my-relationship-with-solid---starting-with-s.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Mon, 25 Mar 2013 09:30:00 GMT</pubDate></item></channel></rss>