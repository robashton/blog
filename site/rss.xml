<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Rob Ashton's blog]]></title><description><![CDATA[Software development dumping ground]]></description><link>http://codeofrob.com</link><image><url>http://codeofrob.com/img/cover.jpg</url><title>Rob Ashton&apos;s blog</title><link>http://codeofrob.com</link></image><generator>NodeJS RSS Module</generator><lastBuildDate>Tue, 19 Mar 2013 14:14:21 GMT</lastBuildDate><atom:link href="http://feeds.feedburner.com/robashton" rel="self" type="application/rss+xml"/><item><title><![CDATA[This week, let's create a start-up - Day 5]]></title><description><![CDATA[<p><strong>This post was written on Friday</strong></p>

<p>Final day of "<a href="entries/this-week,-lets-create-a-start-up.html">Build a start-up in a week</a>", how did we do?</p>

<p>Well, we deployed all assets to Amazon and ported across Sam's first customer from his original single-tenant system and everything works as expected. This work included lots of tidy up and "making nice jobs", very little faffing :)</p>

<ul>
<li>Sticking jPlayer into an iFrame so Sam could skin it </li>
<li>Write a migration script to take data from the original system and create an org for it in the new system</li>
<li>Setting up autoplay from the home page (so pressing play on a sermon would re-direct to the sermon page and play the sermon via jPlayer)</li>
<li>Fulltext search functionality on both the public site and the admin site through sermons</li>
<li>Adding the series info on the sermon viewing page</li>
<li>Styling</li>
</ul>

<p>Not very exciting, but all very trivial (even full text search)</p>

<p><em>Migration script</em></p>

<p>This just loaded the original data into memory as a string, de-serialized it into the old data types, copied it across into new data types, re-wrote ids across references and then called SaveChanges.</p>

<p>Didn't even bother using any of the bulk support in RavenDB as the amount of data was trivial, the dumb solution is sometimes the best- <em>next</em></p>

<p><em>Full text search</em></p>

<p>YAY RAVENDB</p>

<pre><code>    public class SermonSearchIndex : AbstractIndexCreationTask&lt;Sermon&gt;
    {
        public SermonSearchIndex()
        {
            Map = docs =&gt; from doc in docs
                          select new
                              {
                                  doc.OrganisationId,
                                  doc.ServiceType,
                                  doc.Title,
                                  doc.SpeakerName,
                                  doc.BibleReference,
                                  doc.SermonDate,
                                  doc.IsPublished
                              };

            Index(x =&gt; x.ServiceType, FieldIndexing.Analyzed);
            Index(x =&gt; x.Title, FieldIndexing.Analyzed);
            Index(x =&gt; x.SpeakerName, FieldIndexing.Analyzed);
            Index(x =&gt; x.BibleReference, FieldIndexing.Analyzed);

        }
    }
}


if (!string.IsNullOrEmpty(input.Search))
{
    query = query.Where(x =&gt; x.Title == input.Search || x.SpeakerName == input.Search || x.BibleReference == input.Search || x.ServiceType == input.Search);
}
</code></pre>

<p>Can't argue with how easy that was, and it still all works with that original paging stuff I wrote on the first day.</p>

<p><em>Adding the series info for a sermon</em></p>

<pre><code>var sermon = session.Load&lt;Sermon&gt;(id)
// if null etc
var series = session.Load&lt;Series&gt;(sermon.SeriesId)
</code></pre>

<p>Remember that ISecureDocumentSession <a href="/entries/this-week,-lets-create-a-start-up---day-4.html">I wrote yesterday</a>? That made the Include stuff hard to cater for on a Load, so I just do two load calls instead - in a more evolved system we'd have to do this better somehow because it isn't going to scale across all the other usages that IDocumentSession can give us.</p>

<p>Instead I'd look at hooking into RavenDB properly to do this security (either using its server-side security bundle, or adding appropriate extension points for this sort of filtering job on the client)</p>

<p>For this sort of thing though, it's two remote calls rather than one on a fairly low traffic system so it should be okay for now.</p>

<p><em>Deploying</em></p>

<p>Took 10 minutes to get onto Amazon thanks to Sam's efforts earlier in the week, and 5 minutes to replace the content on the old system with the script tag to import data from the new Truthvine system. (That's what the customer would have to do to use it)</p>

<p>If that's not easy I don't know what is. </p>

<p><em>Summary</em></p>

<p>ASP.NET MVC is surprisingly tolerable if you leave your opinions and a bit of brain-matter at the door on your way in, certainly it's pretty fine at throwing together a quick cruddy application on top of something simple like RavenDB.  Oh, and Razor is a thing of beauty - well done Microsoft for getting at least one thing right this past few years (Aww, just kidding, you know I love you really)</p>

<p>Mission accomplished and nothing in the solution is messy at all thanks to the no-crap atttitude of throwing things together that work in the simplest way possible.</p>

<p>I wish Sam luck on his start-up adventure and hope he finds enough clients to make the effort worthwhile, he's got a good project on his hands and I hope I've given him the boost he needed to get going.</p>]]></description><link>http://codeofrob.com/entries/this-week,-lets-create-a-start-up---day-5.html</link><guid isPermaLink="true">http://codeofrob.com/entries/this-week,-lets-create-a-start-up---day-5.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Tue, 19 Mar 2013 09:30:00 GMT</pubDate></item><item><title><![CDATA[This week, let's create a start-up - Day 4]]></title><description><![CDATA[<p><strong>This post was written on Thursday</strong></p>

<p>It's day 4 out of 5 and <a href="/entries/this-week,-lets-create-a-start-up---day-3.html">we pretty much have an end-to-end MVP</a> sorted out, (and a genuine surprise for me given the platform chosen!).</p>

<p>Today we decided we'd concentrate on some of the more fiddly aspects of the job, namely:</p>

<ul>
<li>Getting the workflow nailed for the desktop client</li>
<li>Adding per-customer partitioning to the system</li>
<li>Adding a HTML5 audio player to the public-facing system</li>
</ul>

<p><em>The desktop client</em></p>

<p>Authentication proved to be a stumbling block, we're using forms auth across the site which is pretty unfriendly. I was pretty against creating a whole set of new endpoints just for a desktop client when we had perfectly good forms available for it already but I also didn't want to waste ages faffing with custom auth.</p>

<p>We had a look at creating a separate deployment for the desktop services using WebAPI but that would have meant doing a load of the infra work already achieved for ASP.NET MVC in WebAPI (yeah, they're kinda united and divided at hte same time). This would have been faffing, so we looked at...</p>

<p>Hosting WebAPI inside the ASP.NET MVC system and going for both Basic Auth and Forms Auth, we found a project on Github which federated these two with some custom providers - but that looked like it was going to be a rabbit hole too - and I'm against faffing so we looked at...</p>

<p>The original idea of using the original forms, and issuing tokens to the desktop client ala <a href="http://last.fm">last.fm</a>, this only took 20 minutes or so and proved to be the sane choice for rapid <em>moving on</em></p>

<p><em>Per customer partitioning</em></p>

<p>We actually already had a conversation about this, do we do database-per-client or just field-on-the-document, this is a classic debate and it's not much different in the document database world.</p>

<p>We decided that database-per-client would have been a nightmare faff of session management (shared database as well) - not to mention fun-times if it came to a clouded instance of Raven so lumped our bets in with the database-field.</p>

<p>We looked at hooking the events exposed by the RavenDB client to add the field check to all queries, but unless we wanted to manipulate the final string query this was a dead end (and it didn't seem like there was an easy way to hook the load/save process either). </p>

<p>This is either an omission on our parts, or RavenDB doesn't have those APIs yet, I really want those APIs, so I might add those APIs later, moving on however because we haven't got time for that...</p>

<p>I'm not particularly proud of what we did next, but it worked, everywhere the system asked for IDocumentSession, we changed it to ISecureDocumentSession and exposed the 5 methods on Session that we're actually using </p>

<pre><code>- Load&lt;Type&gt;(string id)
- Load&lt;Type&gt;(int id)
- Load&lt;Type&gt;(string[] ids)
- Query&lt;Type&gt;(string id)
- Store(Object doc)
</code></pre>

<p>In each of these we check for typeof T === ISecured, and apply a check for the OrganisationId of the document.</p>

<p>This took all of 5 minutes, and was an effective use of time, perhaps to be re-visited in the future if it doesn't pan out.</p>

<p><em>Complicated logic</em></p>

<p>However, we now have quite a lot of complicated behaviour around </p>

<ul>
<li>What organisations does the user have permission over</li>
<li>What organisation is the user currently administering</li>
<li>What organisation is being requested by the public facing system</li>
<li>What organisation is being pushed to from the desktop client</li>
<li>etc</li>
</ul>

<p>Most of this logic exists in the infrastructure, and it needs pulling out and unit testing separately - just as a way of documenting the different functionalities exposed by these varients as changes are going to be very time consuing and error-prone otherwise.</p>

<p>This is an example of what <a href="/entries/uncle-bobs-viewpoint-considered-harmful.html">I was talking about in my testing post</a> about diving in when things became too hard to juggle.</p>

<p><em>The HTML5 Audio player</em></p>

<p>"This'll be easy", I said, "We'll just download a widget" I said. We plumped for <a href="http://www.jplayer.org/">jPlayer</a>, which has a few dependencies including jQuery - so it'll have to be hosted inside an iframe so as not to annoy the site we're embedding the public data in.</p>

<p>It wasn't that easy though, first off - our data is in S3, and our public site is on somesubdomain.truthvine.com, cross site calls aren't really allowed...</p>

<p>Actually, they are - turns out that S3 has CORS support these days and on enabling that I was able to hear My Bloody Valentine blaring out of my speakers when visiting the player page in my Chromium install.</p>

<p>The same went for our IE10 instance and our IE9 instance but IE8...? Nope.</p>

<p>After faffing around trying to put the static files on S3 and going to-and-fro a tad, it turned out we'd just mis-typed the path to the SWF file (which will handle CORS for us in crappy browsers) and now we have a cross-browser and cross-platform audio player on the site.</p>

<p>This is the classic example of where faffing costs time, and what I was trying to avoid throughout the day with the other decisions we had.</p>

<p>Trying to build quick, and practising building quick gives you instincts on avoiding these and I'm glad that with the other challenges so far we've managed to avoid faffing. </p>

<p>I guess browser compatability is always gonna be one of those things, but it only took an hour or so and leaving with all of the above achieved in a single day made us feel pretty good :)</p>

<p>Tomorrow time for some more features before I head back off to London and have a couple of days rest before heading to Greece for another coding adventure...</p>]]></description><link>http://codeofrob.com/entries/this-week,-lets-create-a-start-up---day-4.html</link><guid isPermaLink="true">http://codeofrob.com/entries/this-week,-lets-create-a-start-up---day-4.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Mon, 18 Mar 2013 09:30:00 GMT</pubDate></item><item><title><![CDATA[This week, let's create a start-up - Day 3]]></title><description><![CDATA[<p><strong>This post was written on Wednesday</strong></p>

<p>Now we have the <a href="/entries/this-week,-lets-create-a-start-up---day-1.html">Admin functionality</a> and the <a href="/entries/this-week,-lets-create-a-start-up---day-2.html">Public functionality</a> spiked properly into, it's time to look at file uploads properly.</p>

<p>Sam has a tool which will (client-side) be used to process audio files for upload in a very simple and stripped down manner (built specifically for his target demographic) - I quite like that.</p>

<p>I also have admin forms for this task and this is where our first bit of feature-confusion happens.</p>

<p><em>Rob: Okay, so you have audio files attached to sermons, I've implemented that as a list of value types with a URI and display name</em></p>

<p>Sam: "We probably want to have specific buckets for "High quality audio", "Low quality audio" and "Sermon notes"</p>

<p><em>Rob: Wait what? Sermon notes, we can upload arbitrary files?</em></p>

<p>Sam: "Not arbitrary, we have two types of audio file and often a PDF for the sermon notes"</p>

<p><em>Rob: Okay, seems like these are going to be explicit UI concepts so I'll bake that into the actual domain</em></p>

<p>Sam: "Yeah, that'll work... oh except sometimes they upload the entire sermon as well as pieces from the sermon"</p>

<p><em>Rob: So we have explicit files to be uploaded and arbitrary files, I don't mind handling that, although the UI could be complicated</em></p>

<p>Sam: "Hmm, I really want them to be told what to upload as that's user friendly, but I also want the flexibility of anything"</p>

<p>There was also the matter of workflow and how to expose this functionality to the uploading client he was building.</p>

<p>This was a bit of an ongoing conversation, and over the day I actually tried a few workflows out to see what would be easier, and we settled for being able to upload any files at all, but the server would work out what the files were and categorise them when displaying them to either the admin or the public. (Iterating has the advantage of fast feedback)</p>

<p>As for the workflow, I decided I'd use plain old HTML forms with re-directs and links for workflow, done in such a manner that his client can use the same API as the user does through the browser. I can't quite do it with the routes I'd want because I couldn't get ASP.NET MVC to play ball without a bit of faffing around- and I haven't got time to do that. The concepts are there and if we want to be purists later we can change the server without risk of the client breaking so there's that.</p>

<p>With regards to S3, I was surprised how easy this was, I pulled the Amazon SDK NuGet package, and in less than 10 minutes had files being sent to S3, using the Task libraries to manage multiple uploads at the same time. +1 point to the .NET eco-system at this point - definitely better than it used to be.</p>

<p>My experience with ASP.NET MVC is even happier today, making peace with it seems to be paying off, my most pleasing find today was the support for arrays:</p>

<pre><code>for(var i = 0; i &lt; Model.Items.Length; i++ ) {
  Html.EditorFor(x=&gt; x.Items[i].Title)
}
</code></pre>

<p>That made me a happy person. (I last used MVC to build a full product in the days of MVC1 and this was not available)</p>

<p>I spent a bit of time making all the workflows consistent across the admin site and the public site, and polishing the styling and data being displayed across all the pages so I could "mark those features as being done" (After using them a bit and getting feedback they weren't going to change substantially so it was worth investing that time now so I didn't have to later).</p>

<p>We also get everything up onto Amazon so it worked, and verified all the admin functionality and public functionality existed in the way desired.</p>

<p>So that's that, in three days we have built a working product and gotten it deployed into the cloud - and on top of the .NET framework too, which is probably my biggest surprise.</p>

<p>The next two days are about adding data partitions per church, getting the desktop client to use the API properly and adding search/navigation functionality for the viewing public and hopefully even some profile pages for the different preachers along with their sermons. I'll probably nail some pegs into the ground and write some stabilising tests around this stuff so Sam can carry on moving forwards without me keeping the code in a reasonable shape. We'll see how much time we have.</p>

<p>I'm pumped - I <em>love</em> building stuff.</p>]]></description><link>http://codeofrob.com/entries/this-week,-lets-create-a-start-up---day-3.html</link><guid isPermaLink="true">http://codeofrob.com/entries/this-week,-lets-create-a-start-up---day-3.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Fri, 15 Mar 2013 09:30:00 GMT</pubDate></item><item><title><![CDATA[This week, let's create a start-up - Day 2]]></title><description><![CDATA[<p><strong>This post was written on Tuesday</strong></p>

<p>Yesterday <a href="/entries/this-week,-lets-create-a-start-up---day-1.html">I pretty much put together the admin side of the project</a>, and today I decided to focus on the other side of the project - dumping content into a third party site as if it was on the site itelf.</p>

<p>Carrying on from yesterday, this was actually pretty simple</p>

<ul>
<li>Create a JS file for inclusion on the third party site</li>
<li>When imported, it looks for !#/this-stuff-after-the-hash-bang</li>
<li>It then loads the appropriate content from the main truthvine site based on the path after the hashbang</li>
</ul>

<p>We have to use a hashbang because we can't rely on the site we're being embedded on to be able to do server-side redirects but we want back-forward buttons to work.</p>

<p>What we ended up doing was splitting up the system so we have a solution that looks like this:</p>

<ul>
<li>TruthVineAdmin (ASP.NET MVC)</li>
<li>TruthVinePublic (ASP.NET MVC)</li>
<li>TruthVine (RavenDB/Infrastructure)</li>
<li>TestThirdPartyWebsite (Static files only with the script tag in them)</li>
</ul>

<p>I'd normally not like to have a "common" assembly in a solution as the tendency of developers is to shuffle lots of needless 'shared' code to this place (where they'd be better off writing the code separately in the web projects), but I trust that Sam won't do this and the only things that go in this shared assembly are:</p>

<ul>
<li>RavenDB models</li>
<li>Common ASP.NET MVC infrastructure (session-per-request)</li>
<li>The paging code</li>
</ul>

<p>There is little point trying to share view models or play around trying to re-use views and hide and show admin functionality on content pages (that stuff is always horrible to do unless you have time to build up some decent conventions), so this separation makes sense.</p>

<p>As for my happiness rating with ASP.NET MVC today, well - it stayed out of my way because I did everything the way it wanted me to - because of this I ended up building the third party JS content-embedding system to the point where it had pretty much reached feature parity with the system we were basing this off in the first place. (Hurrah)</p>

<p>I'm pretty much doing things the way I've <a href="http://codebetter.com/robashton/2011/06/13/finding-a-balance-with-asp-net-mvc/">Described previously on CodeBetter</a> without any of the feature-based grouping (haven't got time to set it up)</p>

<p>Controller actions all pretty much look like this across the site</p>

<pre><code>[HttpGet]
public ActionResult Edit(int id) {
   var doc = this.Documents().Load&lt;ThatDocument&gt;(id)
   if(doc == null) return new Error404Result()
   return View(new EditViewModel(doc))
}

[HttpPost]
public ActionResult Edit(EditViewModel input) {
   var doc = this.Documents().Load&lt;ThatDocument&gt;(model.Id)
   if(doc == null) return new Error404Result()

   if(ModelState.IsValid) {
    input.SaveTo(doc);
    return RedirectToAction("View", new { Id = model.Id })
   }
   input.CopyFrom(doc);
   return View(input);
}
</code></pre>

<p>Straight down to earth and simple. Coupled with all of those helper methods and Razor it's pretty easy to throw up new forms and use redirects to create a workflow over the site.</p>

<p>My happiness rating with RavenDB is as ever, it stays out the way and handled persistence for me - lovely.</p>

<p>With a working admin portal and a working content embedding system, that's pretty much the end-to-end product written in two days, leaving us three days to mop up the rest of the tasks. </p>

<p>The good thing about all the work so far is that the domain has been very well understood and communicated (thanks to the previous incarnation of the system) so we've not had to spend too long discussing or debating things, I've been showing the work to Sam on a feature by feature basis to get feedback (he is my customer after all) and adopting his feedback every hour or so. </p>

<p>Tomorrow we'll look at getting these three sites deployed on EC2, and throwing the audio files up to S3, because that'll put us in a good place.</p>]]></description><link>http://codeofrob.com/entries/this-week,-lets-create-a-start-up---day-2.html</link><guid isPermaLink="true">http://codeofrob.com/entries/this-week,-lets-create-a-start-up---day-2.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Thu, 14 Mar 2013 09:30:00 GMT</pubDate></item><item><title><![CDATA[This week, let's create a start-up - Day 1]]></title><description><![CDATA[<p><strong>This post was written on Monday</strong></p>

<p>I've finished my <a href="/entries/this-week,-lets-create-a-start-up.html">first day</a> of the build a start-up in a week challenge, and now I know a little more about the project.</p>

<ul>
<li>Sam has already built a codebase that his local church uses to put sermons online</li>
<li>His target audience is non-technical churches that have managed to get a website up there (even if it's just static code)</li>
<li>He wants to be able to embed a single line of code in one of their pages and give them all the functionality</li>
<li>The functionality isn't too complex, but it has to be dead easy</li>
</ul>

<p>Okay, so what the heck are sermons and what's the deal about getting them online</p>

<ul>
<li>Churches have sermons</li>
<li>A sermon is just an audio file (encoded to a variety of formats)</li>
<li>Each sermon can belong to a series of sermons (perhaps on a topic of some sort)</li>
<li>Sermons have meta data associated with them, as do the series</li>
<li>Each church needs their own subset of this data</li>
<li>Each church can have multiple users who can add sermons and series</li>
</ul>

<p>Not that complicated, and my first suggestion is</p>

<p><em>Can't we just use Wordpress/Drupal/Etc and have this done in a few hours?</em></p>

<p>If I was building an MVP, that's pretty much what I'd do, but Sam is pretty insistent that he wants it done in .NET (<em>long time no see</em>) and RavenDB (<em>I was doing this last week!</em>), so I guess I need to get out my Windows laptop and once more work my way around the perils of .NET and ASP.NET MVC)</p>

<p>Why ASP.NET MVC? Because I actually remember quite a lot of it from the last time I did .NET and I'm not going to waste time trying to learn another .NET web framework when I have to start building stuff <em>now</em> - when building something with such a short time-frame, you should be using something you know to make the most of the time.</p>

<p>Anyway, after trying to create a few "empty sites" in Visual Studio, I finally find a configuration that is "emptier" than the other ones and get to work (this is still confusing Microsoft!)</p>

<p>Turns out it's not only "mediocre" these days, but I'm actually able to get most of the basic CRUD operations and workflow done in a few hours, highlights of this experience being:</p>

<ul>
<li>The Razor View Engine</li>
<li>Html.EditorForModel</li>
<li>Html.EditorFor</li>
<li>Data Validation Attributes for the ViewModels (sorry guys, but they work quite well)</li>
<li>Global action filters</li>
</ul>

<p>The default model binding seems to work out of the box for everything, and RavenDB is being managed via global action filters so I only have to do</p>

<pre><code>this.Documents()
</code></pre>

<p>In any controller to get the document session (yay for extension methods), no I'm not bothering wiring up a container, the only objects in play are the input/view models and RavenDB and the state model that is being persisted in it, and it's unlikely to get much more complicated than that (so end-to-end tests will suffice with an in-memory RavenDB for now)</p>

<p>I even wrote a little bit of magic to do paging in a standard way across any view that needs them in RavenDB</p>

<pre><code>pagedResults = session.Query&lt;Whatever&gt;()
       .Where(x=&gt; SomeCondition(x))
       .PerformPaging(inputModel)
</code></pre>

<p>Yay again for extension methods.</p>

<p>I also set up OAuth - I used DotNetOpenAuth, which worked once I'd written a pile of code in a controller action - it's <em>much</em> better than the previous incarnations I used last time I did .NET, but it's still not quite as good as say, passport in NodeJS (and I'll hazard a guess it doesn't quite meet the standards of whatever RoR provides for this either).</p>

<p>I guess that's because with node, we usually have control over the entry point and everything can be done with connect middleware in a standardised fashion, whereas ASP.NET MVC is an opinionated framework which doesn't know what its opinions are and still suffers from a sad-mix of xml configuration and confusion, still - I guess once you know about this stuff you can copy and paste these bits of infrastructure around so it's not too awful.</p>

<p>Anyway, today I achieved with ASP.NET MVC and RavenDB</p>

<ul>
<li>All the basic CRUD forms + workflow around sermons and series (about 10 dynamic pages in all)</li>
<li>Basic paging/filtering controls for anything that needs them</li>
<li>Audio upload (alhough not going to S3 yet)</li>
<li>Authentication with credentials</li>
<li>Authentication via OAuth</li>
<li>Theming using the bootstrap theme Sam has already provided</li>
</ul>

<p>Tomorrow I'll hook up the script to embed content in third party websites, and get the MVP finished - that'll leave three days to do all the value-adds, I'm fairly happy with this progress and feel that <em>this</em> is still how we build software, even if it's not in the technology I'd have chosen.</p>]]></description><link>http://codeofrob.com/entries/this-week,-lets-create-a-start-up---day-1.html</link><guid isPermaLink="true">http://codeofrob.com/entries/this-week,-lets-create-a-start-up---day-1.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Wed, 13 Mar 2013 09:30:00 GMT</pubDate></item><item><title><![CDATA[This week, let's create a start-up]]></title><description><![CDATA[<p>This week (I'm writing this on <strong>Sunday</strong>) on my grand adventure of <a href="/entries/i-am-not-looking-for-a-job.html">"building stuff for people"</a>, I am going to help <a href="http://twitter.com/samuel_d_jack">@samuel_d-jack</a> build a start-up.</p>

<p>Because I'm not yet in the office and don't know what is expected of me there is little I can say about what I'll be doing yet, I'll be writing a blog post each day for publishing in order at some point in the future, so these posts are going to show up out of kilter (I think the final one will be on a Tuesday even though it was penned on a Friday - oh well)</p>

<p>What I do know</p>

<ul>
<li>Sam occasionally works on RavenDB</li>
<li>Sam is a freelance .NET developer</li>
<li>Sam is based up near Birmingham</li>
<li>Sam wants to do something about helping churches get sermons online</li>
</ul>

<p>That's about it, I look forward to seeing what the problem is, and how he wants it solved.</p>]]></description><link>http://codeofrob.com/entries/this-week,-lets-create-a-start-up.html</link><guid isPermaLink="true">http://codeofrob.com/entries/this-week,-lets-create-a-start-up.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Tue, 12 Mar 2013 09:30:00 GMT</pubDate></item><item><title><![CDATA[The fallacy of the Dreyfus model in software development]]></title><description><![CDATA[<p>Not <a href="/entries/uncle-bobs-viewpoint-considered-harmful.html">another</a> <a href="http://blog.markrendle.net/2013/03/09/dont-unit-test-trivial-code/">post</a> <a href="http://blog.ploeh.dk/2013/03/08/test-trivial-code/">about</a> <a href="http://blog.8thlight.com/uncle-bob/2013/03/05/TheStartUpTrap.html">TDD</a> Rob, please no!</p>

<p>Okay, so I'll reply on a tangent because I've had enough of talking about TDD and want to talk about software development as a learned "craft" and why pimping out magic software development strategies to "newbies" is harmful to the overall quality of software development.</p>

<p>First off, I want to make an opening statement so you know where I'm coming from. This is based on various jobs and contracts I have been a part of and various code bases I have been asked to look at over the years.</p>

<p><blockquote>
    Badly applied software patterns and methodologies often cause bigger problems than not applying them at all
  </blockquote></p>

<p>This is obvious to most people (I hope), we <em>know</em> this - so why am I saying it again?</p>

<p>I read <a href="http://programmers.stackexchange.com/questions/185719/how-should-you-tdd-a-yahtzee-game/188188#188188">this response</a> to a question on TDD on StackExchange and the sentiment being expressed is:</p>

<p><blockquote>
    Flexibility isn't for novices
  </blockquote></p>

<p>This hearkens back to the <a href="http://en.wikipedia.org/wiki/Dreyfus_model_of_skill_acquisition">Dreyfus model of skill acquisition</a> and the idea that beginners and novices shouldn't be left alone to fend for themselves; that they should be strictly adhering to the recognised rules during this development phase.</p>

<p>Let me start off by saying that this premise is something that I agree with across any industry where experience is more important than qualifications or badges next to your name. </p>

<p><strong>However...</strong></p>

<p>Software development is definitely one of these industries, so what's my beef/horse with applying this model in reality?</p>

<p><blockquote>
    The statement that beginners should be strictly following the rules makes the assumption that they have access to somebody who isn't a beginner and can tell them that they're doing it wrong.
  </blockquote></p>

<p>The problem is that software development is <em>still</em> an immature and confusing trade. Access to these experts is limited to 1-2 week consultancy stints where they come in, spout a pile of "truths" and then leave the team to fend for themselves (if this even happens at all).</p>

<p>What we actually have out here in the field is thousands of software teams with no experience of these things. They also have no real access to anybody with these skills.</p>

<p>When they hear advice from the so-called Master Craftsmen in the field that:</p>

<p><blockquote>
    You should be testing all trivial code
  </blockquote></p>

<p>They take that as a gospel and get to work creating literally thousands of brittle unit tests (it's likely they're not even doing TDD "properly"). The end result is that their code is worse off then had they just felt their way through the problem with direct solutions and applied evidence-based solutions to the pain points.</p>

<p>The same problem happened 10 years ago with all that rubbish N-Tier advice that the Enterprise Consultants were doling out.</p>

<p><blockquote>
    The problem is that we're encouraging thousands of new developers to follow the rules and not think for themselves.
  </blockquote></p>

<p><em>All of this has happened before and will happen again.</em></p>

<p><strong>The ideal world</strong></p>

<p>In the ideal world where we all have access to mentors with vast experience in this "more professional world" that our master craftsmen <a href="http://blog.8thlight.com/uncle-bob/2013/03/05/TheStartUpTrap.html">aspire to create</a> then we can afford to spend time bringing up our "novices" with:</p>

<ul>
<li>rigid adherence to taught rules or plans</li>
<li>no exercise of discretionary judgement</li>
</ul>

<p>This sounds absolutely wonderful and when we have a few hundred thousand people with 20 years of experience trying TDD and failing at it. When those few hundred thousand people have found the balance and come to their form of pragmatism these people will make <em>wonderful</em> mentors.</p>

<p><strong>The reality</strong></p>

<p>In reality, the mentors that our novices have access to are the ones who still think that SQL Server is the only safe place to put data. You know - the ones who still think that every app needs a <a href="/entries/cqrs-is-too-complicated.html">BLL, DAL, BOLL,ETC</a>, and that if you write your code properly the first time you shouldn't need tests.</p>

<p>In reality, a lot of these developers have decided that TDD is the greatest thing without really trying it out, or that every piece of code should have a Single Responsibility, and Never Be Touched Once Written (it's an interpretation I've seen). This is  because that is <em>how it's written on the internet</em> and because of this they're "engineering" <em>monsters</em> which while financially great for consultants like me are costing the companies that own those monsters dear.</p>

<p>The reality is that new software developers aren't going to spend their first two years being spoon-fed "the correct way of doing things" (whatever that is, because we're still not sure). </p>

<p>The reality is that they might get a couple of months of "help" when starting software development at their first company (where the help is "this is how you use Enterprise Framework X").</p>

<p>The "people on the internet" spouting TDD and associated methods like sermons do not count as mentors to these people. You cannot apply the Dreyfus model to this sort of learning relationship. These people are not there for these novices and trying to reason about software craftmanship as if they are is what is causing the <em>new</em> problems.</p>

<p>The reality is that to every question the answer is <em>it depends</em> and we should be arming our "novices" with the tools to work out what the real answer is instead of handing them a single hammer with a bag of nails. (That's a metaphor for "writing blog entries which state that <em>you're doing it wrong unless...</em>")</p>

<p><strong>Getting a grip</strong></p>

<p>Spouting dogma at people on the internet and at conferences is self-defeating, (Unless you make your living off consultancy in these matters) it either puts people who know better into defensive mode because they recognise the dogma and rightfully fear it, or encourages people to follow your rules without looking at the wider context which causes substantial problems in software products.</p>

<p>Quit it.</p>]]></description><link>http://codeofrob.com/entries/the-fallacy-of-the-dreyfus-model-in-software-development.html</link><guid isPermaLink="true">http://codeofrob.com/entries/the-fallacy-of-the-dreyfus-model-in-software-development.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Mon, 11 Mar 2013 09:30:00 GMT</pubDate></item><item><title><![CDATA[The price of abstraction - using LevelDB in RavenDB]]></title><description><![CDATA[<p>So, in our little foray into LevelDB, we've uncovered the following:</p>

<ul>
<li>We can do <a href="/entries/basic-operations-with-leveldb.htm">atomic writes</a></li>
<li>We can have <a href="/entries/transactional-guarantees-on-top-of-leveldb.html">consistent reads</a>, </li>
<li>We can create a <a href="/entries/writing-a-transaction-manager-on-top-of-leveldb.html">transaction manager</a> on top of LevelDB </li>
<li>We can can <a href="/entries/streaming-large-values-from-leveldb.html">stream data out of LevelDB</a></li>
<li>And we can create <a href="/entries/secondary-indexes-on-top-of-leveldb.html">secondary indexes</a></li>
</ul>

<p>With this, I set about creating a fork of RavenDB with a starting implementation of LevelDB for document storage.</p>

<p>I grabbed the <a href="https://github.com/meebey/leveldb-sharp">C# bindings</a> from Github and set about re-implementing my in-memory transaction manager on top of them.</p>

<p>The first thing I notice about the C# bindings is that the API looks like this:</p>

<pre><code>string value = db.Get(string key)
db.Put(string key, string value)
</code></pre>

<p>This is not what <a href="/entries/streaming-large-values-from-leveldb.html">RavenDB wants</a>, what RavenDB wants is a MemoryStream or at the very least a ByteArray - as it is not in the business of pushing large strings around the place in managed memory.</p>

<p>No problem, I fork the original project and set about addding Byte array support and then notice something.</p>

<p><em>The bindings are using the C API for LevelDB</em> (which makes sense if you're going to support cross platform C#), but more importantly the C API looks something like this:</p>

<pre><code>char* leveldb_get(char* key)
</code></pre>

<p>See that return result? That's a malloc, what does the LevelDB C# code do?</p>

<pre><code> var value = Marshal.PtrToStringAnsi(valuePtr, (int) valueLength);
 return value;
</code></pre>

<p>Ignoring the <a href="https://github.com/meebey/leveldb-sharp/issues/2">memory leak</a>, as it's a known issue and an easy mistake to make, we can see we're copying the data out of the unmanaged array into a managed string via the PtrToStringAnsi call.</p>

<p>What does the C api do?</p>

<pre><code>Status s = db-&gt;rep-&gt;Get(options-&gt;rep, Slice(key, keylen), &amp;tmp);
if (s.ok()) {
  *vallen = tmp.size();
  result = CopyString(tmp);
} else {
  *vallen = 0;
  if (!s.IsNotFound()) {
    SaveError(errptr, s);
  }
}
return result;
</code></pre>

<p>And CopyString?</p>

<pre><code>static char* CopyString(const std::string&amp; str) {
  char* result = reinterpret_cast&lt;char*&gt;(malloc(sizeof(char) * str.size()));
  memcpy(result, str.data(), sizeof(char) * str.size());
  return result;
}
</code></pre>

<p><em>urk</em></p>

<p>In order to get a stored document from LevelDB, the following operations are performed:</p>

<pre><code>- Internally, LevelDB will load the document into memory
- Internally, LevelDB will copy that memory into a string on a Get call
- The C API copies that memory into a mallocced byte array
- The C# API copies that memory into a managed string (probably on the Large Object Heap)
</code></pre>

<p>What we have here, is a C# wrapper around a C API that is a wrapper around a C++ API, and each of them is trying to be semantically equivalent to the underlying API - and the abstraction is <em>killing us</em>.</p>

<p>For small values, this probably isn't a big deal, but RavenDB documents can be any size, and RavenDB documents often <em>are</em> any size - ideally what we want is</p>

<pre><code>MemoryStream stream = db.Get(string key)
</code></pre>

<p>Where that stream reads from the start of an unmanaged array to the end of that unmanaged array and then disposes it.</p>

<p>In essence, the job of getting LevelDB into RavenDB is going to involve writing a new C API for LevelDB specific to our needs, then writing a C# API around that API specific to our needs.</p>

<p>Thankfully we need about 10% of the exposed functionality in LevelDB so our job isn't as hard as "writing a set of managed bindings for LevelDB", but this means that I won't have time to do this before I leave Israel.</p>

<p>I put my sad face on and get to work on something more manageable for my final day of work :-(</p>]]></description><link>http://codeofrob.com/entries/the-price-of-abstraction---using-leveldb-in-ravendb.html</link><guid isPermaLink="true">http://codeofrob.com/entries/the-price-of-abstraction---using-leveldb-in-ravendb.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Fri, 08 Mar 2013 09:00:00 GMT</pubDate></item><item><title><![CDATA[Secondary indexes on top of LevelDB]]></title><description><![CDATA[<p>Got <a href="/entries/basic-operations-with-leveldb.htm">atomic writes</a>, <a href="/entries/transactional-guarantees-on-top-of-leveldb.html">consistent reads</a>, <a href="/entries/writing-a-transaction-manager-on-top-of-leveldb.html">transactions</a> and we can <a href="/entries/streaming-large-values-from-leveldb.html">do streaming</a> if we need to - now if we can do secondary indexes we can move forwards with creating a storage implementation for RavenDB.</p>

<p>An example of a couple of indexes we need in RavenDB is:</p>

<ul>
<li>The document itself stored by key (primary index)</li>
<li>The document key stored by etag (secondary index)</li>
</ul>

<p>Whenever we write a document in RavenDB, we want to</p>

<ul>
<li>Remove the old Etag (or at least update it)</li>
<li>Write the document against its key</li>
<li>Write a new etag for the document</li>
</ul>

<p>(Actually, a bit more goes on than this as we store meta data for headers etc, but this will do for now)</p>

<p>The operation we want to do when indexing is</p>

<ul>
<li>What are all the new etags since last we indexed?</li>
<li>Loop through each of these, load the document and index it</li>
</ul>

<p>Etags are sortable by design, in my rough and ready document database the etag is just an integer that I increase every time I add a new document.</p>

<p>Now, when writing and reading from the database, we need to be able to differentiate between types of key in the database, and ordering for each of these types is set by the key.</p>

<p><em>For example, reading a document</em></p>

<pre><code>#define DOCPREFIX "/docs/"
#define ETAGPREFIX "/etags/"


void Get(std::string id, std::string* doc) {
  std::stringstream st;
  st &lt;&lt; DOCPREFIX;
  st &lt;&lt; id;
  std::string key = st.str();
  this-&gt;lastStatus = this-&gt;db-&gt;Get(leveldb::ReadOptions(), key, doc);
}
</code></pre>

<p>So I'm sticking a prefix in front of each key to denote what sort of key it is, yay.</p>

<p>That means that my secondary index of etags will end up looking like this</p>

<pre><code>/etags/0 -&gt; /docs/1
/etags/1 -&gt; /docs/5
/etags/2 -&gt; /docs/4
/etags/3 -&gt; /docs/6
/etags/4 -&gt; /docs/3
/etags/6 -&gt; /docs/7
/etags/8 -&gt; /docs/12
/etags/9 -&gt; /docs/5
</code></pre>

<p>When I want to index documents newer than /etags/5, I can do</p>

<pre><code>int IndexNewDocumentsNewerThan(int etag) {
  std::stringstream st;
  st &lt;&lt; ETAGPREFIX;
  st &lt;&lt; etag;
  std::string startkey = st.str();

  leveldb::Iterator* it = this-&gt;db-&gt;NewIterator(leveldb::ReadOptions());
  for (it-&gt;Seek(startkey); it-&gt;Valid() &amp;&amp; this-&gt;IsEtagKey(it-&gt;key().ToString()); it-&gt;Next()) {
    std::string document = it-&gt;value().ToString();
    IndexDocument(value);
  }
  delete it;
}
</code></pre>

<p>Building up other indexes is a trivial matter of making sure we have a suitable ordering in place, and queries are simply a matter of building up these keys appropriately.</p>

<p>With all of this, I think we have enough information to go and build persistence for RavenDB on top of LevelDB (and then test test test, performance performance performance etc).</p>

<p>If you want to check out the spikes and playarounds with LevelDB, you can find them on my <a href="https://github.com/robashton/leveldb-play">Github</a></p>]]></description><link>http://codeofrob.com/entries/secondary-indexes-on-top-of-leveldb.html</link><guid isPermaLink="true">http://codeofrob.com/entries/secondary-indexes-on-top-of-leveldb.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Thu, 07 Mar 2013 09:00:00 GMT</pubDate></item><item><title><![CDATA[Streaming large values from LevelDB]]></title><description><![CDATA[<p>We've uncovered that we can do <a href="/entries/basic-operations-with-leveldb.htm">atomic writes</a>, <a href="/entries/transactional-guarantees-on-top-of-leveldb.html">consistent reads</a> and <a href="/entries/writing-a-transaction-manager-on-top-of-leveldb.html">implement transactions</a> on top of LevelDB, but we've got another need in RavenDB which is that we have large objects (documents) inside RavenDB and we don't always want to load them into managed memory as a whole unit (instead, we want to stream them into a set of smaller objects)</p>

<p>Why don't we want to load them into memory in one go? Because large objects can wreck havoc with the garbage collector (the large object heap), and we also have streaming APIs in RavenDB that allow us to limit this sort of thing that we'd like to honour.</p>

<p>So, back to C++ as we answer the question "What about large objects inside LevelDB"?</p>

<p>Looking at the APIs in our first entry, we can see that "Get" will load entire value into memory ala this:</p>

<pre><code>db-&gt;Get(leveldb::ReadOptions(), key, &amp;document);
</code></pre>

<p>We <em>could</em> simply read bytes from this document (remember, it's an std::string) and marshal them through to C# bit by bit, but that means copying the whole document out of the store before doing this. This isn't a big deal as it's still completely on the native side at this point, but it's still an expense I'd prefer to skip if we're reading more than a few documents.</p>

<p>That's where Slice comes in.</p>

<pre><code>  leveldb::Iterator* it = this-&gt;db-&gt;NewIterator(leveldb::ReadOptions());
  for (it-&gt;Seek(start_key); it-&gt;Valid() &amp;&amp; it-&gt;key().ToString() &lt;= end_key; it-&gt;Next()) {
    Slice* slice = it-&gt;value();

    // Do stuff with a slice
  }
</code></pre>

<p>This is quite cool, LevelDB gives us the notion of a "Slice", which is merely a pointer to the beginning of the value, and the length of the value.</p>

<p>A "Get" operation merely copies this data into an std::string, and calling slice->ToString() will do this as well.</p>

<p>What we can do is </p>

<pre><code>char buffer[BUFFER_SIZE];
memcpy(slice-&gt;data(), buffer, BUFFER_SIZE);
</code></pre>

<p>Rinse and repeat, we can actually do this within an enumerator and send appropriately sized chunks aross to C# to be processed without copying the whole thing out at once.  This could be useful when iterating over a set of documents, although most likely simply copying into the string and using string.c_str() to get the pointer to the copied data will give us easier code - we'll see how that works out when it comes to actually writing the C# we need.</p>

<p>Anyway, now we just need support for secondary indexes, and for this I'll write a rudimentary document store.</p>]]></description><link>http://codeofrob.com/entries/streaming-large-values-from-leveldb.html</link><guid isPermaLink="true">http://codeofrob.com/entries/streaming-large-values-from-leveldb.html</guid><dc:creator><![CDATA[Rob Ashton]]></dc:creator><pubDate>Wed, 06 Mar 2013 09:00:00 GMT</pubDate></item></channel></rss>